"","cluster","topic_rank_in_cluster","topic_in_cluster","title","abstract","doi","id","topic_probability_in_doc"
"1","cold_topic_cluster_no_1","rank_1","77","Identifying priority sites for the conservation of freshwater fish biodiversity in a Mediterranean basin with a high degree of threatened endemics","The Guadiana River basin's freshwater fish species richness, endemicity and threatened status (92% of native species are threatened) highlight the need for a large-scale study to identify priority areas for their conservation. One of the most common problems in conservation planning is the assessment of a site's relative value for the conservation of regional biodiversity. Here we used a two-tiered approach, which integrates an assessment of biodiversity loss and the evaluation of conservation value through site-specific measures. These measures based on the reference condition approach introduce the ability to make objective comparisons throughout the Guadiana River basin, thus avoiding a priori target areas. We identified a set of biodiversity priority areas of special conservation significance-which contain rare taxa as well as intact fish communities-because of their outstanding contribution to the basin's biodiversity. The inclusion of complete sub-basins in these priority areas might guarantee an optimal solution in terms of spatial aggregation and connectivity. However, the high spatial fragmentation to which the Guadiana River basin is submitted due to river regulation highlights the necessity of a systematic approach to evaluate the capability of the identified priority areas to maintain the Guadiana's freshwater fish biodiversity.","10.1007/s10750-008-9653-0","84135","0.2573034"
"2","cold_topic_cluster_no_1","rank_1","77","Operationalizing ecological connectivity in spatial conservation planning with Marxan Connect","Globally, protected areas are being established to protect biodiversity and to promote ecosystem resilience. The typical spatial conservation planning process leading to the creation of these protected areas focuses on representation and replication of ecological features, often using decision support tools such as Marxan. Yet, despite the important role ecological connectivity has in metapopulation persistence and resilience, Marxan currently requires manual input or specialized scripts to explicitly consider connectivity. 'Marxan Connect' is a new open source, open access Graphical User Interface (GUI) tool designed to assist conservation planners with the appropriate use of data on ecological connectivity in protected area network planning. Marxan Connect can facilitate the use of estimates of demographic connectivity (e.g. derived from animal tracking data, dispersal models, or genetic tools) or structural landscape connectivity (e.g. isolation by resistance). This is accomplished by calculating metapopulation-relevant connectivity metrics (e.g. eigenvector centrality) and treating those as conservation features or by including the connectivity data as a spatial dependency amongst sites in the prioritization process. Marxan Connect allows a wide group of users to incorporate directional ecological connectivity into conservation planning with Marxan. The solutions provided by Marxan Connect, combined with ecologically relevant post-hoc testing, are more likely to support persistent and resilient metapopulations (e.g. fish stocks) and provide better protection for biodiversity.","10.1111/2041-210X.13349","14685","0.2610619"
"3","cold_topic_cluster_no_1","rank_1","77","Assessing protected area overlaps and performance to attain China's new national park system","Overlapping protected area (PA) designations are a global widespread phenomenon and have seriously deteriorated the performance of China's PA system. However, a comprehensive investigation on the extents of both overlapping designations and conservation complementation among different PA categories is still lacking in China. Here, we used the case of Yunnan, southwest China, to quantitatively assess the spatial and institutional overlaps among six primary PA categories. We then investigated the conservation complementation among the different PA categories by measuring their contributions to improving the ecological representation, coverage of areas of conservation importance, and connectivity of the entire PA network. Overlaps existed within most pairs of PA categories; approximately 10% of the total PA area was designated as multiple categories and simultaneously managed by more than one institution. Different PA categories could complement each other in terms of protection coverage, ecological representation and connectivity. The sustainable use PAs effectively complemented the strict PAs by doubling the total PA coverage, increasing the ecological representation, raising the overall connectivity index from 1.44% to 4.03% and taking similar to 50% of the total area of key connecting PAs. Conflicting management regimes in combination with expanded PA objectives are the major drivers of the overlapping PA designations. By explicitly measuring the extents of overlapping and complementation among different PA categories, our analysis helps to efficiently assess the existing PM and integrate them into China's new national park system. This study provides insights into empirical generalities about overlapping designations and conservation complementation among multiple PA categories.","10.1016/j.biocon.2019.108382","17782","0.2686441"
"4","cold_topic_cluster_no_1","rank_1","77","Biologically representative and well-connected marine reserves enhance biodiversity persistence in conservation planning","Current methods in conservation planning for promoting the persistence of biodiversity typically focus on either representing species geographic distributions or maintaining connectivity between reserves, but rarely both, and take a focal species, rather than a multispecies, approach. Here, we link prioritization methods with population models to explore the impact of integrating both representation and connectivity into conservation planning for species persistence. Using data on 288 Mediterranean fish species with varying conservation requirements, we show that: (1) considering both representation and connectivity objectives provides the best strategy for enhanced biodiversity persistence and (2) connectivity objectives were fundamental to enhancing persistence of small-ranged species, which are most in need of conservation, while the representation objective benefited only wide-ranging species. Our approach provides a more comprehensive appraisal of planning applications than approaches focusing on either representation or connectivity, and will hopefully contribute to build more effective reserve networks for the persistence of biodiversity.","10.1111/conl.12439","36612","0.2924242"
"5","cold_topic_cluster_no_1","rank_1","77","Mapping conservation priorities and connectivity pathways under climate change for tropical ecosystems","Climate change and land use conversion are global threats to biodiversity. Protected areas and biological corridors have been historically implemented as biodiversity conservation measures and suggested as tools within planning frameworks to respond to climate change. However, few applications to national protected areas systems considering climate change in tropical countries exist. Our goal is to define new priority areas for biodiversity conservation and biological corridors within an existing protected areas network. We aim at preserving samples of all biodiversity under climate change and facilitate species dispersal to reduce the vulnerability of biodiversity. The analysis was based on a three step strategy: i) protect representative samples of various levels of terrestrial biodiversity across protected area systems given future redistributions under climate change, ii) identify and protect areas with reduced climate velocities where populations could persist for relatively longer periods, and iii) ensure species dispersal between conservation areas through climatic connectivity pathways. The study was integrated into a participatory planning approach for biodiversity conservation in Costa Rica. Results showed that there should be an increase of 11 % and 5 % on new conservation areas and biological corridors respectively. Our approach integrates climate change into the design of a network of protected areas for tropical ecosystems and can be applied to other biodiversity rich areas to reduce the vulnerability of biodiversity to global warming.","10.1007/s10584-016-1789-8","48049","0.3362637"
"6","cold_topic_cluster_no_1","rank_2","450","Functional responses in habitat selection by tropical birds moving through fragmented forest","1. The ability of animals to move through a landscape is a fundamental determinant of population persistence in fragmented habitats. This movement can be affected by both the composition and configuration of the remaining habitat. To date, few studies have examined the habitat selection of animals moving in novel landscapes or addressed whether animals exhibit a functional response in selection as available habitat changes. 2. To assess habitat selection during movement, we translocated 60 individuals of two species of birds with differing forest dependency in three configuration treatments in a highly fragmented, tropical dry forest landscape: along a forested riparian corridor, along a fencerow (row of live trees) or across pasture. We closely followed the return routes of translocated birds to determine their choice of habitat and proximity to the forest edge. We then tested whether habitat composition or configuration (treatment) best explained individual variation in habitat selection. 3. Both species preferred habitat closer to the forest edge, but this preference was weaker in the forest specialist, the barred antshrike Thamnophilus doliatus. This species selected routes in forest habitat, which included riparian corridors, over fencerow and stepping stone habitat, which were all preferred over pasture habitat. 4. By contrast, the forest generalist, the rufous-naped wren Campylorhynchus rufinucha preferred forest equally to fencerow and stepping stone habitat over pasture habitat. For it, fencerows were selected more strongly than stepping stones. 5. Analysis of the individual variation in selection for forest habitat revealed that both species exhibited a functional response to habitat configuration, selecting forest more strongly in riparian corridor treatments where it was also more abundant. The forest specialist also reduced its preference for edge habitat in riparian corridor treatments. 6.Synthesis and applications. The unprecedented precision of our route information demonstrates the extent to which our forest specialist preferred to travel in forest relative to fencerow and stepping stone habitat. Functional responses to habitat configuration indicated that both species make more use of other habitats when forested corridors are not present. Stepping stones in particular may be important features to the conservation of forest birds in highly fragmented habitats.","10.1111/j.1365-2664.2009.01756.x","82754","0.2379310"
"7","cold_topic_cluster_no_1","rank_2","450","Fruit feeding Cetoniinae community structure in an anthropogenic landscape in West Africa","The potential contribution of palm plantations to the conservation of forest-dwelling Cetoniinae (Coleoptera, Scarabaeidae) is assessed in southern Benin. Sample plots of 10 aerial traps per habitat type were set in two sites containing a forest patch, a palm plantation and some agricultural land. Overall, 2,217 individuals belonging to 31 species were collected. Typical species of each habitat were identified with the IndVal method (Dufrne and Legendre 1997). Species were categorized into three habitat-groups: forest specialists (11 species), farmland and open habitat specialists (9 species) and ubiquitous, generalist species found in all habitats (5 species). Only six species were too rare for assessing habitat preference. Palm plantations host a low density of Cetoniinae with no unique species. However, species composition reveals that they are used by forest specialist species that avoid open habitats like farmlands, therefore providing structural connectivity. Cetoniine flower beetles have potential as an indicator group, which can be used in multi-taxa approaches for habitat assessments in Africa. Using species-level metrics, their response to habitat change is clear, including the response of common species. This method is selective, cost-effective in time and materials, and species identification is rather straightforward.","10.1007/s10841-012-9483-2","75320","0.2520833"
"8","cold_topic_cluster_no_1","rank_2","450","Connectivity change in habitat networks","Habitat management is essential for safeguarding important flora and fauna. Further, habitat connectivity is a crucial component for maintaining biodiversity given that it is known to have implications for species persistence. However, damage to habitat due to natural and human induced hazards can alter spatial relationships between habitats, potentially impacting biodiversity. Therefore, the susceptibility of spatial relationships to patch loss and associated connectivity degradation is obviously an important factor in maintaining existing or planned habitat networks. Identifying patches vital to connectivity is critical both for effectively prioritizing protection (e.g., enhancing habitat connectivity) and establishing disaster mitigation measures (e.g., stemming the spread of habitat loss). This paper presents a methodology for characterizing connectivity associated with habitat networks. Methods for evaluating habitat network connectivity change are formalized. Examples are presented to facilitate analysis of connectivity in the management of biodiversity.","10.1007/s10980-008-9282-z","84759","0.2550725"
"9","cold_topic_cluster_no_1","rank_2","450","The diverse effects of habitat fragmentation on plant-pollinator interactions","Habitat fragmentation affects a wide variety of biological variables including species' abundance and richness (population demography), phenology, male and female reproductive fitness, and it also affects the degree of specialization versus generalization of pollination networks. Evidence is accumulating that suggests that habitat fragmentation can have significant impacts on plant-pollinator interactions. In this article, we review the literature on habitat fragmentation effects on plants, pollinators, and the pollination network. We also discuss pollination network mechanisms that may be affected by habitat fragmentation. Habitat fragmentation isolates populations and affects ecological properties at both population and community levels. Evidence shows that habitat size and connectivity directly or indirectly influence the abundance of both plant and pollinator species. In general, plant and pollinator diversity and population size decrease with the decreasing size and habitat connectivity. Habitat fragmentation of plant communities can shift plant phenological patterns, contract flowering periods and increase the risk of local pollinator extirpation. Fragmentation has the potential to influence pollination dynamics by altering pollinator or plant densities and by altering pollinator behavior. However, evidence for the impact of habitat fragmentation on plant species' flowering phenology is relatively limited, and little is known about the effect of habitat fragmentation on the phenology of pollinators. Habitat fragmentation also leads to reduced reproduction in many species. In contrast, other species showed neutral or positive responses to habitat fragmentation in female reproductive fitness, especially in plants regularly affected by pollen limitation and pollination limitation which lead to plants' experience selection for increased autogamy in isolated habitats. Habitat fragmentation often leads to the extirpation of specialist species and results in an influx of generalists. However, studies have shown that pollinators tend to be more generalized as habitat fragmentation increases. The reason is that habitat fragmentation changes the composition of the flora, and scatters floral resources, so any remaining pollinators may need to behave as generalists in order to survive. However, the knowledge of potential ecological consequences of habitat fragmentation is limited, especially regarding the effects on a long-term scale and at landscape scales. We propose experiments involving long-term monitoring, permanent samples of flowering plants, pollinators, and their interactions at large spatial and temporal scales.","10.1007/s11258-016-0608-7","58500","0.2615819"
"10","cold_topic_cluster_no_1","rank_2","450","Prioritising sites for pollinators in a fragmented coastal nectar habitat network in Western Europe","Context Habitat loss and fragmentation contribute significantly to pollinator decline and biodiversity loss globally. Conserving high quality habitats whilst restoring and connecting remnant habitat is critical to halt such declines. Objectives We quantified the connectivity of pollinator habitats for a generic focal species (GFS) which represented three groups of pollinators in an existing coastal nectar habitat network. Subsequently, in partnership with a conservation agency, we modelled an improved landscape that identified priority habitat patches to increase connectivity for pollinators. Methods We selected 4260 pollinator habitats along an 80 km section of coastland in Scotland using Phase 1 habitat data. A GFS represented three vulnerable European pollinator groups while graph theory and spatial metrics were used to identify optimal sites that could enhance habitat connectivity. Results Higher dispersing species experienced greater habitat connectivity in the improved landscape and habitat availability increased substantially in response to small increases in habitat. The improved landscape revealed important habitat patches in the existing landscape that should be protected and developed. Conclusions Our findings highlight that optimal landscapes can be designed through the integration of habitat data with spatial metrics for a GFS. By adopting this novel approach, conservation strategies can be targeted in an efficient manner to conserve at-risk species and their associated habitats. Integrating these design principles with policy and practice could enhance biodiversity across Europe.","10.1007/s10980-019-00884-x","19548","0.3052174"
"11","cold_topic_cluster_no_1","rank_3","119","THE EFFECT OF PIRACY WEBSITE BLOCKING ON CONSUMER BEHAVIOR","In this study, we ask what drives the success or failure of various supply-side anti-piracy enforcement actions such as piracy website blocking. We do this in the context of three court-ordered events affecting consumers in the United Kingdom: We first study Internet Service Providers' blocking of 53 video piracy sites in 2014 and of 19 piracy sites in 2013, and we then study the blocking of a single dominant site, ""The Pirate Bay,"" in 2012. We show that blocking 53 sites in 2014 caused treated users to decrease piracy and to increase their usage of legal subscription sites between 7% and 12%. It also caused an increase in new paid subscriptions. We find similar results for the blocking of 19 piracy sites in 2013. However, blocking a single site in 2012 caused no increase in usage of legal sites but instead caused users to increase visits to other unblocked piracy sites and VPN sites. We find evidence that increased search and learning costs associated with piracy drive the effectiveness of blocking multiple sites rather than just one primary site. This suggests that to increase legal IP use when faced with a dominant piracy channel, the optimal policy response must block multiple channels of access to pirated content, a distinction that the current literature has not made clear.","10.25300/MISQ/2020/15791","10424","0.1409091"
"12","cold_topic_cluster_no_1","rank_3","119","The Digital Divide: A Comparison of Online Consumer Health Information for African-American and General Audiences","Objectives: We sought to assess the quality of health information on internet sites with missions to serve African Americans and to compare the quality to that of sites targeting a general audience. Methods: Sites were identified by entering ""black Health,"" ""African American health,"" and ""health"" into 2 search engines. Websites were assessed for quality and usability by 2 independent readers using published criteria. Results: Disease-specific information was found on 64.7% of African-American sites and 86.2% of general sites. Among these sites, 730 of African-American sites listed authors' qualifications, compared to 960 of general sites (p=0.04). Sixty-four percent of African-American sites provided date last updated; compared with 100% of general sites (p=0.001). The mean literacy level for both types of sites was similar to 10th grade. The literacy level of African-American sites at governmental and educational domains was lower (NS). Conclusions: This is the first study to examine critically the quality of health information on Internet sites serving African-American audiences. Our study suggests methods to guide healthcare providers and health educators in counseling patients regarding internet-based health information. The ""digital divide"" is about quality as well as access.","10.1016/S0027-9684(15)31513-3","85026","0.1426966"
"13","cold_topic_cluster_no_1","rank_3","119","Search Engine Optimization: An Analysis of Rhinoplasty Web sites","The Internet is the primary source of information for facial plastic surgery patients. Most patients only analyze information in the first 10 Web sites retrieved. The aim of this study was to determine factors critical for improving Web site traffic and search engine optimization. A Google search of ""rhinoplasty"" was performed in Michigan. The first 20 distinct Web sites originating from private sources were included. Private was defined as personal Web sites for private practice physicians. The Web sites were evaluated using SEOquake and WooRANK, publicly available programs that analyze Web sites. Factors examined included the presence of social media, the number of distinct pages on the Web site, the traffic to the Web site, use of keywords, such as rhinoplasty in the heading and meta description, average visit duration, traffic coming from search, bounce rate, and the number of advertisements. Readability and Web site quality were also analyzed using the DISCERN and Health on the Net Foundation code principles. The first 10 Web sites were compared with the latter 10 Web sites using Student's t-tests. The first 10 Web sites received a significantly lower portion of traffic from search engines than the second 10 Web sites. The first 10 Web sites also had significantly fewer tags of the keyword ""nose"" in the meta description of the Web site. The first 10 Web sites were significantly more reliable according to the DISCERN instrument, scoring an average of 2.42 compared with 2.05 for the second 10 Web sites (p=0.029). Search engine optimization is critical for facial plastic surgeons as it improves online presence. This may potentially result in increased traffic and an increase in patient visits. However, Web sites that rely too heavily on search engines for traffic are less likely to be in the top 10 search results. Web site curators should maintain a wide focus for obtaining Web site traffic, possibly including advertising and publishing information in third party sources such as ""RealSelf.""","10.1055/s-0037-1607973","42996","0.1451389"
"14","cold_topic_cluster_no_1","rank_3","119","The World Heritage list: Which sites promote the brand? A big data spatial econometrics approach","UNESCO's World Heritage Convention encourages inscribed sites to promote the World Heritage brand by clearly communicating their affiliation. Based on the feedback from over 319,000 visitors at 791 locations, we create an index that shows the extent to which World Heritage sites are actually branding themselves as such. We find great heterogeneity throughout the list and explain this econometrically with site-specific incentives. Notably, the sites that benefit more from the World Heritage brand are significantly more willing to contribute to the collective brand than sites that benefit less. Specifically, rural sites are much better branded than urban sites, as rural sites benefit more from the brand than urban sites. We also find a positive relationship between World Heritage branding and its conservation status and a U-shaped relationship between a site's visitor numbers and its branding. Furthermore, Asian sites are much better branded than sites in the Middle East, and richer countries and those with already more international tourists are branded less. The difficulty of effective branding, e.g., for large, open-access sites, has no significant effect. Our findings suggest that mandatory World Heritage branding obligations would have a positive effect on the World Heritage brand equity, bringing conservation and economic benefits to a much wider range of World Heritage sites.","10.1007/s10824-016-9266-9","48583","0.1473684"
"15","cold_topic_cluster_no_1","rank_3","119","Quality of arthritis information on the Internet","Purpose. The quality and reliability of Internet-based arthritis information were studied. Methods. The search terms ""arthritis,"" ""osteoarthritis,"" and ""rheumatoid arthritis"" were entered into the AOL, MSN, Yahoo, Google, and Lycos search engines. The Web sites for the first 40 matches generated by each search engine were grouped by URL suffix and evaluated on the basis of four categories of criteria: disease and medication information content, Web-site navigability, required literacy level, and currentness of information. Ratings were assigned by using an assessment tool derived from published literature (maximum score of 15 points). Results. Of the 600 arthritis Web sites identified, only 69 were unique and included in the analysis. Fifty-seven percent were .com sites, 20% .org sites, 7% .gov sites, 6% .edu sites, and 10% other sites. Total scores for individual sites reviewed ranged from 3 to 14. Eighty percent of .gov sites, 75% of .edu sites, 29% of other sites, 36% of .com sites, and 21% of .org sites were within the top tertile of scores. No Web site met the criterion for being understandable to people with no more than a sixth-grade reading ability. .Gov sites scored significantly higher overall than .com sites, .org sites, and other sites. .Edu sites also scored relatively well. Conclusion. The quality of arthritis information on the Internet varied widely. Sites with URLs having suffixes of .gov and .edu were ranked higher than other types of sites.","10.1093/ajhp/62.11.1184","88482","0.2520000"
"16","cold_topic_cluster_no_1","rank_4","794","A MAPPING OF CHANGES IN CORAL REEFS CONDITION BASED ON DEVELOPMENT THE MARINE ECOTOURISM IN THE SOUTHERN PART COAST OF PADANG CITY - INDONESIA","This study aims to determine changes in the condition of coral reef ecosystems using satellite imagery. The research method used is the Geographic Information System (GIS) approach using Lyzenga algorithm method with data of Landsat +ETM 7 in 1998, 2008 and Landsat OLI 8 in 2018. The results of this study indicate a decrease in the extent of coral reef conditions in 1998-2008 occurs in live coral cover covering 106.60 ha and the addition of area occurred in dead coral cover covering 147.97 ha. While the decrease of coral reefs condition in 2008-2018 occurs in dead coral cover covering 29.66 ha and the addition of area occurred in live coral cover covering 112.29 ha. Coral mortality index in Nirwana beach with an average 62.28 %, meaning very large coral deaths in this region. Coral mortality index in Cindakir estuary with an average 46.60 %, meaning quite large coral deaths in this region so that coral reefs need to be rehabilitated in these waters, which can be done in form of artificial coral transplants. The coral mortality index in Muara Dua bay average 39 %, meaning that coral mortality is quite large in this region, while the coral mortality index in Ujung Siboko bay average 44.65 %, this means quite a large number of coral deaths in this region, but this location has a very good soft coral and it is necessary to rehabilitate corals in these waters with the form of artificial coral transplants.","10.21660/2020.76.ICGeo4"," 1419","0.2719101"
"17","cold_topic_cluster_no_1","rank_4","794","High Latitude Corals Tolerate Severe Cold Spell","Climatically extreme weather events often drive long-term ecological responses of ecosystems. By disrupting the important symbiosis with zooxanthellae, Marine Cold Spells (MCS) can cause bleaching and mortality in tropical and subtropical scleractinian corals. Here we report on the effects of a severe MCS on high latitude corals, where we expected to find bleaching and mortality. The MCS took place off the coast of Perth (32 degrees S), Western Australia in 2016. Bleaching was assessed before (2014) and after (2017) the MCS from surveys of permanent plots, and with timed bleaching searches. Temperature data was recorded with in situ loggers. During the MCS temperatures dipped to the coldest recorded in ten years (15.3 degrees C) and periods of < 17 degrees C lasted for up to 19 days. Only 4.3% of the surveyed coral colonies showed signs of bleaching. Bleaching was observed in 8 species where those most affected were Plesiastrea versipora and Montipora mollis. These findings suggest that high latitude corals in this area are tolerant of cold stress and are not persisting near a lethal temperature minimum. It has not been established whether other environmental conditions are limiting these species, and if so, what the implications are for coral performance on these reefs in a warmer future.","10.3389/fmars.2018.00014","40177","0.2755319"
"18","cold_topic_cluster_no_1","rank_4","794","Detecting coral bleaching, using QuickBird multi-temporal data: A feasibility study at Kish Island, the Persian Gulf","Coral bleaching events have become more frequent and intense worldwide and speculated to be a severe threat for coral survival in future. The Persian Gulf, as one of the warmest seas, has also experienced coral mortalities and bleaching events. Historically, bleaching events are known to occur south of the Persian Gulf, such information is scarce in the northern side. Perhaps remoteness and inaccessibility to Iran main coral communities which have developed on offshore islands can explain such lack of data. To address this issue, the feasibility of using multi-temporal satellite images for detecting past bleaching events were investigated. Two QuickBird images (2005, 2008) were selected to detect 2007 bleaching event at Kish Island, Iran, and the accuracy of results were compared to in situ observations. Current study might represent ""algae-challenged"" scenario in terms of having 7 months' time lapse between bleaching event and post-bleaching satellite image. As a result of this, we had algae-covered corals instead of white bleached corals. In the proposed procedure pre and post-bleaching images were classified, and changes in reflectance values within coral classes were interpreted as bleaching areas. By using this method we could eliminate the effect of miss-classification between bleached corals and sand; as well as algae-covered corals and live corals. Furthermore, it is not necessary to have a post-bleaching image acquired during bleaching events, although having such image will increase the accuracy. The proposed technique detected similar to 28% of bleached corals and the results support the idea that coral bleaching can be distinguished by detecting the changes in reflectance values in pre and post-bleaching images. Understanding the occurrence, severity, and extent of past bleaching events may help us understand the population dynamics of Iran corals and reveal coral connectivity patterns in the Persian Gulf. ","10.1016/j.ecss.2012.12.006","75401","0.2789855"
"19","cold_topic_cluster_no_1","rank_4","794","The potential for self-seeding by the coral Pocillopora spp. in Moorea, French Polynesia","Coral reefs in Moorea, French Polynesia, suffered catastrophic coral mortality through predation by Acanthaster planci from 2006 to 2010, and Cyclone Oli in 2010, yet by 2015 some coral populations were approaching pre-disturbance sizes. Using long-term study plots, we quantified population dynamics of spawning Pocillopora spp. along the north shore of Moorea between 2010 and 2014, and considered evidence that population recovery could be supported by self-seeding. Results scaled up from study plots and settlement tiles suggest that the number of Pocillopora spp. colonies on the outer reef increased 1,890-fold between 2010 and 2014/2015, and in the back reef, 8-fold between 2010 and 2014/2015. Assuming that spawning Pocillopora spp. in Moorea release similar numbers of eggs as con-generics in Hawaii, and fertilization success is similar to other spawning corals, the capacity of Pocillopora spp. to produce larvae was estimated. These estimates suggest that Pocillopora spp. in Moorea produced a large excess of larvae in 2010 and 2014 relative to the number required to produce the recruits found in the back reef and outer reef in 2010 and 2014, even assuming that similar to 99.9% of the larvae do not recruit in Moorea. Less than a third of the recruits in one year would have to survive to produce the juvenile Pocillopora spp. found in the back and outer reefs in 2010 and 2014/2015. Our first order approximations reveal the potential for Pocillopora spp. on the north shore of Moorea to produce enough larvae to support local recruitment and population recovery following a catastrophic disturbance.","10.7717/peerj.2544","56445","0.2814815"
"20","cold_topic_cluster_no_1","rank_4","794","Holocene key coral species in the Northwest Pacific: indicators of reef formation and reef ecosystem responses to global climate change and anthropogenic stresses in the near future","The geological record of key coral species that contribute to reef formation and maintenance of reef ecosystems is important for understanding the ecosystem response to global-scale climate change and anthropogenic stresses in the near future. Future responses can be predicted from accumulated data on Holocene reef species identified in drillcore and from data on raised reef terraces. The present study analyzes a dataset based on 27 drillcores, raised reef terraces, and 134 radiocarbon and U-Th ages from reefs of the Northwest Pacific, with the aim of examining the role of key coral species in reef growth and maintenance for reef ecosystem during Holocene sea-level change. The results indicate a latitudinal change in key coral species: arborescent Acropora (Acropora intermedia and Acropora muricata) was the dominant reef builder at reef crests in the tropics, whereas Porites (Porites australiensis, Porites lutea, and Porites lobata) was the dominant contributor to reef growth in the subtropics between 10,000 and 7000 cal. years BP (when the rate of sea-level rise was 10 m/ka). Acropora digitifera, Acropora hyacinthus, Acropora robusta/A. abrotanoides, Isopora palifera, Favia stelligera, and Goniastrea retiformis from the corymbose and tabular Acropora facies were the main key coral species at reef crests between 7000 and 5000 cal. years BP (when the rate of sea-level rise was 5 m/ka) and during the following period of stable sea-level. Massive Porites (P australiensis, P. lutea, and P. lobata) contributed to reef growth in shallow lagoons during the period of stable sea level. Key coral species from the corymbose and tabular Acropora facies have the potential to build reefs and maintain ecosystems in the near future under a global sea-level rise of 2-6 m/ka, as do key coral species from the arborescent Acropora fades and massive Porites facies, which show vigorous growth and are tolerant to relatively deep-water, low-energy environments. However, these species are likely to experience severe mortality in upcoming decades due to natural and anthropogenic stresses. Consequently, this damage will lead to a collapse in reef formation and the maintenance of reef ecosystems in the near future. This study emphasizes the need for research into the conservation of key coral species. ","10.1016/j.quascirev.2012.01.011","77896","0.2856287"
"21","cold_topic_cluster_no_1","rank_5","24","Impact of reservoirs on zooplankton diversity and implications for the conservation of natural aquatic environments","Reservoirs are intermediate ecosystems between rivers and lakes. These ecosystems break up the landscape by forming artificial lakes connected by rivers and change important ecological processes of lotic ecosystems (e.g. organic matter production and nutrient cycling) and thus have direct effects on aquatic biodiversity. We test the hypothesis that damming water courses exerts a negative effect on the zooplankton community by decreasing the alpha diversity. Samplings were conducted in 30 reservoirs and 29 natural lakes in the Neotropics, during two hydrological periods (dry and rainy) in 2001. The study analysed differences in alpha diversity, the number of rare, accessory and constant species, and species composition between the types of lakes. Alpha diversity was significantly higher in natural lakes and species composition differed between natural lakes and reservoirs. Zooplankton alpha diversity was positively related to total phosphorus (best model), indicating that productive environments (natural lakes) gather more species. Our results suggest that reservoirs have negative effect on community structure, whereas natural lakes have an important ecological function for biodiversity conservation because they are refuges for biodiversity.","10.1007/s10750-015-2260-y","63795","0.2056818"
"22","cold_topic_cluster_no_1","rank_5","24","Local-global overlap in diversity informs mechanisms of bacterial biogeography","Spatial variation in environmental conditions and barriers to organism movement are thought to be important factors for generating endemic species, thus enhancing global diversity. Recent microbial ecology research suggested that the entire diversity of bacteria in the global oceans could be recovered at a single site, thus inferring a lack of bacterial endemism. We argue this is not the case in the global ocean, but might be in other bacterial ecosystems with higher dispersal rates and lower global diversity, like the human gut. We quantified the degree to which local and global bacterial diversity overlap in a diverse set of ecosystems. Upon comparison of observed local-global diversity overlap with predictions from a neutral biogeography model, human-associated microbiomes (gut, skin, mouth) behaved much closer to neutral expectations whereas soil, lake and marine communities deviated strongly from the neutral expectations. This is likely a result of differences in dispersal rate among 'patches', global diversity of these systems, and local densities of bacterial cells. It appears that overlap of local and global bacterial diversity is surprisingly large (but likely not one-hundred percent), and most importantly this overlap appears to be predictable based upon traditional biogeographic parameters like community size, global diversity, inter-patch environmental heterogeneity and patch connectivity.","10.1038/ismej.2015.51","63313","0.2072072"
"23","cold_topic_cluster_no_1","rank_5","24","Comparison of plant species diversity with different plant communities in deciduous forests","Species diversity is one of the most important indices used for evaluating the sustainability of forest communities. This study aims to characterize the forest communities and to identify and compare the plant species diversity in the study area. For this purpose, 152 releves were sampled by a randomized-systematic method, using the Braun-Blanquet scale. Classification of the vegetation was conducted by the twinspan algorithm. Four communities, including Querco-Carpinetum betulii, Carpineto-Fagetum Oriental, Rusco-Fagetum Oriental and Fagetum Oriental were recognized. Species richness, Shannon, and Simpson indices were applied to quantify diversity of the different communities. Turkey test was used to investigate the differences in the species richness, diversity and evenness indices among the different communities. The results illustrate that Querco-Carpinetum betulii and Carpineto-Fagetum Oriental communities are significantly more diverse than Rusco-Fagetum Oriental and Fagetum Oriental communities. The spatial structure of the releves becomes more 'homogenous' and the dominance structure changes: the proportion of beech-forest species is gradually increasing. At the same time, the number of species per unit area decreases constantly, reaching eventually the value comparable to that recorded for hornbeam forest. Generally, species diversity is inversely correlated with the dominance of shade tolerant climax species.","10.1007/BF03326077","84019","0.2275862"
"24","cold_topic_cluster_no_1","rank_5","24","Diversity of river fishes influenced by habitat heterogeneity across hydrogeomorphic divisions","Freshwater fish communities are structured through complex interactions among multiple spatial scales. Efforts to conserve and rehabilitate fish communities in riverine systems can benefit from an understanding of how processes across spatial scales influence species diversity patterns. We assessed species diversity at different hierarchical spatial scales and changes in species composition along the longitudinal gradient of the Niobrara River basin, Great Plains, USA. We assessed the contribution of - and -diversity components to -diversity at five spatial scales (i.e., mesohabitat, site, reach, segment, and river) using an additive partitioning approach. The observed mean -diversity was significantly greater than expected at the site, reach, and segment spatial scales. The most significant difference between expected and observed -diversity occurred at the segment spatial scale and suggests differences in community structure along the Niobrara River. Additive partitioning of diversity components provided a framework with which to assess patterns at multiple hierarchical levels. Our results suggest that changes in channel geomorphic and hydrologic conditions provide the impetus for species sorting resulting in unique fish assemblages along the Niobrara River. Conservation of species diversity for Great Plains fishes and other similar systems will likely benefit from considering species filtering processes at multiple spatial scales and maintaining intact hydrologic regimes across unique geomorphic boundaries.","10.1002/rra.3306","35297","0.2330357"
"25","cold_topic_cluster_no_1","rank_5","24","Species richness and composition differ in response to landscape and biogeography","ContextUnderstanding how landscape patterns affect species diversity is of great importance in the fields of biogeography, landscape ecology and conservation planning, but despite the rapid advance in biodiversity analysis, investigations of spatial effects on biodiversity are still largely focused on species richness.ObjectivesWe wanted to know if and how species richness and species composition are differentially driven by the spatial measures dominating studies in landscape ecology and biogeography. As both measures require the same limited presence/absence information, it is important to choose an appropriate diversity measure, as differing results could have important consequences for interpreting ecological processes.MethodsWe recorded plant occurrences on 112 islands in the Baltic archipelago. Species richness and composition were calculated for each island, and the explanatory power of island area and habitat heterogeneity, distance to mainland and structural connectivity at three different landscape sizes were examined.ResultsA total of 354 different plant species were recorded. The influence of landscape variables differed depending on which diversity measure was used. Island area and structural connectivity determined plant species richness, while species composition revealed a more complex pattern, being influenced by island area, habitat heterogeneity and structural connectivity.ConclusionsAlthough both measures require the same basic input data, species composition can reveal more about the ecological processes affecting plant communities in fragmented landscapes than species richness alone. Therefore, we recommend that species community composition should be used as an additional standard measure of diversity for biogeography, landscape ecology and conservation planning.","10.1007/s10980-018-0742-9","32954","0.2760684"
"26","cold_topic_cluster_no_10","rank_1","803","Versions g1.0 and g1.1 of the LASG/IAP Flexible Global Ocean-Atmosphere-Land System Model","The latest two versions of the TAP Flexible Global Ocean-Atmosphere-Land System (FGOALS) model-versions g1.0 and g1.1, are described in this study. Both two versions are fully coupled GaAs without any flux correction, major changes for g1.1 mainly lie in four aspects: (1) advection schemes for tracer in the ocean component model; (2) zonal filter scheme in high latitudes in the ocean component model; (3) coupling scheme for fresh water flux in high latitudes; and (4) an improved algorithm of air-sea turbulent flux depending on the surface current of the ocean. As a result, the substantial cold biases in the tropical Pacific and high latitudes are improved by g1.1, especially g1.1 simulates more reasonable equatorial thermocline, poleward heat transport, zonal overturning stream function in the ocean and sea. ice distribution than g1.0. Significant; ENSO variability are simulated by both versions, however the ENSO behavior by g1.0 differs from the observed one in many aspects: about twice ENSO amplitude as observed, false ENSO asymmetry, only one peak period around 3 years, etc. Due to improved mean climate state by g1 1, many basic characteristics of ENSO are reproduced by g1.1, e.g., more reasonable ENSO amplitude, two peaks of power spectra for ENSO events, and positive SST skewness in the eastern Pacific as observed.","10.1007/s00376-010-9112-5","81003","0.2434343"
"27","cold_topic_cluster_no_10","rank_1","803","Greenland surface mass balance simulated by a regional climate model and comparison with satellite-derived data in 1990-1991","The 1990 and 1991 ablation seasons over Greenland are simulated with a coupled atmosphere-snow regional climate model with a 25-km horizontal resolution. The simulated snow water content allows a direct comparison with the satellite-derived melt signal. The model is forced with 6-hourly ERA-40 reanalysis at its boundaries. An evaluation of the simulated precipitation and a comparison of the modelled melt zone and the surface albedo with remote sensing observations are presented. Both the distribution and quantity of the simulated precipitation agree with observations from coastal weather stations, estimates from other models and the ERA-40 reanalysis. There are overestimations along the steep eastern coast, which are most likely due to the ""topographic barrier effect"". The simulated extent and time evolution of the wet snow zone compare generally well with satellite-derived data, except during rainfall events on the ice sheet and because of a bias in the passive microwave retrieved melt signal. Although satellite-based surface albedo retrieval is only valid in the case of clear sky, the interpolation and the correction of these data enable us to validate the simulated albedo on the scale of the whole Greenland. These two comparisons highlight a large sensitivity of the remote sensing observations to weather conditions. Our high-resolution climate model was used to improve the retrieval algorithms by taking more fully into account the atmosphere variability. Finally, the good agreement of the simulated melting surface with the improved satellite signal allows a detailed estimation of the melting volume from the simulation.","10.1007/s00382-005-0010-y","88549","0.2452174"
"28","cold_topic_cluster_no_10","rank_1","803","A comparison of solar radiative flux above clouds from MODIS with BALTIMOS regional climate model simulations","A comparison study for the solar radiative flux above clouds is presented between the regional climate model system BALTEX integrated model system (BALTIMOS) and Moderate Resolution Imaging Spectroradiometer (MODIS) satellite observations. For MODIS, an algorithm has been developed to retrieve reflected shortwave fluxes over clouds. The study area is the Baltic Sea catchment area during an 11-month period from February to December 2002. The intercomparison focuses on the variations of the daily and seasonal cycle and the spatial distributions. We found good agreement between the observed and the simulated data with a bias of the temporal mean of 13.6 W/m(2) and a bias of the spatial mean of 35.5 W/m(2). For summer months, BALTIMOS overestimates the solar flux with up to 90 W/m(2) (20%). This might be explained by the insufficient representation of cirrus clouds in the regional climate model.","10.1007/s00704-010-0253-3","68785","0.2490566"
"29","cold_topic_cluster_no_10","rank_1","803","Impacts of Assimilated Data on Reanalysis Climatology","Impacts of the observing systems and meteorological parameters on a reanalyzed climatology are investigated using an assimilation algorithm based on the National Centers for Environmental Prediction/Department of Energy (NCEP/DOE) reanalysis system. The one-year analyzed climatology is generated in a standard analysis using all of the observations, which is named the control run. Additional climatology data sets are produced by selectively choosing observational systems or meteorological variables in the reanalysis system. It is found that the radiosonde-only observation is sufficient for reproducing the reanalysis climatology. Satellite observations have no significant contribution to the large-scale fields. The importance of meteorological variables on the analyzed climatology is temperature, wind, and moisture in that order, where the mass observation is found to have a greater impact on the analyzed climatology than wind. It is, however, noted that the moisture field demonstrates a profound influence on the surface hydroclimate such as cloud cover, radiation flux, and land surface temperature.","10.1007/s13143-010-0017-0","82335","0.2563380"
"30","cold_topic_cluster_no_10","rank_1","803","An assessment of the latent and sensible heat flux on the simulated regional climate over Southwestern South Atlantic Ocean","A Regional Climate Model (RegCM3) 10-year (1990-1999) simulation over southwestern South Atlantic Ocean (SAO) is evaluated to assess the mean climatology and the simulation errors of turbulent fluxes over the sea. Moreover, the relationship between these fluxes and the rainfall over some cyclogenetic areas is also analyzed. The RegCM3 results are validated using some reanalyses datasets (ERA40, R2, GPCP and WHOI). The summer and winter spatial patterns of latent and sensible heat fluxes simulated by the RegCM3 are in agreement with the reanalyses (WHOI, R2 and ERA40). They show large latent heat fluxes exchange in the subtropical SAO and at higher latitudes in the warm waters of Brazil Current. In particular, the magnitude of RegCM3 latent heat fluxes is similar to the WHOI, which is probably related to two factors: (a) small specific humidity bias, and (b) the RegCM3 flux algorithm. In contrast, the RegCM3 presents large overestimation of sensible heat flux, though it simulates well their spatial pattern. This simulation error is associated with the RegCM3 underestimation of the 2-m air temperature. In southwestern SAO, in three known cyclogenetic areas, the reanalyses and the RegCM3 show the existence of different physical mechanisms that control the annual cycles of latent/sensible heating and rainfall. It is shown that over the eastern coast of Uruguay (35A degrees-43A degrees S) and the southeastern coast of Argentina (44A degrees-52A degrees S) the sea-air moisture and heat exchange play an important role to control the annual cycle of precipitation. This does not happen on the south/southeastern coast of Brazil.","10.1007/s00382-009-0681-x","82339","0.2733945"
"31","cold_topic_cluster_no_10","rank_2","707","Using airborne HIAPER Pole-to-Pole Observations (HIPPO) to evaluate model and remote sensing estimates of atmospheric carbon dioxide","In recent years, space-borne observations of atmospheric carbon dioxide (CO2) have been increasingly used in global carbon-cycle studies. In order to obtain added value from space-borne measurements, they have to suffice stringent accuracy and precision requirements, with the latter being less crucial as it can be reduced by just enhanced sample size. Validation of CO2 column-averaged dry air mole fractions (XCO2) heavily relies on measurements of the Total Carbon Column Observing Network (TCCON). Owing to the sparseness of the network and the requirements imposed on space-based measurements, independent additional validation is highly valuable. Here, we use observations from the High-Performance Instrumented Airborne Platform for Environmental Research (HIAPER) Pole-to-Pole Observations (HIPPO) flights from 01/2009 through 09/2011 to validate CO2 measurements from satellites (Greenhouse Gases Observing Satellite - GOSAT, Thermal Emission Sounder - TES, Atmospheric Infrared Sounder - AIRS) and atmospheric inversion models (CarbonTracker CT2013B, Monitoring Atmospheric Composition and Climate (MACC) v13r1). We find that the atmospheric models capture the XCO2 variability observed in HIPPO flights very well, with correlation coefficients (r(2)) of 0.93 and 0.95 for CT2013B and MACC, respectively. Some larger discrepancies can be observed in profile comparisons at higher latitudes, in particular at 300aEuro-hPa during the peaks of either carbon uptake or release. These deviations can be up to 4aEuro-ppm and hint at misrepresentation of vertical transport. Comparisons with the GOSAT satellite are of comparable quality, with an r(2) of 0.85, a mean bias mu of -0.06aEuro-ppm, and a standard deviation sigma of 0.45aEuro-ppm. TES exhibits an r(2) of 0.75, mu of 0.34aEuro-ppm, and sigma of 1.13aEuro-ppm. For AIRS, we find an r(2) of 0.37, mu of 1.11aEuro-ppm, and sigma of 1.46aEuro-ppm, with latitude-dependent biases. For these comparisons at least 6, 20, and 50 atmospheric soundings have been averaged for GOSAT, TES, and AIRS, respectively. Overall, we find that GOSAT soundings over the remote Pacific Ocean mostly meet the stringent accuracy requirements of about 0.5aEuro-ppm for space-based CO2 observations.","10.5194/acp-16-7867-2016","62212","0.2917241"
"32","cold_topic_cluster_no_10","rank_2","707","Intercomparison of Carbon Dioxide Products Retrieved from GOSAT Short-Wavelength Infrared Spectra for Three Years (2010-2012)","This paper presents the comparison of two CO2 datasets from the National Institute for Environmental Studies (NIES) of Japan and the Atmospheric CO2 Observations from Space (ACOS) of NASA for three years (2010 to 2012). Both CO2 datasets are retrieved from the Greenhouse gases Observing SATellite (GOSAT) short-wavelength infrared spectra over High gain surface land. In this three-year period, the yield of the NIES CO2 column averaged dry air mole fractions (XCO2) is about 71% of ACOS retrievals. The overall bias is 0.21 +/- 1.85 ppm and 0.69 +/- 2.13 ppm for ACOS and NIES XCO2, respectively, when compared with ground-based Fourier Transform Spectrometer (FTS) observations from twelve Total Carbon Column Observing Network (TCCON) sites. The differences in XCO2 three-year means and seasonal means are within about 1 to 2 ppm. Strong consistency is obtained for the ACOS and NIES XCO2 monthly averages time series over different regions, with the greatest mean difference of ACOS to NIES monthly means over China (1.43 +/- 0.60 ppm) and the least over Brazil (0.03 +/- 0.64 ppm). The intercomparison between the two XCO2 datasets indicates that the ACOS XCO2 is globally higher than NIES by about 1 ppm and has smaller bias and better consistency than NIES data.","10.3390/atmos7090109","57591","0.2987654"
"33","cold_topic_cluster_no_10","rank_2","707","Validation of XCO2 derived from SWIR spectra of GOSAT TANSO-FTS with aircraft measurement data","Column-averaged dry air mole fractions of carbon dioxide (XCO2) retrieved from Greenhouse gases Observing SATellite (GOSAT) Short-Wavelength InfraRed (SWIR) observations were validated with aircraft measurements by the Comprehensive Observation Network for TRace gases by AIrLiner (CONTRAIL) project, the National Oceanic and Atmospheric Administration (NOAA), the US Department of Energy (DOE), the National Institute for Environmental Studies (NIES), the HIAPER Pole-to-Pole Observations (HIPPO) program, and the GOSAT validation aircraft observation campaign over Japan. To calculate XCO2 based on aircraft measurements (aircraft-based XCO2), tower measurements and model outputs were used for additional information near the surface and above the tropopause, respectively. Before validation, we investigated the impacts of GOSAT SWIR column averaging kernels (CAKs) and the shape of a priori profiles on the aircraft-based XCO2 calculation. The differences between aircraft-based XCO2 with and without the application of GOSAT CAK were evaluated to be less than +/-0.4 ppm at most, and less than +/-0.1 ppm on average. Therefore, we concluded that the GOSAT CAK produces only a minor effect on the aircraft-based XCO2 calculation in terms of the overall uncertainty of GOSAT XCO2. We compared GOSAT data retrieved within +/-2 or +/-5 degrees latitude/longitude boxes centered at each aircraft measurement site to aircraft-based data measured on a GOSAT overpass day. The results indicated that GOSAT XCO2 over land regions agreed with aircraft-based XCO2, except that the former is biased by -0.68 ppm (-0.99 ppm) with a standard deviation of 2.56 ppm (2.51 ppm), whereas the averages of the differences between the GOSAT XCO2 over ocean and the aircraft-based XCO2 were -1.82 ppm (-2.27 ppm) with a standard deviation of 1.04 ppm (1.79 ppm) for +/-2 degrees (+/-5 degrees) boxes.","10.5194/acp-13-9771-2013","75894","0.3476562"
"34","cold_topic_cluster_no_10","rank_2","707","XCO2 retrieval algorithm in short-wave infrared spectrum and validation with TCCON data","Based on the optimal estimation method, the global carbon dioxide hyperspectral remote sensing inversion system GF_VRTM is established for the greenhouse gas carbon dioxide remote sensing detection of GF-5 satellite. The period of April 2013 to April 2014 GOSAT-FTS observations over TCCON site Tsukba are simulated by GF_VRTM, and the retrieval results are compared with GOSAT-FTS products and TCCON XCO2. The mean residuals between the GOSAT-measured and GF_VRTM-simulated spectra are nearly zero for the O-2-A band and WCO2 band over the spectral ranges. The average and standard deviation of AXCO(2) between GF_VRTM retrievals and GOSAT-FTS L2 products are evaluated to be -1.63 and 1.78 ppm for the whole period, and the average and standard deviation of AXCO2 between GF_VRTM retrievals and TCCON data are evaluated to be -0.38 and 2.25 ppm for the whole period. The accuracy of the GF_VRTM retrieval is less than 1% based on the validation results with GOSAT-FTS L2 products and TCCON data. ","10.1016/j.ijleo.2016.04.074","61796","0.3694915"
"35","cold_topic_cluster_no_10","rank_2","707","CMAQ simulation of atmospheric CO2 concentration in East Asia: Comparison with GOSAT observations and ground measurements","Satellite observations are widely used in global CO2 assimilations, but their quality for use in regional assimilation systems has not yet been thoroughly determined. Validation of satellite observations and model simulations of CO2 is crucial for carbon flux inversions. In this study, we focus on evaluating the uncertainties of model simulations and satellite observations. The atmospheric CO2 distribution in East Asia during 2012 was simulated using a regional chemical transport model (RAMS-CMAQ) and compared with both CO2 column density (XCO2) from the Gases Observing SATellite (GOSAT) and CO2 concentrations from the World Data Centre for Greenhouse Gases (WDCGG). The results indicate that simulated XCO2 is generally lower than GOSAT XCO2 by 1.19 ppm on average, and their monthly differences vary from 0.05 to 2.84 ppm, with the corresponding correlation coefficients ranging between 0.1 and 0.67. CMAQ simulations are good to capture the CO2 variation as ground-based observations, and their correlation coefficients are from 0.62 to 0.93, but the average value of CMAQ simulation is 2.4 ppm higher than ground-based observation. Thus, we inferred that the GOSAT retrievals may overestimate XCO2, which is consistent with the validation of GOSAT XCO2 using Total Carbon Column Observing Network measurements. The near-surface CO2 concentration was obviously overestimated in GOSAT XCO2. Compared with the relatively small difference between CMAQ and GOSAT XCO2, the large difference in CO2 near surface or their vertical profiles indicates more improvements are needed to reduce the uncertainties in both satellite observations and model simulations. ","10.1016/j.atmosenv.2017.03.056","45992","0.3787037"
"36","cold_topic_cluster_no_10","rank_3","30","Advances and limitations of atmospheric boundary layer observations with GPS occultation over southeast Pacific Ocean","The typical atmospheric boundary layer (ABL) over the southeast (SE) Pacific Ocean is featured with a strong temperature inversion and a sharp moisture gradient across the ABL top. The strong moisture and temperature gradients result in a sharp refractivity gradient that can be precisely detected by the Global Positioning System (GPS) radio occultation (RO) measurements. In this paper, the Constellation Observing System for Meteorology, Ionosphere & Climate (COSMIC) GPS RO soundings, radiosondes and the high-resolution ECMWF analysis over the SE Pacific are analyzed. COSMIC RO is able to detect a wide range of ABL height variations (1-2 km) as observed from the radiosondes. However, the ECMWF analysis systematically underestimates the ABL heights. The sharp refractivity gradient at the ABL top frequently exceeds the critical refraction (e. g., -157 N-unit km(-1)) and becomes the so-called ducting condition, which results in a systematic RO refractivity bias (or called N-bias) inside the ABL. Simulation study based on radiosonde profiles reveals the magnitudes of the N-biases are vertical resolution dependent. The N-bias is also the primary cause of the systematically smaller refractivity gradient (rarely exceeding -110 N-unit km(-1)) at the ABL top from RO measurement. However, the N-bias seems not affect the ABL height detection. Instead, the very large RO bending angle and the sharp refractivity gradient due to ducting allow reliable detection of the ABL height from GPS RO. The seasonal mean climatology of ABL heights derived from a nine-month composite of COSMIC RO soundings over the SE Pacific reveals significant differences from the ECMWF analysis. Both show an increase of ABL height from the shallow stratocumulus near the coast to a much higher trade wind inversion further off the coast. However, COSMIC RO shows an overall deeper ABL and reveals different locations of the minimum and maximum ABL heights as compared to the ECMWF analysis. At low latitudes, despite the decreasing number of COSMIC RO soundings and the lower percentage of soundings that penetrate into the lowest 500-m above the mean-sea-level, there are small sampling errors in the mean ABL height climatology. The difference of ABL height climatology between COSMIC RO and ECMWF analysis over SE Pacific is significant and requires further studies.","10.5194/acp-12-903-2012","78710","0.1651934"
"37","cold_topic_cluster_no_10","rank_3","30","Radiation Component Calculation and Energy Budget Analysis for the Korean Peninsula Region","In this study, a radiation component calculation algorithm was developed using channel data from the Himawari-8 Advanced Himawari Imager (AHI) and meteorological data from the Unified Model (UM) Local Data Assimilation and Prediction System (LDAPS). In addition, the energy budget of the Korean Peninsula region in 2016 was calculated and its regional differences were analyzed. Radiation components derived using the algorithm were calibrated using the broadband radiation component data from the Clouds and the Earth's Radiant Energy System (CERES) to improve their accuracy. The calculated radiation components and the CERES data showed an annual mean percent bias of less than 3.5% and a high correlation coefficient of over 0.98. The energy budget of the Korean Peninsula region was -2.4 Wm(-2) at the top of the atmosphere (R-T), - 14.5 Wm(-2) at the surface (R-S), and 12.1 Wm(-2) in the atmosphere (R-A), with regional energy budget differences. The Seoul region had a high surface temperature (289.5 K) and a R-S of - 33.4 Wm(-2) (surface emission), whereas the Sokcho region had a low surface temperature (284.7 K) and a R-S of 5.0 Wm(-2) (surface absorption), for a difference of 38.5 Wm(-2). In short, regions with relatively high surface temperatures tended to show energy emission, and regions with relatively low surface temperatures tended to show energy absorption. Such regional energy imbalances can cause weather and climate changes and bring about meteorological disasters, and thus research on detecting energy budget changes must be continued.","10.3390/rs10071147","36631","0.1684783"
"38","cold_topic_cluster_no_10","rank_3","30","Comparisons of CH4 ground-based FTIR measurements near Saint Petersburg with GOSAT observations","Atmospheric column-average methane mole fractions measured with ground-based Fourier-transform spectroscopy near Saint Petersburg, Russia (59.9 degrees N, 29.8 degrees E, 20 ma.s.l.) are compared with similar data obtained with the Japanese GOSAT (Greenhouse gases Observing SATellite) in the years 2009-2012. Average CH4 mole fractions for the GOSAT data version V01.xx are -15.0 +/- 5.4 ppb less than the corresponding values obtained from ground-based measurements (with the standard deviations of biases at 13.0 +/- 4.2 ppb). For the GOSAT data version V02.xx, the average values of the differences are -1.9 +/- 1.8 ppb with standard deviations of 14.5 +/- 1.3 ppb. This verifies that FTIR (Fourier transform infrared) spectroscopic observations near Saint Petersburg have similar biases with GOSAT satellite data as FTIR measurements at other ground-based networks and aircraft CH4 estimations.","10.5194/amt-7-1003-2014","72058","0.1690909"
"39","cold_topic_cluster_no_10","rank_3","30","Variability of the Boundary Layer Over an Urban Continental Site Based on 10 Years of Active Remote Sensing Observations in Warsaw","Atmospheric boundary layer height (ABLH) was observed by the CHM15k ceilometer (January 2008 to October 2013) and the PollyXT lidar (July 2013 to December 2018) over the European Aerosol Research LIdar NETwork to Establish an Aerosol Climatology (EARLINET) site at the Remote Sensing Laboratory (RS-Lab) in Warsaw, Poland. Out of a maximum number of 4017 observational days within this period, a subset of quasi-continuous measurements conducted with these instruments at the same wavelength (1064 nm) was carefully chosen. This provided a data sample of 1841 diurnal cycle ABLH observations. The ABLHs were derived from ceilometer and lidar signals using the wavelet covariance transform method (WCT), gradient method (GDT), and standard deviation method (STD). For comparisons, the rawinsondes of the World Meteorological Organization (WMO 12374 site in Legionowo, 25 km distance to the RS-Lab) were used. The ABLHs derived from rawinsondes by the skew-T-log-p method and the bulk Richardson (bulk-Ri) method had a linear correlation coefficient (R-2) of 0.9 and standard deviation (SD) of 0.32 km. A comparison of the ABLHs obtained for different methods and instruments indicated a relatively good agreement. The ABLHs estimated from the rawinsondes with the bulk-Ri method had the highest correlations, R-2 of 0.80 and 0.70 with the ABLHs determined using the WCT method on ceilometer and lidar signals, respectively. The three methods applied to the simultaneous, collocated lidar, and ceilometer observations (July to October 2013) showed good agreement, especially for the WCT method (R-2 of 0.94, SD of 0.19 km). A scaling threshold-based algorithm was proposed to homogenize ceilometer and lidar datasets, which were applied on the lidar data, and significantly improved the coherence of the results (R-2 of 0.98, SD of 0.11 km). The difference of ABLH between clear-sky and cloudy conditions was on average below 230 m for the ceilometer and below 70 m for the lidar retrievals. The statistical analysis of the long-term observations indicated that the monthly mean ABLHs varied throughout the year between 0.6 and 1.8 km. The seasonal mean ABLH was of 1.16 +/- 0.16 km in spring, 1.34 +/- 0.15 km in summer, 0.99 +/- 0.11 km in autumn, and 0.73 +/- 0.08 km in winter. In spring and summer, the daytime and nighttime ABLHs appeared mainly in a frequency distribution range of 0.6 to 1.0 km. In winter, the distribution was common between 0.2 and 0.6 km. In autumn, it was relatively balanced between 0.2 and 1.2 km. The annual mean ABLHs maintained between 0.77 and 1.16 km, whereby the mean heights of the well-mixed, residual, and nocturnal layer were 1.14 +/- 0.11, 1.27 +/- 0.09, and 0.71 +/- 0.06 km, respectively (for clear-sky conditions). For the whole observation period, the ABLHs below 1 km constituted more than 60% of the retrievals. A strong seasonal change of the monthly mean ABLH diurnal cycle was evident; a mild weakly defined autumn diurnal cycle, followed by a somewhat flat winter diurnal cycle, then a sharp transition to a spring diurnal cycle, and a high bell-like summer diurnal cycle. A prolonged summertime was manifested by the September cycle being more similar to the summer than autumn cycles.","10.3390/rs12020340","18132","0.1772512"
"40","cold_topic_cluster_no_10","rank_3","30","Decadal Climate Simulations Using Accurate and Fast Neural Network Emulation of Full, Longwave and Shortwave, Radiation","An approach to calculating model physics using neural network emulations, previously proposed and developed by the authors, has been implemented in this study for both longwave and shortwave radiation parameterizations, or to the full model radiation, the most time-consuming component of model physics. The developed highly accurate neural network emulations of the NCAR Community Atmospheric Model (CAM) longwave and shortwave radiation parameterizations are 150 and 20 times as fast as the original/control longwave and shortwave radiation parameterizations, respectively. The full neural network model radiation was used for a decadal climate model simulation with the NCAR CAM. A detailed comparison of parallel decadal climate simulations performed with the original NCAR model radiation parameterizations and with their neural network emulations is presented. Almost identical results have been obtained for the parallel decadal simulations. This opens the opportunity of using efficient neural network emulations for the full model radiation for decadal and longer climate simulations as well as for weather prediction.","10.1175/2008MWR2385.1","85119","0.1942857"
"41","cold_topic_cluster_no_10","rank_4","163","Agglomeration economies: Are they exaggerated by industrial aggregation?","This paper investigates whether estimates of agglomeration economies are misleading because of industrial aggregation. Most estimates of these economies use data for manufacturing industries at the 2-digit SIC level. But it has been argued that using more disaggregated industries would yield more reliable results. To study this proposition, I compare estimates from 2-digit data to estimates from 3-digit data and from pooled 3-digit data. A comparison of the results shows that estimating agglomeration economies at the 2-digit level does not exaggerate their importance. ","10.1016/S0166-0462(97)00019-7","91801","0.1694444"
"42","cold_topic_cluster_no_10","rank_4","163","Spatial scaling of evapotranspiration as affected by heterogeneities in vegetation, topography, and soil texture","To date, simulating land surface hydrological processes over large areas at spatial resolutions higher than 1 km remains technically unfeasible, because of limitations of data availability and computational resources. Several studies have demonstrated, however, that gridding the land surface into coarse homogeneous pixels may cause important biases on ecosystem model estimations of water budget components at local, regional and global scales. These biases result from the overlook of sub-pixel variability of land surface characteristics. This study suggests a simple algorithm that uses sub-pixel information on the spatial variability of vegetation and soil cover, and surface topography to correct evapotranspiration (ET) estimates, made at coarse spatial resolutions where the land surface is considered as homogeneous within each pixel. The algorithm operates in such a way that the ET rates as obtained from calculations made at coarse spatial resolutions are multiplied by separate simple functions that attempt to reproduce the effects of sub-pixel variability of land cover, leaf area index, soil texture, and topography on ET. Its application to a remote sensing process-based model estimates made at a 1-km resolution over a watershed located in the southern part of the Canadian boreal forest improved estimates of average ET as well as its temporal and spatial variability. ","10.1016/j.rse.2006.01.017","87751","0.1791667"
"43","cold_topic_cluster_no_10","rank_4","163","Evaluation of satellite-based evapotranspiration estimates in China","Accurate and continuous estimation of evapotranspiration (ET) is crucial for effective water resource management. We used the moderate resolution imaging spectroradiometer (MODIS) standard ET algorithm forced by the MODIS land products and the three-hourly solar radiation datasets to estimate daily actual evapotranspiration of China (ET_MOD) for the years 2001 to 2015. From the point scale validations using seven eddy covariance tower sites, the results showed that the agreement of ET_MOD estimates and observations was higher for monthly and daily values than that of instantaneous values. Under the major river basin and subbasin levels' comparisons with the variable infiltration capacity hydrological model estimates, the ET_MOD exhibited a slight overestimation in northern China and underestimation in southern China. The mean annual ET_MOD estimates agreed favorably with the hydrological model with coefficients of determination (R-2) of 0.93 and 0.83 at major river basin and subbasin scale, respectively. At national scale, the spatiotemporal variations of ET_MOD estimates matched well with those ET estimates from various sources. However, ET_MOD estimates were generally lower than the other estimates in the Tibetan Plateau. This underestimation may be attributed to the plateau climate along with low air temperature and sparsely vegetated surface on the Tibetan Plateau. ","10.1117/1.JRS.11.026019","46765","0.1858824"
"44","cold_topic_cluster_no_10","rank_4","163","A MODIS-Based Energy Balance to Estimate Evapotranspiration for Clear-Sky Days in Brazilian Tropical Savannas","Evapotranspiration (ET) plays an important role in global climate dynamics and in primary production of terrestrial ecosystems; it represents the mass and energy transfer from the land to atmosphere. Limitations to measuring ET at large scales using ground-based methods have motivated the development of satellite remote sensing techniques. The purpose of this work is to evaluate the accuracy of the SEBAL algorithm for estimating surface turbulent heat fluxes at regional scale, using 28 images from MODIS. SEBAL estimates are compared with eddy-covariance (EC) measurements and results from the hydrological model MGB-IPH. SEBAL instantaneous estimates of latent heat flux (LE) yielded r(2) = 0.64 and r(2) = 0.62 over sugarcane croplands and savannas when compared against in situ EC estimates. At the same sites, daily aggregated estimates of LE were r(2) = 0.76 and r(2) = 0.66, respectively. Energy balance closure showed that turbulent fluxes over sugarcane croplands were underestimated by 7% and 9% over savannas. Average daily ET from SEBAL is in close agreement with estimates from the hydrological model for an overlay of 38,100 km(2) (r(2) = 0.88). Inputs to which the algorithm is most sensitive are vegetation index (NDVI), gradient of temperature (dT) to compute sensible heat flux (H) and net radiation (Re). It was verified that SEBAL has a tendency to overestimate results both at local and regional scales probably because of low sensitivity to soil moisture and water stress. Nevertheless the results confirm the potential of the SEBAL algorithm, when used with MODIS images for estimating instantaneous LE and daily ET from large areas.","10.3390/rs4030703","78059","0.2019608"
"45","cold_topic_cluster_no_10","rank_4","163","Effect of spatial resolution on remote sensing estimation of total evaporation in the uMngeni catchment, South Africa","This study evaluated the effect of two readily available multispectral sensors: the newly launched 30 m spatial resolution Landsat 8 and the long-serving 1000 m moderate resolution imaging spectroradiometer (MODIS) datasets in the spatial representation of total evaporation in the heterogeneous uMngeni catchment, South Africa, using the surface energy balance system model. The results showed that sensor spatial resolution plays a critical role in the accurate estimation of energy fluxes and total evaporation across a heterogeneous catchment. Landsat 8 estimates showed better spatial representation of the biophysical parameters and total evaporation for different land cover types, due to the relatively higher spatial resolution compared to the coarse spatial resolution MODIS sensor. Moreover, MODIS failed to capture the spatial variations of total evaporation estimates across the catchment. Analysis of variance (ANOVA) results showed that MODIS-based total evaporation estimates did not show any significant differences across different land cover types (one-way ANOVA; F-1.924 = 1.412, p = 0.186). However, Landsat 8 images yielded significantly different estimates between different land cover types (one-way ANOVA; F-1.993 = 5.185, p < 0.001). The validation results showed that Landsat 8 estimates were more comparable to eddy covariance (EC) measurements than the MODIS-based total evaporation estimates. EC measurement on May 23, 2013, was 3.8 mm/day, whereas the Landsat 8 estimate on the same day was 3.6 mm/day, with MODIS showing significantly lower estimates of 2.3 mm/day. The findings of this study underscore the importance of spatial resolution in estimating spatial variations of total evaporation at the catchment scale, thus, they provide critical information on the relevance of the readily available remote sensing products in water resources management in data-scarce environments. ","10.1117/1.JRS.9.095997","63554","0.2200000"
"46","cold_topic_cluster_no_10","rank_5","446","Spatio-temporal analysis of alpine ecotones: A spatial explicit model targeting altitudinal vegetation shifts","There is general agreement in literature that Alpine vegetation belt ecotones have shown a trend of upward migration in the last few decades. Despite the potential of such shifts as indicators of global change effects in mountain ecosystems, there are relatively few works focused on their assessment in a systematic and spatially explicit way. In this work our aim is to quantify the altitudinal shifts and analyse the spatial pattern dynamics of mountain ecotones. We developed a novel procedure to delineate the current and former state of three characteristic mountain ecotones, which we formalised as forest, tree and tundra lines. Our approach is based on the recognition of altitudinal extreme outposts identified with ecotone locations at a slope scale. The integration of multi-temporal datasets allows the identification and quantification of altitudinal advances and retreats in the outpost locations for a given period. We tested the method in a section of the Italian Alps for the period 1957-2003. Results show a general trend of an increase in altitude for the three ecotones, despite the occurrence of occasional decreases. We estimate decadal altitude increments of 25 m for forest line, 13 m for treeline and 11 m for tundra line. We also identified changes in ecotone spatial morphology between the two dates, with significant implications in connectivity and colonisation dynamics. ","10.1016/j.ecolmodel.2009.11.010","82679","0.1837209"
"47","cold_topic_cluster_no_10","rank_5","446","The role of alluvial fans in the mountain fluvial systems of southeast Spain: Implications of climatic change","The mountain fluvial systems of southeast Spain involve sediment supply from steep mountain slopes into headwater channels. Alluvial fans often occur where these headwater channels emerge from the mountain areas, and may influence the connectivity of the sediment transport system from the mountain source areas to the main lowland drainages. Critical in this role is whether the alluvial fans are aggrading or dissecting, and whether there is a break or continuity in the channel through the fan environment. Previous work has identified some of the factors influencing the behaviour of the alluvial fans in southeast Spain. This paper deals with the mountain front alluvial fans in the semi-arid areas of Murcia and Almeria provinces. It attempts, by mapping the location of alluvial fans, then their classification into aggrading or dissecting fans, to identify the extent to which the mountain fluvial systems are buffered by aggrading alluvial fans or exhibit channel continuity through the mountain front environment. It further considers the implications of climatically induced changes between aggradational and dissectional behaviour on alluvial fans.","","92193","0.1837500"
"48","cold_topic_cluster_no_10","rank_5","446","Spatial Correlation between Type of Mountain Area and Land Use Degree in Guizhou Province, China","A scientific definition of the type of mountain area and an exploration of the spatial correlation between different types of mountain areas and regional land use at the county level are important for reasonable land resource utilization and regional sustainable development. Here, a geographic information system was used to analyze digital elevation model data and to define the extent of mountainous land and types of mountain areas in Guizhou province. Exploratory spatial data analysis was used to study the spatial coupling relation between the type of mountain area and land use degree in Guizhou province at the county level. The results were as follows: (1) Guizhou province has a high proportion of mountainous land, with a ratio of mountainous land to non-mountainous land of 88:11. The county-level administrative units in Guizhou province were exclusively mountainous, consisting of eight semi mountainous counties, nine quasi mountainous counties, 35 apparently mountainous counties, 13 type I completely mountainous counties, and 23 type II completely mountainous counties; (2) The land use degree at the county level in Guizhou province have remarkable spatial differentiation characteristics. Counties with a high cultivation coefficient are mainly located in the western area along the line between Yinjiang county and Anlong county in west Guizhou province. Counties with a large proportion of construction land or a high integrated index of land use degree are mainly distributed in the economically developed area of central Guizhou province, including parts of the counties (districts/cities) administrated by Guiyang, Zunyi, Liupanshui, Anshun, Duyun, and Kaili; (3) County-level administrative units with relatively flat topography and a low proportion of mountainous land have a large proportion of construction land and a large degree of regional land exploitation. However, the extent of cultivation of county-level administrative units under similar topography differs considerably; (4) The increase in urban land intensity and the decrease in cultivated land intensity are distinctive features of land system change in mountain areas, which is conducive to the sustainable development of mountain.","10.3390/su8090849","57848","0.1844720"
"49","cold_topic_cluster_no_10","rank_5","446","A New High-Resolution Map of world Mountains and an Online Tool for Visualizing and Comparing Characterizations of Global Mountain Distributions","Answers to the seemingly straightforward questions ""what is a mountain?' and ""where are the mountains of the world?"" are in fact quite complex, and there have been few attempts to map the mountains of the earth in a consistent and rigorous fashion. However, knowing exactly where mountain ecosystems are distributed on the planet is a precursor to conserving them, as called for in Sustainable Development Goals 6 and 15 of the United Nations 2030 Agenda for Sustainable Development. In this article we first compare 3 characterizations of global mountain distributions, including a new, high-resolution (250 m) map of global mountains derived from terrain characteristics. We show how differences in conceptual definition, methodology, and spatial resolution of source data can result in differences in the extent and location of lands classed as mountains. For example, the new 250-m resource documents a larger global mountain extent than previous characterizations, although it excludes plateaus, hilly forelands, and other landforms that are often considered part of mountain areas. We then introduce the Global Mountain Explorer, a new web-based application specifically developed for exploration, visualization, and comparison of these maps. This new open-access tool is an intuitive and versatile resource suitable for a broad range of users and applications.","10.1659/MRD-JOURNAL-D-17-00107.1","35854","0.1963855"
"50","cold_topic_cluster_no_10","rank_5","446","An innovative approach to improve SRTM DEM using multispectral imagery and artificial neural network","Although the Shuttle Radar Topography Mission [SRTM) data are a publicly accessible Digital Elevation Model [DEM) provided at no cost, its accuracy especially at forested area is known to be limited with root mean square error (RMSE) of approx. 14 m in Singapore's forested area. Such inaccuracy is attributed to the 5.6 cm wavelength used by SRTM that does not penetrate vegetation well. This paper considers forested areas of central catchment of Singapore as a proof of concept of an approach to improve the SRTM data set. The approach makes full use of (1) the introduction of multispectral imagery (Landsat 8), of 30 m resolution, into SRTM data; (2) the Artificial Neural Network (ANN) to flex its known strengths in pattern recognition and; (3) a reference DEM of high accuracy (1 m) derived through the integration of stereo imaging of worldview-1 and extensive ground survey points. The study shows a series of significant improvements of the SRTM when assessed with the reference DEM of 2 different areas, with RMSE reduction of approximate to 68% (from 13.9 m to 4.4 m) and approximate to 52% (from 14.2 m to 6.7 m). In addition, the assessment of the resulting DEM also includes comparisons with simple denoising methodology (Low Pass Filter) and commercially available product called NEXTMap (R) World 30.","10.1002/2015MS000536","59094","0.2153846"
"51","cold_topic_cluster_no_11","rank_1","832","Identification of Sun Glint Contamination in GMI Measurements Over the Global Ocean","This paper utilizes the model regression difference method to identify sun glint contamination on Global Precipitation Measurement Microwave Imager (GMI) data over the ocean based on observations from 2015 to 2016. The spatial distribution characteristics and the critical angles of the sun glint flags are analyzed in depth. It is found that the GMI measurements with horizontal and vertical polarizations at 10.65 GHz over the ocean are sometimes contaminated by the solar radiation reflected by the sea surface. The sun glint contamination has also been detected over high reflected land surface. The intensity and locations of the contamination are related to the sun glint angle. Only those GMI field of views with smaller sun glint angles are easily contaminated. The closer the sun glint angle is to 0 degrees, the stronger the magnitude of the contamination. The GMI observations at other channels are not contaminated mainly because sun glint is most pronounced at 10 GHz. There are too strong constraints and tossing out of too many useful data in current GMI sun glint algorithms. The suggested critical angles of the sun glint flags for 10.65GHz is 20 degrees to reduce false flagging. By applying the model regression difference method, the error in brightness temperature caused by sun glint can be corrected. The Tropical Rainfall Measuring Mission Microwave Imager (TMI) observations at 10.65 GHz are also contaminated by the reflected solar radiation from the ocean, and the intensity and locations of the contamination are similar to those of the GMI.","10.1109/TGRS.2019.2906380","22921","0.2801980"
"52","cold_topic_cluster_no_11","rank_1","832","Accuracy of Satellite Sea Surface Temperatures at 7 and 11 GHz","Satellite microwave radiometers capable of accurately retrieving sea surface temperature (SST) have provided great advances in oceanographic research. A number of future satellite missions are planned to carry microwave radiometers of various designs and orbits. While it is well known that the 11 GHz SST retrievals are less accurate than the 7 GHz retrievals, particularly in colder waters, it has not been demonstrated using existing microwave data. The Advanced Microwave Scanning Radiometer-Earth Observing System (AMSR-E) provides the means to examine the accuracies of SST retrievals using these channels in a systematic manner. In this paper, the accuracies of SSTs at 7 and 11 GHz are determined using two approaches: modeled and empirical. The modeled accuracies are calculated using an emissivity model and climatology SSTs, while empirical accuracies are estimated through validation of AMSR-E 7 and 11 GHz SST retrievals using over six years of data. It was found that the 7 GHz SST retrievals have less errors due to radiometer noise and geophysical errors than the 11 GHz retrievals at all latitudes. Additionally, while averaging the 11 GHz retrievals will diminish error due to uncorrelated radiometer noise, the geophysical error is still higher than for the 7 GHz retrievals, particularly at the higher latitudes.","10.1109/TGRS.2009.2030322","82611","0.3023810"
"53","cold_topic_cluster_no_11","rank_1","832","Validation and analysis of aerosol optical thickness retrieval over land","Aerosol optical thickness (AOT) retrieval from Moderate Resolution Imaging Spectroradiometer (MODIS) data has been well established over oceans, but this is not the case over land. In this article, the AOT data sets retrieved by exploiting the synergy of TERRA and AQUA MODIS data (SYNTAM) over land are validated with ground-based measurements from Aerosol Robotic Network (AERONET) data, as well as from the National Aeronautics and Space Administration (NASA) AOT products, amended with a DeepBlue algorithm in Asian (15-60 degrees N and 35-150 degrees E) and American areas (30-40 degrees N and 100-120 degrees W). Overall, AOT retrieval errors of around 10-20% against AERONET data are found at both 1 and 10 km resolutions. The spectral and spatial sensitivities of the AOT correlation are explicitly addressed at both 1 and 10 km resolutions. Three window sizes, 1 x 1, 3 x 3 and 5 x 5, are tested for SYNTAM to evaluate the effect of window size on parameter statistics, and it is found that the accuracy of the SYNTAM method decreases with increasing window size. The validations at three spectral bands of 0.47, 0.55 and 0.66 mu m show that the accuracies of different bands are 80-90% similar, and that the band at 0.47 mu m has the highest accuracy most of the time. Comparisons between AOT data sets derived from the SYNTAM and AOT products from the NASA Dark Dense Vegetation (DDV) and the DeepBlue algorithms are also conducted using data from the USA. More pixels with AOT values for the area could be retrieved using the SYNTAM method with the NASA DeepBlue algorithm. The AOT values of more than 90% of pixels derived by both methods are very close. This clearly shows that AOT data from SYNTAM are very close to the AOT data set from the NASA DeepBlue algorithm in cloud-free areas. The synergic use of both the SYNTAM and DeepBlue algorithms could produce AOT values over much greater land areas.","10.1080/01431161.2011.577831","78689","0.3089286"
"54","cold_topic_cluster_no_11","rank_1","832","Multispectral information from TANSO-FTS instrument - Part 2: Application to aerosol effect on greenhouse gas retrievals","This article is the second in a series of studies investigating the benefits of multispectral measurements to improve the atmospheric parameter retrievals. In the first paper, we presented an information content (IC) analysis from the thermal infrared (TIR) and shortwave infrared (SWIR) bands of Thermal And Near infrared Sensor for carbon Observations-Fourier Transform Spectrometer (TANSO-FTS) instrument dedicated to greenhouse gas retrieval in clear sky conditions. This second paper presents the potential of the spectral synergy from TIR to visible for aerosol characterization, and their impact on the retrieved CO2 and CH4 column concentrations. The IC is then used to determine the most informative spectral channels for the simultaneous retrieval of greenhouse gas total columns and aerosol parameters. The results show that a channel selection spanning the four bands can improve the computation time and retrieval accuracy. Therefore, the spectral synergy allows obtaining up to almost seven different aerosol parameters, which is comparable to the most informative dedicated instruments. Moreover, a channel selection from the TIR to visible bands allows retrieving CO2 and CH4 total columns simultaneously in the presence of one aerosol layer with a similar accuracy to using all channels together to retrieve each gas separately in clear sky conditions.","10.5194/amt-6-3313-2013","75484","0.3114943"
"55","cold_topic_cluster_no_11","rank_1","832","Multispectral information from TANSO-FTS instrument - Part 1: Application to greenhouse gases (CO2 and CH4) in clear sky conditions","The Greenhouse gases Observing SATellite (GOSAT) mission, and in particular the Thermal And Near infrared Sensor for carbon Observations-Fourier Transform Spectrometer (TANSO-FTS) instrument, has the advantage of being able to measure simultaneously the same field of view in different spectral ranges with a high spectral resolution. These features allow studying the benefits of using multispectral measurements to improve the CO2 and CH4 retrievals. In order to quantify the impact of the spectral synergy on the retrieval accuracy, we performed an information content (IC) analysis from simulated spectra corresponding to the three infrared bands of TANSO-FTS. The advantages and limitations of using thermal and shortwave infrared simultaneously are discussed according to surface type and state vector composition. The IC is then used to determine the most informative spectral channels for the simultaneous retrieval of CO2 and CH4. The results show that a channel selection spanning the three infrared bands can improve the computation time and retrieval accuracy. Therefore, a selection of less than 700 channels from the thermal infrared (TIR) and shortwave infrared (SWIR) bands allows retrieving CO2 and CH4 simultaneously with a similar accuracy to using all channels together to retrieve each gas separately.","10.5194/amt-6-3301-2013","75483","0.3337349"
"56","cold_topic_cluster_no_11","rank_2","765","Mean radiation fluxes in the near-IR spectral range: Algorithms for calculation","Algorithms are presented to compute the mean fluxes of shortwave radiation, modulated by statistically homogeneous broken clouds, in the near-IR spectral range (0.7-3.2 mu m). One method of the mean spectral flux computation consists of dividing the spectral interval in question into N-int subintervals according to the spectral resolution specified. The mean spectral fluxes are then calculated with the Monte Carlo method by assuming constant cloud optical characteristics within each subinterval. This last method (algorithm 1) accurately accounts for the spectral behavior of optical characteristics of clouds and atmospheric gases and can therefore be regarded as a reference. The mean spectral flux computations of high spectral resolution (say, Delta nu = 10-20 cm(-1)) may require several hundreds of spectral intervals. Therefore the method of dependent tests (algorithm 2) is proposed, which effectively uses notable features of the spectral dependence of cloud optical parameters, thus allowing significant simplifications and extra savings of computer time. Comparison of results from algorithms 1 and 2 shows that the mean radiant fluxes agree to within the relative computation error (3%). This indicates that algorithm 2 is reasonably accurate; in addition, its efficiency is several orders of magnitude better than that of the reference algorithm 1.","10.1029/96JD02218","92050","0.2096386"
"57","cold_topic_cluster_no_11","rank_2","765","Deriving inherent optical properties from decomposition of hyperspectral non-water absorption","Semi-analytical algorithms (SAAs) developed for multispectral ocean color sensors have benefited from a variety of approaches for retrieving the magnitude and spectral shape of inherent optical properties (IOPs). SAAs generally follow two approaches: 1) simultaneous retrieval of all IOPs, resulting in pre-defined bio-optical models and spectral dependence between IOPs and 2) retrieval of bulk IOPs (absorption and backscattering) first followed by decomposition into separate components, allowing for independent retrievals of some components. Current algorithms used to decompose hyperspectral remotely-sensed reflectance into IOPs follow the first strategy. Here, a spectral deconvolution algorithm for incorporation into the second strategy is presented that decomposes a(t-w)(lambda) from in situ measurements and estimates absorption due to phytoplankton (a(ph)(lambda)) and colored detrital material (a(dg)(lambda)) free of explicit assumptions. The algorithm described here, Derivative Analysis and Iterative Spectral Evaluation of Absorption (DAISEA), provides estimates of a(ph)(lambda) and a(dg)(lambda) over a spectral range from 350 to 700 nm. Estimated a(ph)(lambda) and a(dg)(lambda) showed an average normalized root mean square difference of < 30% and < 20%, respectively, from 350 to 650 nm for the majority of optically distinct environments considered. Estimated S-dg median difference was < 20% for all environments considered, while distribution of S-dg uncertainty suggests that biogeochemical variability represented by S-dg can be estimated free of bias. DAISEA results suggest that hyperspectral satellite ocean color data will improve our ability to track biogeochemical processes affiliated with variability in a(dg)(lambda) and S-dg free of explicit assumptions.","10.1016/j.rse.2019.03.004","26733","0.2274510"
"58","cold_topic_cluster_no_11","rank_2","765","Approach for determining the contributions of phytoplankton, colored organic material, and nonalgal particles to the total spectral absorption in marine waters","Using a data set of 1333 samples, we assess the spectral absorption relationships of different wave bands for phytoplankton (ph) and particles. We find that a nonlinear model (second-order quadratic equations) delivers good performance in describing their spectral characteristics. Based on these spectral relationships, we develop a method for partitioning the total absorption coefficient into the contributions attributable to phytoplankton [alpha(ph)(lambda)], colored dissolved organic material [CDOM; alpha(CDOM)(lambda)], and nonalgal particles [NAP; alpha(NAP)(lambda)]. This method is validated using a data set that contains 550 simultaneous measurements of phytoplankton, CDOM, and NAP from the NASA bio-Optical Marine Algorithm Dataset. We find that our method is highly efficient and robust, with significant accuracy: the relative root-mean-square errors (RMSEs) are 25.96%, 38.30%, and 19.96% for alpha(ph)(443), alpha(CDOM)(443), and the CDOM exponential slope, respectively. The performance is still satisfactory when the method is applied to water samples from the northern South China Sea as a regional case. The computed and measured absorption coefficients (167 samples) agree well with the RMSEs, i.e., 18.50%, 32.82%, and 10.21% for alpha(ph)(443), alpha(CDOM)(443), and the CDOM exponential slope, respectively. Finally, the partitioning method is applied directly to an independent data set (1160 samples) derived from the Bermuda Bio-Optics Project that contains relatively low absorption values, and we also obtain good inversion accuracy [RMSEs of 32.37%, 32.57%, and 11.52% for alpha(ph)(443), alpha(CDOM)(443), and the CDOM exponential slope, respectively]. Our results indicate that this partitioning method delivers satisfactory performance for the retrieval of alpha(ph), alpha(CDOM), and alpha(NAP). Therefore, this may be a useful tool for extracting absorption coefficients from in situ measurements or remotely sensed ocean-color data. ","10.1364/AO.52.004249","74206","0.2590164"
"59","cold_topic_cluster_no_11","rank_2","765","Variability of light absorption properties in optically complex inland waters of Lake Chaohu, China","Absorption coefficients of phytoplankton, colored detrital matter (CDM), non-algal particles (NAP), colored dissolved organic matter (CDOM), and their relative contributions to total non-water absorption (at,) are essential variables for bio-optical and radiative transfer models. Light absorption properties showed large range and variability sampled at 194 stations throughout Lake Chaohu between May 2013 and April 2015. The at was dominated by phytoplankton absorption (a(ph)) and NAP absorption (a(d)). The contribution of CDOM absorption to a(t -) (w) was lower than 30%. Phytoplankton and NAP were the primary sources of spatial and vertical variability in absorption properties. Light absorption by CDOM, though significant in magnitude, was relatively constant. CDM absorption (a(dg)) was dominated by NAP. The spatial variation of the absorption coefficients from each of the optically active constituents were driven by several main inflow rivers in the western and middle part of Lake Chaohu. Algal blooms and bottom resuspension contributed to vertical variability as observed by phytoplankton and NAP profiles. Specific absorption of phytoplankton had significant spatial and seasonal variations without vertical variation. The spectral slope of absorption showed no significant spatial variability (p > 0.05). Variations of absorption affected different ranges of remote sensing reflectance (R-rs) spectrum, thereby increasing the difficulty of applying the remote sensing algorithm in optically complex waters. Parameters and relationships presented in this study provide useful information for bio-optical models and remote sensing of lakes similar to Lake Chaohu in terms of optical properties. ","10.1016/j.jglr.2016.10.006","48595","0.2767857"
"60","cold_topic_cluster_no_11","rank_2","765","Covariance of the absorption of phytoplankton, colored dissolved organic matter, and detritus in case I waters, as deduced from the Coastal Zone Color Scanner bio-optical algorithm","The universal bio-optical algorithm of the Coastal Zone Color Scanner (CZCS) for case I waters implicitly contains an average covariance of the absorption by phytoplankton and colored dissolved organic matter (CDOM) and detritus. We made that covariance explicit by combining the CZCS algorithm with an expression for reflectance. The spectral variation of absorption by CDOM plus detritus for case I waters may be estimated by the expression a((gd)(lambda)) = 2a((ph)(443))*chl[exp[-0.013(lambda - 443)]}. ","10.1364/AO.35.002109","92231","0.3000000"
"61","cold_topic_cluster_no_11","rank_3","75","CO2 retrieval model and analysis in short-wave infrared spectrum","The global carbon dioxide hyperspectral remote sensing inversion system GF_VRTM is established for the greenhouse gas carbon dioxide remote sensing detection of GF-5 satellite. The validation and error analysis of global inversion is performed by using GF_VRTM with 21 June 2013 observation data of GOSAT-FTS in this study. The simulation results show that the CO2 averaged column concentrations (XCO2) overall trends are basically identical between GF_VRTM retrievals and GOSAT-FTS observations. There are 138 exposure points in whole observation points, and the relative error less than 2% is about 85%. The mean square error is 5.00 ppm, but the global average error is 1.09 ppm. If the assumption that GOSAT-FTS observation data is true, GF_VRTM satisfies the average precision requirements (less than 1%) of XCO2 global inversion. For the results of the comparison, the minimum error scenario and maximum error scenario are selected for error analysis. Error sources are smooth error, measurement noise error, the forward model parameter error and forward model error, respectively. For the minimum error scenario and maximum error scenario, the relative error of XCO2 inversion caused by smooth error and measurement noise error are 0.44% and -1.62%, respectively, and the relative error of XCO2 inversion caused by the forward model parameter error are -0.43% and -1.53%, respectively. At the same time, the forward model error is ignored. ","10.1016/j.ijleo.2016.01.144","61971","0.2580000"
"62","cold_topic_cluster_no_11","rank_3","75","Laplacian-Uniform Mixture-Driven Iterative Robust Coding With Applications to Face Recognition Against Dense Errors","Outliers due to occlusion, pixel corruption, and so on pose serious challenges to face recognition despite the recent progress brought by sparse representation. In this article, we show that robust statistics implemented by the state-of-the-art methods are insufficient for robustness against dense gross errors. By modeling the distribution of coding residuals with a Laplacian-uniform mixture, we obtain a sparse representation that is significantly more robust than the previous methods. The nonconvex error term of the implemented objective function is nondifferentiable at zero and cannot be properly addressed by the usual iteratively reweighted least-squares formulation. We show that an iterative robust coding algorithm can be derived by local linear approximation of the nonconvex error term, which is both effective and efficient. With iteratively reweighted l(1) minimization of the error term, the proposed algorithm is capable of handling the sparsity assumption of the coding errors more appropriately than the previous methods. Notably, it has the distinct property of addressing error detection and error correction cooperatively in the robust coding process. The proposed method demonstrates significantly improved robustness for face recognition against dense gross errors, either contiguous or discontiguous, as verified by extensive experiments.","10.1109/TNNLS.2019.2945372"," 6114","0.2609195"
"63","cold_topic_cluster_no_11","rank_3","75","ACM: An Energy-Efficient Accuracy Configurable Multiplier for Error-Resilient Applications","The multimedia applications such as image, audio and video processing allow approximation in computations, provided that, errors are of definite types and have austerities within confined limits, thus exhibiting error-resiliency. An approximate arithmetic circuit can be exploited to avail this error-resiliency for improving energy-efficiency. This paper presents an approximate multiplier that provides higher energy-efficiency at the cost of minor loss of accuracy. The proposed multiplier offers twofold improved performance because of reduced level of gates and curtailed inherent switched capacitances. Further, to achieve variable accuracy, an Accuracy Configurable Multiplier (ACM) algorithm is proposed that provides improved Speed-Power-Area-Accuracy (SPAA) metrics. The proposed ACM enables dynamic accuracy configurability via small error correction logic. Simulation results over accurate 8-bit multiplier indicate 57.37% and 25.17% reduced power and area, respectively. Moreover, accuracy configurability is achieved with only 10.5% and 12.32%, area and power overhead, respectively. Moreover, the proposed multiplier in real applications such as Gaussian smoothing filter attains better SPAA tradeoff over the existing approximate multipliers.","10.1007/s10836-017-5667-8","45414","0.2666667"
"64","cold_topic_cluster_no_11","rank_3","75","Error-Detection Enhanced Decoding of Difference Set Codes for Memory Applications","To prevent soft errors from causing data corruption, memories are typically protected with error correction codes (ECCs). For example, single-error correction (SEC) codes that can correct one error in a memory word are commonly used. More advanced ECCs are also used when additional protection is needed. While the error correction capability of a code is important, it is also important to detect errors that cannot be corrected to avoid silent data corruption. For example, in a SEC code, a double error may be misinterpreted as a single error and corrected, ending with an incorrect word. For that reason, codes that can also detect double errors are preferred. Those are known as SEC double-error-detection codes. The same reasoning applies to more advanced ECCs. Among those, difference set (DS) codes have shown interesting properties that enable an efficient implementation in terms of decoding latency and complexity. However, existing decoders for DS codes focus on error correction only, making them less attractive for memory protection. In this paper, modified decoding algorithms for DS codes are proposed that, in addition to error correction, provide error detection when the number of correctable bit errors is exceeded by one. This combined error detection and correction capability of the modified decoder makes the proposed scheme a promising option for memory applications. HDL implementation and synthesis results are included, showing that the proposed techniques can be efficiently implemented.","10.1109/TDMR.2012.2183873","77393","0.2666667"
"65","cold_topic_cluster_no_11","rank_3","75","Half-Quadratic-Based Iterative Minimization for Robust Sparse Representation","Robust sparse representation has shown significant potential in solving challenging problems in computer vision such as biometrics and visual surveillance. Although several robust sparse models have been proposed and promising results have been obtained, they are either for error correction or for error detection, and learning a general framework that systematically unifies these two aspects and explores their relation is still an open problem. In this paper, we develop a half-quadratic ( HQ) framework to solve the robust sparse representation problem. By defining different kinds of half-quadratic functions, the proposed HQ framework is applicable to performing both error correction and error detection. More specifically, by using the additive form of HQ, we propose an l(1)-regularized error correction method by iteratively recovering corrupted data from errors incurred by noises and outliers; by using the multiplicative form of HQ, we propose an l(1)-regularized error detection method by learning from uncorrupted data iteratively. We also show that the l(1)-regularization solved by soft-thresholding function has a dual relationship to Huber M-estimator, which theoretically guarantees the performance of robust sparse representation in terms of M-estimation. Experiments on robust face recognition under severe occlusion and corruption validate our framework and findings.","10.1109/TPAMI.2013.102","71590","0.2685393"
"66","cold_topic_cluster_no_11","rank_4","33","Augmentation of Noise Free Speech Recognizer using Adaptive Microphone Array","This paper describes the design and implementation of a 5-channel microphone array that is an adaptive beamformer used for hands-free telephony in a noisy environment. The microphone signals are amplified, and then sent to an A/D converter. The microprocessor board takes the data from the 5 channels and utilizes digital signal processing to determine the direction-of-arrival of the sources and create an output which steers the microphone array to the desired look direction while trying to minimize the energy of interference sources and noise.","","86514","0.2105263"
"67","cold_topic_cluster_no_11","rank_4","33","Coherent noise suppression by learning and analyzing the morphology of the data","We have developed a method for suppressing coherent noise from seismic data by using the morphological differences between the noise and the signal. This method consists of three steps: First, we applied a dictionary learning method on the data to extract a redundant dictionary in which the morphological diversity of the data is stored. Such a dictionary is a set of unit vectors called atoms that represent elementary patterns that are redundant in the data. Because the dictionary is learned on data contaminated by coherent noise, it is a mix of atoms representing signal patterns and atoms representing noise patterns. In the second step, we separate the noise atoms from the signal atoms using a statistical classification. Hence, the learned dictionary is divided into two subdictionaries: one describing the morphology of the noise and the other one describing the morphology of the signal. Finally, we separate the seismic signal and the coherent noise via morphological component analysis (MCA); it uses sparsity with respect to the two subdictionaries to identify the signal and the noise contributions in the mixture. Hence, the proposed method does not use prior information about the signal and the noise morphologies, but it entirely adapts to the signal and the noise of the data. It does not require a manual search for adequate transforms that may sparsify the signal and the noise, in contrast to existing MCA-based methods. We develop an application of the proposed method for removing the mechanical noise from a marine seismic data set. For mechanical noise that is coherent in space and time, the results show that our method provides better denoising in comparison with the standard FX-Decon, FX-Cadzow, and the curvelet-based denoising methods.","10.1190/GEO2017-0092.1","43501","0.2107843"
"68","cold_topic_cluster_no_11","rank_4","33","Noise adaptive switching median-based filter for impulse noise removal from extremely corrupted images","In this study an approach to impulse noise removal is presented. The introduced algorithm is a switching filter which identifies the noisy pixels and then corrects them by using median filter. In order to identify pixels corrupted by noise an analysis of local intensity extrema is applied. Comprehensive analysis of the algorithm performance [in terms of peak signal-to-noise ratio (PSNR) and Structural SIMilarity (SSIM) index] is presented. Results obtained on wide range of noise corruption (up to 98%) are shown and discussed. Moreover, comparison with well-established methods for impulse noise removal is provided. Presented results reveal that the proposed algorithm outperforms other approaches to impulse noise removal and its performance is close to ideal switching median filter. For high noise densities, the method correctly detects up to 100% of noisy pixels.","10.1049/iet-ipr.2009.0178","79773","0.2132075"
"69","cold_topic_cluster_no_11","rank_4","33","Cauchy greedy algorithm for robust sparse recovery and multiclass classification","Greedy algorithms have attracted considerable interest for sparse signal recovery (SSR) due to their appealing efficiency and performance recently. However, conventional greedy algorithms utilize the l(2) norm based loss function and suffer from severe performance degradation in the presence of gross corruption and outliers. Furthermore, they cannot be directly applied to the recovery of quaternion sparse signals due to the noncommutativity of quaternion multiplication. To alleviate these problems, we propose a robust greedy algorithm referred as Cauchy matching pursuit (CauchyMP) for SSR and extend it for quaternion SSR. By leveraging the Cauchy estimator and generalizing it to the quaternion space to measure the residual error, our method can robustly recover the sparse signal in both real and quaternion space from noisy data corrupted by various severe noises and outliers. To tackle the resulting quaternion optimization problem, we develop an efficient half-quadratic optimization algorithm by introducing two quaternion operators. In addition, we have also devised a CauchyMP based classifier termed CauchyMPC for robust multiclass classification. The experiments on both synthetic and real-world datasets validate the efficacy and robustness of the proposed methods for SSR, block SSR, quaternion SSR and multiclass classification. ","10.1016/j.sigpro.2019.06.006","20973","0.2218182"
"70","cold_topic_cluster_no_11","rank_4","33","Perceptual reinforcement of speech signal based on partial specific loudness","In the presence of background noise, the perceptual loudness of speech signal significantly decreases, resulting in the deterioration of intelligibility and clarity. In this letter, we propose a novel approach to enhance the perceived quality of the speech signal when the additive noise cannot be directly controlled. Instead of controlling the background noise, we propose to reinforce the speech signal so that it can be heard more clearly in noisy environments. To find a suitable reinforcement rule, the loudness perception model proposed by Moore et al. [1] is adopted. Experimental results show that the loudness of the reinforced signal can be maintained at the level almost the same as that of the original noise-free speech, and the proposed algorithm can enhance the perceived speech quality under various noise environments.","10.1109/LSP.2007.900222","86331","0.2384615"
"71","cold_topic_cluster_no_11","rank_5","485","Parameter Identification by Statistical Learning of a Stochastic Dynamical System Modelling a Fishery with price variation","In this short paper we report on an inverse problem for parameter setting of a model used for the modelling of fishing on the West African coast. We compare the solution of this inverse problem by a Neural Network with the more classical algorithms of optimisation and stochastic control. The Neural Network does much better.","10.5802/crmath.2","16899","0.1500000"
"72","cold_topic_cluster_no_11","rank_5","485","On a sustainability interval index and its computation through global optimization","Prior sustainability models have taken exact basic indicator (BI) values of an entity's sustainability and computed an exact value of the entity's sustainability index. However, BI values are either uncertain or difficult to obtain. In this work, we propose a novel approach that transforms BI data in the form of intervals containing all possible BI values to an interval containing all possible values of the sustainability index. This interval is termed the sustainability index interval (SII). Computation of the SII is achieved through solution of a minimization and a maximization problem using global optimization techniques. Although the underlying global optimization problems are nonconvex, they are shown to possess a number of properties that can be utilized to reduce the burden associated with SII's computation. Based on these properties, a branch-and-bound algorithm is developed, which exactly quantifies the SII in a finite number of iterations. ","10.1002/aic.12777","76879","0.1523810"
"73","cold_topic_cluster_no_11","rank_5","485","APPROXIMATE SOLUTION OF THE SMOOTH TRANSITION EQUATION","The problems of stability and the approximate solution of the integral smooth transition equation first introduced and studied by Yu.I. Chersky are considered. Using the solution of the smooth transition equation under classical assumptions, it is possible to construct the solution of the equation under weaker constraints on the kernels. For the approximate solution, an error estimation and a theorem on the uniqueness and sustainability are provided.","10.33048/semi.2020.17.125","16660","0.1714286"
"74","cold_topic_cluster_no_11","rank_5","485","Systematic Analysis of Alternative Splicing Landscape in Pancreatic Adenocarcinoma Reveals Regulatory Network Associated with Tumorigenesis and Immune Response","Background: Pancreatic ductal adenocarcinoma (PDAC) is one of the most aggressive gastrointestinal tumors and has an extremely high mortality rate. Recent studies indicate that alternative splicing (AS), a common post-transcriptional process, has important roles in tumor biological behaviors and may provide novel immunotherapeutic targets. This study systematically analyzes AS profiles in PDAC and reveals their potential regulatory effects on cancer immune response. Material/Methods: AS event, RNA sequencing, and splicing factor (SF) data were extracted from SpliceSeq, The Cancer Genome Atlas, and SpliceAid2, respectively. Overall survival (OS)-associated AS events and SFs were identified with univariate analysis. The LASSO method and multivariate Cox regression analysis were used to construct predictive signatures for the prediction of patient prognosis. The proportions of immune cells within PDAC samples were evaluated using the CIBERSORT algorithm. The correlations among AS events, SFs, and immune cell proportions were calculated using Spearman correlation analysis. Consensus clustering and immune classification were performed on the PDAC cohort. Results: A total of 4812 OS-related AS events from 3341 parent genes were identified, and 8 AS-based predictive models were constructed for PDAC. An OS-related SF-AS regulatory network was constructed. The AS events regulated by ELAVL4 exhibited strong correlations with CD8 T cells and regulatory T cells. In addition, AS-based clusters demonstrated distinct OS outcomes and immune features. Conclusions: AS-based predictive models with high accuracy were constructed to facilitate prognosis prediction and treatment of PDAC An SF-AS regulatory network was constructed, revealing the potential relationships among SF, AS, and immune response.","10.12659/MSM.925733"," 7840","0.1786765"
"75","cold_topic_cluster_no_11","rank_5","485","Sparse multichannel blind deconvolution","We developed a sparse multichannel blind deconvolution (SMBD) method. The method is a modification of the multichannel blind deconvolution technique often called Euclid deconvolution, in which the multichannel impulse response of the earth is estimated by solving an homogeneous system of equations. Classical Euclid deconvolution is unstable in the presence of noise and requires the correct estimation of the length of the seismic wavelet. The proposed method, on the other hand, can tolerate moderate levels of noise and does not require a priori knowledge of the length of the wavelet. SMBD solves the homogeneous system of equations arising in Euclid deconvolution by imposing sparsity on the unknown multichannel impulse response. Trivial solutions to the aforementioned homogeneous system of equations are avoided by seeking sparse solutions on the unit sphere. We tested SMBD with synthetic and real data examples. Synthetic examples were used to judge the viability of the method in terms of noise. We found that SMBD gives reasonable estimates of the wavelet and reflectivity series for S/N >= 4. The results clearly deteriorated when we tried to work on data that were severely contaminated by noise. A real marine data set was also used to test SMBD. In this case, the estimated wavelet was compared with a wavelet estimated by averaging first breaks. The estimated wavelet showed a noticeable resemblance to the average first break with normalized correlation coefficient of 0.92.","10.1190/GEO2013-0465.1","69364","0.3044643"
"76","cold_topic_cluster_no_12","rank_1","243","Assimilation of the satellite SST data in the 3D CEMBS model","The 3D CEMBS (3D Coupled Ecosystem Model of the Baltic Sea) is a coupled ecosystem model of the Baltic Sea. In operational mode it computes 48-h forecasts of the hydrodynamic and biochemical parameters describing the Baltic Sea state. The Cressman assimilation scheme was implemented as part of the system in order to improve overall model accuracy. The system uses satellite-measured sea surface temperature from the MODIS Aqua spectroradiometer for the assimilation process. The satellite measured SST is obtained from a predefined server, which is part of the Satellite Monitoring of the Baltic Sea Environment project (SatBattyk). To validate the model results and the impact of assimilation on the model's accuracy, two separate test runs were performed using historical data covering the years 2011 and 2012. Independent computations were performed for the model with and without satellite SST assimilation, respectively referred to in this paper as 3D CEMBS_A and 3D CEMBS. The results of the computations were then compared with satellite and in situ measured data to validate the model and the assimilation scheme's implementation. The objective of this paper is to describe the implementation of the satellite SST data assimilation algorithm and to present the results of the preliminary validation of the models with observations. ","10.1016/j.oceano.2014.07.001","68187","0.2210526"
"77","cold_topic_cluster_no_12","rank_1","243","Single star Doppler passive positioning accuracy analysis and processing based on sea state sensor","Marine state information is important in many areas of research, e.g. marine monitoring, marine weather forecast and climate forewarning, and this information can be collected by ocean buoys. In the process of measurement, Doppler passive positioning is used to determine the position of the buoy. This paper aims to identify the factors that influence the buoy positioning accuracy. It is derived from the error formula that the positioning accuracy of the buoy is related to the relative position error between satellite and buoy, satellite orbit error, and frequency measurement error. In the simulation, satellite ephemeris data are generated, the iterative location algorithm is used to get the location of buoy, and noises are analyzed at last. We find that the relative position and frequency measurement error have a significant impact on the positioning accuracy of the buoy. Therefore, in order to improve the buoy positioning accuracy, it is necessary to select a satellite position that is near the radiation source but not directly above it. Improving the frequency measurement accuracy will also greatly enhance the positioning accuracy. ","10.1016/j.measurement.2020.107555","13024","0.2269231"
"78","cold_topic_cluster_no_12","rank_1","243","Assimilation of SeaWiFS data into a global ocean-biogeochemical model using a local SEIK filter","Chlorophyll data from the Sea-viewing Wide Field-of-view Sensor (SeaWiFS) is assimilated into the three-dimensional global NASA Ocean Biogeochemical Model (NOBM) for the period 1998-2004 in order to obtain an improved representation of chlorophyll in the model. The assimilation is performed by the SEIK filter, which is based on the Kalman filter algorithm. The filter is implemented to univariately correct the concentration of surface total chlorophyll. A localized filter analysis is used and the filter is simplified by using a static state error covariance matrix. The assimilation provides daily global surface chlorophyll fields and improves the chlorophyll estimates relative to a model simulation without assimilation. The comparison with independent in situ data over the seven years also shows a significant improvement of the chlorophyll estimate. The assimilation reduces the RMS log error of total chlorophyll from 0.43 to 0.32, while the RMS log error is 0.28 for the in situ data considered. That is, the global RMS log error of chlorophyll estimated by the model is reduced by the assimilation from 53% to 13% above the error of SeaWiFS. Regionally, the assimilation estimate exhibits smaller errors than SeaWiFS data in several oceanic basins. ","10.1016/j.jmarsys.2006.11.009","86351","0.2313953"
"79","cold_topic_cluster_no_12","rank_1","243","Comparison and assimilation of global soil moisture retrievals from the Advanced Microwave Scanning Radiometer for the Earth Observing System (AMSR-E) and the Scanning Multichannel Microwave Radiometer (SMMR)","[1] Two data sets of satellite surface soil moisture retrievals are first compared and then assimilated into the NASA Catchment land surface model. The first satellite data set is derived from 4 years of X-band (10.7 GHz) passive microwave brightness temperature observations by the Advanced Microwave Scanning Radiometer for the Earth Observing System (AMSR-E), and the second is from 9 years of C-band (6.6 GHz) brightness temperature observations by the Scanning Multichannel Microwave Radiometer (SMMR). Despite the similarity in the satellite instruments, the retrieved soil moisture data exhibit very large differences in their multiyear means and temporal variability, primarily because they are computed with different retrieval algorithms. The satellite retrievals are also compared to a soil moisture product generated by the NASA Catchment land surface model when driven with surface meteorological data derived from observations. The climatologies of both satellite data sets are different from those of the model products. Prior to assimilation of the satellite retrievals into the land model, satellite-model biases are removed by scaling the satellite retrievals into the land model's climatology through matching of the respective cumulative distribution functions. Validation against in situ data shows that for both data sets the soil moisture fields from the assimilation are superior to either satellite data or model data alone. A global analysis of the innovations ( defined as the difference between the observations and the corresponding model values prior to the assimilation update) reveals how changes in model and observations error parameters may enhance filter performance in future experiments.","10.1029/2006JD008033","86797","0.2898148"
"80","cold_topic_cluster_no_12","rank_1","243","1st Snow Data Assimilation Workshop in the framework of COST HarmoSnow ESSEM 1404","The 1st Snow Data Assimilation Workshop, organized under the COST Action ESSEM 1404 HarmoSnow, took place in Offenbach, Germany, on 8-9 March 2017. Of particular relevance for the workshop were thematic sessions on i) data assimilation methods and the use of snow observations, ii) snow observations and evaluation, iii) snow observations and physical snow models, and iv) snow observations and hydrological models. This report summarizes the scientific contributions presented at the workshop. The discussions mainly focused on methods for combining satellite observations with conventional in-situ snow measurements and modeling results, as well as on errors in the spatial and temporal representation of snow measurements for data assimilation in NWP and hydrological models. It has been shown that the assimilation of in-situ and satellite-based snow observations improves the quality of the snow analysis and forecast. However, in order to achieve this positive impact, a thorough quality control of the observational data is necessary, in particular because of the automation of the ground-based networks.","10.1127/metz/2018/0906","41041","0.3119403"
"81","cold_topic_cluster_no_12","rank_2","55","A Framework for Estimating Clear-Sky Atmospheric Total Precipitable Water (TPW) from VIIRS/S-NPP","Atmospheric water vapor content or total precipitable water (TPW) is a highly variable atmospheric constituent, yet it remains one of the meteorological parameters that is most difficult to characterize accurately. We develop a framework for estimating atmospheric TPW from Visible Infrared Imaging Radiometer Suite (VIIRS) data in this study. First, TPW is retrieved from VIIRS top-of-atmosphere (TOA) radiance of channels 15 and 16 using the refined split-window covariance-variance ratio (SWCVR) method. Then, the VIIRS TPW is blended with the microwave integrated retrieval system (MIRS) derived TPW via Bayesian model averaging (BMA) to improve the accuracy of VIIRS TPW. Three years (2014-2017) of ground measurements collected from SuomiNet sites over North America are used to validate the VIIRS TPW and blended TPW. The mean bias error (MBE) and root mean square error (RMSE) of the VIIRS TPW are 0.21 g/cm(2) and 0.73 g/cm(2), respectively, and the accuracy of the VIIRS TPW in daytime is much better than at night time. The MBE and RMSE of BMA integrated TPW are 0.06 g/cm(2) and 0.35 g/cm(2), and the accuracy difference between daytime and nighttime is also removed. The global radiosonde measurements are also collected to validate the BMA integrated VIIRS TPW. The MBE and RMSE of the BMA integrated TPW are 0.09 g/cm(2) and 0.44 g/cm(2) compared to the radiosonde measurements. This accuracy is also superior to the VIIRS TPW. Therefore, it is concluded that the developed framework can be used to derive accurate clear-sky TPW for VIIRS. This is the first time that we can obtain high accuracy TPW from VIIRS. This study will certainly benefit the study of atmospheric processes and climate change.","10.3390/rs11080916","27309","0.2622047"
"82","cold_topic_cluster_no_12","rank_2","55","An Overview of the Science Performances and Calibration/Validation of Joint Polar Satellite System Operational Products","The Suomi National Polar-orbiting Partnership (S-NPP) satellite, launched in October 2011, initiated a series of the next-generation weather satellites for the National Oceanic and Atmospheric Administration (NOAA) Joint Polar Satellite System (JPSS) program. The JPSS program at the Center for Satellite Applications and Research (JSTAR) leads the development of the algorithms, the calibration and validation of the products to meet the specified requirements, and long-term science performance monitoring and maintenance. All of the S-NPP products have been validated and are in successful operation. The recently launched JPSS-1 (renamed as NOAA-20) satellite is producing high-quality data products that have been available from S-NPP, along with additional products, as a direct result of the instrument upgrades and science improvements. This paper presents an overview of the JPSS product suite, the performance metrics achieved for the S-NPP, and the utilization of the products by NOAA stakeholders and user agencies worldwide. The status of NOAA-20 science data products and ongoing calibration/validation (Cal/Val) efforts are discussed for user awareness. In addition, operational implementation statuses of JPSS enterprise (multisensor and multiplatform) science algorithms for product generation and science product reprocessing efforts for the S-NPP mission are discussed.","10.3390/rs11060698","28301","0.2657407"
"83","cold_topic_cluster_no_12","rank_2","55","Intercomparisons of Cloud Mask Products Among Fengyun-4A, Himawari-8, and MODIS","In this paper, we developed a unified and operational cloud mask algorithm for the new-generation geostationary (GEO) meteorological satellite imagers of the Advanced Geostationary Radiation Imager (AGRI) aboard Fengyun-4A (FY-4A) and the Advanced Himawari Imager (AHI) aboard Himawari-8 (H08). We investigated the all-round performance of the cloud mask algorithm. Spatiotemporally, the algorithm matches the official Collection-6 cloud mask products of a Moderate Resolution Imaging Spectroradiometer (MODIS) from both the Terra and Aqua platforms, which we employed as the benchmark for performing intercomparisons and validations. The robust cloud mask algorithm can show high consistency between FY-4A/AGRI and H08/AHI. The MODIS-based validation results suggest that cloudy scene identification is better than that observed for clear skies for both FY-4A/AGRI and H08/AHI; there is also a relatively low false-alarm ratio (FAR). Moreover, the algorithm is more reliable during daytime hours, with a hit rate (HR) of approximately 92 for both FY-4A/AGRI and H08/AHI. We found slightly higher accuracy in cloud-masking results over water than those over land. Furthermore, we found that more than 67 of the matched pixels for both advanced GEO imagers had no bias when taking MODIS as the benchmark. Overall, HR values were approximately 91.04 and 91.82 for FY-4A/AGRI and H08/AHI, respectively. These results confirm the high quality of the algorithm for retrieving real-time cloud mask products.","10.1109/TGRS.2019.2923247","20327","0.2704082"
"84","cold_topic_cluster_no_12","rank_2","55","Operational cloud classification for the Iberian Peninsula using Meteosat Second Generation and AQUA-AIRS image fusion","The aim of this work was the adaptation and improvement of a previous cloud detection and classification algorithm that was developed for the Meteosat-7 satellite. The functions of this satellite have now been taken on by the new series of Meteosat Second Generation (MSG) satellites, which are not just replicas but new, much improved versions of their predecessor. The formerly used Advanced/Tiros-N Operational Vertical Sounder (A/TOVS) probe has also been superseded technologically by new sensors with better spatial resolution, capable of carrying out more accurate measurements at a greater number of wavelengths. This is the case of the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor onboard the TERRA and AQUA satellites and of the Atmospheric Infrared Sounder (AIRS) probe. In this context, new potential improvements are analysed for this algorithm by using these new platforms and sensors and the results are compared to those obtained in the first classification.","10.1080/01431160902882553","83043","0.2784314"
"85","cold_topic_cluster_no_12","rank_2","55","Radiometric Intercomparison between Suomi-NPP VIIRS and Aqua MODIS Reflective Solar Bands Using Simultaneous Nadir Overpass in the Low Latitudes","On-orbit radiometric performance of the Suomi National Polar-Orbiting Partnership (Suomi-NPP) Visible Infrared Imaging Radiometer Suite (VIIRS) is studied using the extended simultaneous nadir overpass (SNO-x) approach. Unlike the traditional SNO analysis of data in the high latitudes, this study extends the analysis to the low latitudesin particular, over desert and ocean sites with relatively stable and homogeneous radiometric propertiesfor intersatellite comparisons. This approach utilizes a pixel-by-pixel match with an efficient geospatial matching algorithm to map VIIRS data into the Moderate Resolution Imaging Spectroradiometer (MODIS). VIIRS moderate-resolution bands M-1 through M-8 are compared with Aqua MODIS equivalent bands to quantify radiometric bias over the North African desert and over the ocean. Biases exist between VIIRS and MODIS in several bands, primarily because of spectral differences as well as possible calibration uncertainties, residual cloud contamination, and bidirectional reflectance distribution function (BRDF). The impact of spectral differences on bias is quantified by using the Moderate Resolution Atmospheric Transmission (MODTRAN) and hyperspectral measurements from the Earth Observing-1 (EO-1) Hyperion and the Airborne Visible and Infrared Imaging Spectrometer (AVIRIS). After accounting for spectral differences and bias uncertainties, the VIIRS radiometric bias over desert agrees with MODIS measurements within 2% except for the VIIRS shortwave infrared (SWIR) band M-8, which indicates a nearly 3% bias. Over ocean, VIIRS agrees with MODIS within 2% by the end of January 2013 with uncertainty less than 1%. Furthermore, VIIRS bias relative to MODIS is also computed at the Antarctica Dome C site for validation and the result agrees well within 1% with the bias estimated using SNO-x over desert.","10.1175/JTECH-D-13-00071.1","72655","0.2804511"
"86","cold_topic_cluster_no_12","rank_3","696","A long-term Global LAnd Surface Satellite (GLASS) data-set for environmental studies","Recently, five Global LAnd Surface Satellite (GLASS) products have been released: leaf area index (LAI), shortwave broadband albedo, longwave broadband emissivity, incident short radiation, and photosynthetically active radiation (PAR). The first three products cover the years 1982-2012 (LAI) and 1981-2010 (albedo and emissivity) at 1-5 km and 8-day resolutions, and the last two radiation products span the period 2008-2010 at 5 km and 3-h resolutions. These products have been evaluated and validated, and the preliminary results indicate that they are of higher quality and accuracy than the existing products. In particular, the first three products have much longer time series, and are therefore highly suitable for various environmental studies. This paper outlines the algorithms, product characteristics, preliminary validation results, potential applications and some examples of initial analysis of these products.","10.1080/17538947.2013.805262","72572","0.2596154"
"87","cold_topic_cluster_no_12","rank_3","696","COMPARISON OF THE SURFACE SOLAR-RADIATION BUDGET DERIVED FROM SATELLITE DATA WITH THAT SIMULATED BY THE NCAR CCM2","A comparison of the monthly mean shortwave surface radiation budget (SRB) obtained from the World Climate Research Program (WCRP) shortwave global dataset with that simulated by the National Center for Atmospheric Research Community Climate Model version 2.0 (CCM2) is presented. WCRP/SRB data are derived from the international Satellite Cloud Climatology Project (ISCCP) C1 data using the Pinker algorithm. Large differences are found in monthly mean surface solar fluxes. The largest discrepancies are found in the summer midlatitude regions where CCM2 overestimates surface solar fluxes relative to Pinker by as much as 100 W m(-2). Most of the differences are associated with deficiencies in CCM2's prediction of cloud optical properties and cloud amount. However, significant differences also occur in clear-sky fluxes and surface albedo.","10.1175/1520-0442(1995)008<2824:COTSSR>2.0.CO;2","92335","0.2725806"
"88","cold_topic_cluster_no_12","rank_3","696","Theoretical uncertainties for global satellite-derived burned area estimates","Quantitative information on the error properties of global satellite-derived burned area (BA) products is essential for evaluating the quality of these products, e.g. against modelled BA estimates. We estimate theoretical uncertainties for three widely used global satellite-derived BA products using a multiplicative triple collocation error model. The approach provides spatially unique uncertainties at 1 degrees for the Moderate Resolution Imaging Spectroradiometer (MODIS) Collection 6 burned area product (MCD64), the MODIS Collection 5.1 (MCD45) product, and the European Space Agency (ESA) Climate Change Initiative Fire product version 5.0 (FireCCI50) for 2001-2013. The uncertainties on mean global burned area for three products are 3.76 +/- 0.15 x 10(6) km(2) for MCD64, 3.70 +/- 0.17 x 10(6) km(2) for FireCCI50, and 3.31 +/- 0.18 x 10(6) km(2) for MCD45. These correspond to relative uncertainties of 4 %-5.5% and also indicate previous uncertainty estimates to be underestimated. Relative uncertainties are 8 %-10% in Africa and Australia, for example, and larger in regions with less annual burned area. The method provides uncertainties that are likely to be more consistent with modelling and data analysis studies due to their spatially explicit properties. These properties are also intended to allow spatially explicit validation of current burned area products.","10.5194/bg-16-3147-2019","23287","0.2827586"
"89","cold_topic_cluster_no_12","rank_3","696","Hydrological Utility and Uncertainty of Multi-Satellite Precipitation Products in the Mountainous Region of South Korea","Satellite-derived precipitation can be a potential source of forcing data for assessing water availability and managing water supply in mountainous regions of East Asia. This study investigates the hydrological utility of satellite-derived precipitation and uncertainties attributed to error propagation of satellite products in hydrological modeling. To this end, four satellite precipitation products (tropical rainfall measuring mission (TRMM) multi-satellite precipitation analysis (TMPA) version 6 (TMPAv6) and version 7 (TMPAv7), the global satellite mapping of precipitation (GSMaP), and the climate prediction center (CPC) morphing technique (CMORPH)) were integrated into a physically-based hydrologic model for the mountainous region of South Korea. The satellite precipitation products displayed different levels of accuracy when compared to the intra-and inter-annual variations of ground-gauged precipitation. As compared to the GSMaP and CMORPH products, superior performances were seen when the TMPA products were used within streamflow simulations. Significant dry (negative) biases in the GSMaP and CMORPH products led to large underestimates of streamflow during wet-summer seasons. Although the TMPA products displayed a good level of performance for hydrologic modeling, there were some over/underestimates of precipitation by satellites during the winter season that were induced by snow accumulation and snowmelt processes. These differences resulted in streamflow simulation uncertainties during the winter and spring seasons. This study highlights the crucial need to understand hydrological uncertainties from satellite-derived precipitation for improved water resource management and planning in mountainous basins. Furthermore, it is suggested that a reliable snowfall detection algorithm is necessary for the new global precipitation measurement (GPM) mission.","10.3390/rs8070608","58416","0.2895522"
"90","cold_topic_cluster_no_12","rank_3","696","Evaluation of high-resolution satellite precipitation products using rain gauge observations over the Tibetan Plateau","High-resolution satellite precipitation products are very attractive for studying the hydrologic processes in mountainous areas where rain gauges are generally sparse. Four high-resolution satellite precipitation products are evaluated using gauge measurements over different climate zones of the Tibetan Plateau (TP) within a 6 yr period from 2004 to 2009. The four satellite-based precipitation data sets are: Tropical Rainfall Measuring Mission (TRMM) Multisatellite Precipitation Analysis 3B42 version 6 (TMPA) and its Real Time version (TMPART), Climate Prediction Center Morphing Technique (CMOPRH) and Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network (PERSIANN). TMPA and CMORPH, with higher correlation coefficients and lower root mean square errors (RMSEs), show overall better performance than PERSIANN and TMPART. TMPA has the lowest biases among the four precipitation data sets, which is likely due to the correction process against the monthly gauge observations from global precipitation climatology project (GPCP). TMPA also shows large improvement over TMPART, indicating the importance of gauge-based correction on accuracy of rainfall. The four products show better agreement with gauge measurements over humid regions than that over arid regions where correlation coefficients are less than 0.5. Moreover, the four precipitation products generally tend to overestimate light rainfall (0-10 mm) and underestimate moderate and heavy rainfall (> 10 mm). Moreover, this study extracts 24 topographic variables from a DEM (digital elevation model) and uses a linear regression model to explore the bias-topography relationship. Results show that biases of TMPA and CMORPH present weak dependence on topography. However, biases of TMPART and PERSIANN present dependence on topography and variability of elevation and surface roughness plays important roles in explaining their biases.","10.5194/hess-17-837-2013","75703","0.2938596"
"91","cold_topic_cluster_no_12","rank_4","165","SMART INTERPOLATION OF ANNUALLY AVERAGED AIR-TEMPERATURE IN THE UNITED-STATES","Two ''smart'' interpolation procedures are presented and assessed with respect to their ability to estimate annual-average air temperatures at unsampled points in space from available station averages. Smart approaches examined here improve upon commonly used procedures in that they incorporate spatially high-resolution digital elevation information, an average environmental lapse rate, and/or another higher-resolution longer-term average temperature field. Two other straightforward or commonly used interpolation methods also are presented and evaluated as benchmarks to which the smart interpolators can be compared. Interpolation from a spatially high-resolution, long-term-average air temperature climatology serves as a first approximation, while ''traditional'' interpolation (from a single realization of annual average air temperature on a single station network) is the other benchmark. Traditional interpolation continues to be the most commonly used interpolation approach within many of the atmospheric and environmental sciences. Smart approaches are significantly more accurate than either traditional methods or estimates spatially interpolated from a high-resolution climatology alone. A smart interpolation method that makes combined use of a digital elevation model(DEM) and traditional interpolation was nearly 24% more accurate than traditional interpolation by itself. Average error associated with this DEM-assisted interpolation algorithm, for interpolating yearly average air temperatures in the United States, was 0.44 degrees C. The other smart method that was evaluated combines DEM information with a high-resolution average air temperature field. It was even more accurate, as expressed in an overall average interpolation error of only 0.38 degrees C per year, which makes it some 34% more accurate than traditional interpolation. It is likely that the performance of smart interpolation, relative to traditional interpolation, will be even better when used with relatively sparse station networks.","10.1175/1520-0450(1995)034<2577:SIOAAA>2.0.CO;2","92317","0.2233871"
"92","cold_topic_cluster_no_12","rank_4","165","Detailed analysis of the error associated with the rainfall retrieved by the NOAA/NESDIS Special Sensor Microwave/Imager algorithm 2. Rainfall over land","[1] This study has investigated the error associated with the estimation of instantaneous areal rain rate over land by the NOAA/NESDIS Office of Research and Application's Special Sensor Microwave/Imager (SSM/I) algorithm. By comparing with ground-based rain observations from the Oklahoma Mesonet rain gage network, the error inherent in the SSM/I-based areal rain estimate is quantified. Since the ground-based gauges use point measurements to estimate areal rain, the gauges do not make a perfect estimate and contain an associated error. A formulation has been developed by which the difference between the SSM/I- and Mesonet-based estimates is divided into two parts: one due to the error in the Mesonet and the other due to the error in the SSM/I. A separate formulation has also been developed by which the error in Mesonet-based rain measurement is quantified according to the gage density and the statistics of rain (variance and spatial correlation). A nine-month data set of 15-min rain accumulation for over 100 gages from the Mesonet has been used to obtain the rain statistics and to compare with the SSM/I estimate over various spatial scales. Results show that the error for the instantaneous SSM/I rain rate over 0.5, 1.0, and 2.5-degree boxes are approximately 150%, 100%, and 70% of the mean areal rain rate, respectively.","10.1029/2001JD001172","90052","0.2255556"
"93","cold_topic_cluster_no_12","rank_4","165","Secular gravity variation at Svalbard (Norway) from ground observations and GRACE satellite data","P>The Svalbard archipelago, Norway, is affected by both the present-day ice melting (PDIM) and Glacial Isostatic Adjustment (GIA) subsequent to the Last Pleistocene deglaciation. The induced deformation of the Earth is observed by using different techniques. At the Geodetic Observatory in Ny-Alesund, precise positioning measurements have been collected since 1991, a superconducting gravimeter (SG) has been installed in 1999, and six campaigns of absolute gravity (AG) measurements were performed between 1998 and 2007. Moreover, the Gravity Recovery and Climate Experiment (GRACE) satellite mission provides the time variation of the Earth gravity field since 2002. The goal of this paper is to estimate the present rate of ice melting by combining geodetic observations of the gravity variation and uplift rate with geophysical modelling of both the GIA and Earth's response to the PDIM. We estimate the secular gravity variation by superimposing the SG series with the six AG measurements. We collect published estimates of the vertical velocity based on GPS and VLBI data. We analyse the GRACE solutions provided by three groups (CSR, GFZ, GRGS). The crux of the problem lies in the separation of the contributions from the GIA and PDIM to the Earth's deformation. To account for the GIA, we compute the response of viscoelastic Earth models having different radial structures of mantle viscosity to the deglaciation histories included in the models ICE-3G or ICE-5G. To account for the effect of PDIM, we compute the deformation of an elastic Earth model for six models of ice-melting extension and rates. Errors in the gravity variation and vertical velocity are estimated by taking into account the measurement uncertainties and the variability of the GRACE solutions and GIA and PDIM models. The ground observations agree with models that involve a current ice loss of 25 km3 water equivalent yr-1 over Svalbard, whereas the space observations give a value in the interval [5, 18] km3 water equivalent yr-1. A better modelling of the PDIM, which would include the precise topography of the glaciers and altitude-dependency of ice melting, is necessary to decrease the discrepancy between the two estimates.","10.1111/j.1365-246X.2010.04922.x","80525","0.2258503"
"94","cold_topic_cluster_no_12","rank_4","165","Optimal estimation of changes in the mass of ice sheets","We describe a new approach for estimating changes in ice sheet mass. Two methods are in common use: the ice budget and geodetic methods. The first makes use of separate estimates of the mass fluxes into and out of a domain, differencing them to obtain the local mass balance. The second estimates mass balance directly, using measurements of the change in surface elevation, often from aircraft or satellites. Here we combine ice budget and geodetic approaches to obtain an optimal estimate of mass balance. We seek maximum likelihood solutions for three terms: (1) the rate of change of surface elevation, (2) the rate of snow accumulation, and (3) the local divergence of the ice flux. These estimates are constrained to obey the continuity equation. We allow the location and temporal averaging interval of the estimates to be chosen arbitrarily. This approach can use all relevant measurements. The fidelity of any measurement is lowered by measurement error, and by fluctuations in each of the three terms driven by random year-to-year snowfall variations. We take full account of both error sources, weighting the data so as to minimize the confounding effect of these influences. Realistic covariance between randomly forced fluctuations are provided by a linearized model of ice sheet flow. We test the approach by applying the algorithm to synthetically generated measurements. The method performs better than either ice budget or geodetic methods applied in isolation, and has the important advantage that good estimates may still be derived when measurements appropriate to either technique are lacking or inaccurate.","10.1029/2003JF000021","89533","0.2420561"
"95","cold_topic_cluster_no_12","rank_4","165","Systematic errors between VLBI and GPS precipitable water vapor estimations from 5-year co-located measurements","Space geodetic techniques (e.g., Global Positioning System, GPS and very long baseline interference, VLBI) have been widely used to determine the precipitable water vapor (PWV) for meteorology and climatology, which was verified by comparing with co-located independent technique observations. However, most of these comparisons have been conducted using only short-time spanning observations at several stations. The goal of this study is to identify and quantify the systematic errors between VLBI and GPS PWV estimates using a 5-year (2002-2007), PWV data set constructed from co-located measurements and radiosonde data as well. it has found systematic errors between VLBI and GPS PWV estimations from comparisons with long-term co-located GPS measurements. The total mean VLBI PWVs are systematically smaller than GPS estimates with 0.8-2.2 mm for all sites, but can be as much as 15-30%. The subdiurnal PWV variation magnitudes and long-term trends between VLBI and GPS are nearly similar, but the VLBI-derived PWV trends are systematically smaller than GPS estimates with about 0.1+/-0.02 mm/year. These systematic errors in PWV estimates between VLBI and GPS are probably due to technique own problems, different used elevation angles and co-location separation. ","10.1016/j.jastp.2008.11.018","84437","0.2469880"
"96","cold_topic_cluster_no_12","rank_5","195","Retrieval of suspended particulate matter from turbidity - model development, validation, and application to MERIS data over the Baltic Sea","Suspended particulate matter (SPM) causes most of the scattering in natural waters and thus has a strong influence on the underwater light field, and consequently on the whole ecosystem. Turbidity is related to the concentration of SPM which usually is measured gravimetrically, a rather time-consuming method. Measuring turbidity is quick and easy, and therefore also more cost-effective. When derived from remote sensing data the method becomes even more cost-effective because of the good spatial resolution of satellite data and the synoptic capability of the method. Turbidity is also listed in the European Union's Marine Strategy Framework Directive as a supporting monitoring parameter, especially in the coastal zone. In this study, we aim to provide a new Baltic Sea algorithm to retrieve SPM concentration from in situ turbidity and investigate how this can be applied to satellite data. An in situ dataset was collected in Swedish coastal waters to develop a new SPM model. The model was then tested against independent datasets from both Swedish and Lithuanian coastal waters. Despite the optical variability in the datasets, SPM and turbidity were strongly correlated (r = 0.97). The developed model predicts SPM reliably from in situ turbidity (R-2 = 0.93) with a mean normalized bias (MNB) of 2.4% for the Swedish and 14.0% for the Lithuanian datasets, and a relative error (RMS) of 25.3% and 37.3%, respectively. In the validation dataset, turbidity ranged from 0.3 to 49.8 FNU (Formazin Nephelometric Unit) and correspondingly, SPM concentration ranged from 0.3 to 34.0 g m(-3) which covers the ranges typical for Baltic Sea waters. Next, the medium-resolution imaging spectrometer (MERIS) standard SPM product MERIS Ground Segment (MEGS) was tested on all available match-up data (n = 67). The correlation between SPM retrieved from MERIS and in situ SPM was strong for the Swedish dataset with r = 0.74 (RMS = 47.4 and MNB = 11.3%; n = 32) and very strong for the Lithuanian dataset with r = 0.94 (RMS = 29.5% and MNB = -1.5%; n = 35). Then, the turbidity was derived from the MERIS standard SPM product using the new in situ SPM model, but retrieving turbidity from SPM instead. The derived image was then compared to existing in situ data and showed to be in the right range of values for each sub-area. The new SPM model provides a robust and cost-efficient method to determine SPM from in situ turbidity measurements (or vice versa). The developed SPM model predicts SPM concentration with high quality despite the high coloured dissolved organic matter (CDOM) range in the Baltic Sea. By applying the developed SPM model to already existing remote sensing data (MERIS/Envisat) and most importantly to a new generation of satellite sensors (in particular OLCI on board the Sentinel-3), it is possible to derive turbidity for the Baltic Sea.","10.1080/01431161.2016.1230289","55376","0.2213483"
"97","cold_topic_cluster_no_12","rank_5","195","VIIRS-Derived Water Turbidity in the Great Lakes","Satellite ocean color products from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (SNPP) since 2012 and in situ water turbidity measurements from the U.S. Environmental Protection Agency's Great Lakes Environmental Database System are used to develop a water turbidity algorithm for satellite ocean color applications in the Great Lakes for water quality monitoring and assessments. Results show that the proposed regional algorithm can provide reasonably accurate estimations of water turbidity from satellite observations in the Great Lakes. Therefore, VIIRS-derived water turbidity data are used to investigate spatial and temporal variations in water turbidity for the entirety of the Great Lakes. Water turbidity values are overall the highest in Lake Erie, moderate in Lake Ontario, and relatively low in lakes Superior, Michigan, and Huron. Significantly high values in water turbidity appear in the nearshore regions, particularly in Thunder Bay (Lake Superior), Green Bay (Lake Michigan), and Saginaw Bay (Lake Huron). Seasonal patterns of water turbidity are generally similar in lakes Superior, Michigan, Huron, and Ontario, showing relatively high values in the spring and autumn months and lows in the winter season, while the seasonal pattern in Lake Erie is apparently different from the other lakes, with the highest value in the winter season and the lowest in the summer season. A strong interannual variability in water turbidity is shown in the time series of the VIIRS-derived water turbidity data for most of the lakes.","10.3390/rs11121448","25403","0.2315789"
"98","cold_topic_cluster_no_12","rank_5","195","Assessment of satellite-derived diffuse attenuation coefficients and euphotic depths in south Florida coastal waters","Optical data collected in coastal waters off South Florida and in the Caribbean Sea between January 2009 and December 2010 were used to evaluate products derived with three bio-optical inversion algorithms applied to MODIS/Aqua, MODIS/Terra, and SeaWiFS satellite observations. The products included the diffuse attenuation coefficient at 490 nm (K-d_490) and for the visible range (K-d_PAR), and euphotic depth (Z(eu), corresponding to 1% of the surface incident photosynthetically available radiation or PAR). Above-water hyperspectral reflectance data collected over optically shallow waters of the Florida Keys between June 1997 and August 2011 were used to help understand algorithm performance over optically shallow waters. The in situ data covered a variety of water types in South Florida and the Caribbean Sea, ranging from deep clear waters, turbid coastal waters, and optically shallow waters (K-d_490 range of similar to 0.03-1.29 m(-1)). An algorithm based on Inherent Optical Properties (IOPs) showed the best performance (RMSD<13% and R-2 similar to 1.0 for MODIS/Aqua and SeaWiFS). Two algorithms based on empirical regressions performed well for offshore clear waters, but underestimated K-d_490 and K-d_PAR in coastal waters due to high turbidity or shallow bottom contamination. Similar results were obtained when only in situ data were used to evaluate algorithm performance. The excellent agreement between satellite-derived remote sensing reflectance (R-rs) and in situ R-rs suggested that the different product uncertainties resulted primarily from algorithm inversion as opposed to atmospheric correction. A simple empirical model was developed to derive Z(eu), from K-d_490 for satellite measurements of nearshore waters. MODIS/Aqua gave the best results in general relative to in situ observations. Our findings lay the basis for synoptic time-series studies of water quality in coastal ecosystems, yet more work is required to minimize the bottom interference in the Florida Keys optically shallow waters. ","10.1016/j.rse.2012.12.009","74742","0.2423358"
"99","cold_topic_cluster_no_12","rank_5","195","Improved turbidity estimates in complex inland waters using combined NIR-SWIR atmospheric correction approach for Landsat 8 OLI data","Turbidity is one of the important water quality parameters, essentially a proxy to assess eutrophication state in inland coastal systems. In this article, a method of combined near-infrared-shortwave infrared (NIR-SWIR) atmospheric correction for Landsat 8 (L8) Operational Land Imager data is proposed to improve the turbidity retrieval in optically complex waters. From the extremely turbid to moderately turbid waters, the relative ranges in water-leaving reflectance in band 3 (rho(3)(w)) are found to be 19-92% and 31-79% in band 4 (rho(4)(w)). The SWIR reflectances in rho(4)(w) and rho(3)(w) are 57% and 66% higher than that of standard NIR correction in extremely turbid waters. However, this method has resulted in similar to 30% higher reflectances than the NIR method in relatively less turbid waters; the latter method is still good in moderately turbid waters. Using Rayleigh corrected reflectances, a turbidity index, T-ind (865; 1610), was computed to discriminate the productive and/or turbid waters. The SWIR method was applied for water having T-ind >1.5 threshold and the NIR method in the other regions. A new turbidity algorithm has been developed using L8 two band ratio (rho(4)(w) = rho(3)(w)) optimized with in situ turbidity data from four data buoys for 2014. The Landsat 8 band-weighted in situ reflectances for bands 3 and 4 are used to derive turbidity using the present algorithm and validated against in situ turbidity, providing a good coefficient of determination of R-2 = 0.87. As compared to the NIR-based correction, the turbidity obtained from the combined (NIR + SWIR) correction in extremely turbid waters is around 80-90% (absolute percentage difference (APD)) different. Whereas in the moderately turbid waters, the APD between the two corrections was around 50-75%. There are no obvious data discontinuities in using the combined approach. Comparisons were made with available single-band turbidity algorithms and found that the present turbidity algorithm performed well in the optically complex lagoon environment.","10.1080/01431161.2018.1471538","40474","0.2690647"
"100","cold_topic_cluster_no_12","rank_5","195","Improved MODIS-Aqua Chlorophyll-a Retrievals in the Turbid Semi-Enclosed Ariake Bay, Japan","The accurate retrieval of chlorophyll-a concentration (Chl-a) from ocean color satellite data is extremely challenging in turbid, optically complex coastal waters. Ariake Bay in Japan is a turbid semi-enclosed bay of great socio-economic significance, but it suffers from serious water quality problems, particularly due to red tide events. Chl-a derived from the MODerate resolution Imaging Spectroradiometer (MODIS) sensor on satellite Aqua in Ariake Bay was investigated, and it was determined that the causes of the errors were from inaccurate atmospheric correction and inappropriate in-water algorithms. To improve the accuracy of MODIS remote sensing reflectance (Rrs) in the blue and green bands, a simple method was adopted using in situ Rrs data. This method assumes that the error in MODIS Rrs(547) is small, and MODIS Rrs(412) can be estimated from MODIS Rrs(547) using a linear relation between in situ Rrs(412) and Rrs(547). We also showed that the standard MODIS Chl-a algorithm, OC3M, underestimated Chl-a, which was mostly due to water column turbidity. A new empirical switching algorithm was generated based on the relationship between in situ Chl-a and the blue-to-green band ratio, max(Rrs(443), Rrs(448)/Rrs(547), which was the same as the OC3M algorithm. The criterion of Rrs(667) of 0.005 sr(-1) was used to evaluate the extent of turbidity for the switching algorithm. The results showed that the switching algorithm performed better than OC3M, and the root mean square error (RMSE) of estimated Chl-a decreased from 0.414 to 0.326. The RMSE for MODIS Chl-a using the recalculated Rrs and the switching algorithm was 0.287, which was a significant improvement from the RMSE of 0.610, which was obtained using standard MODIS Chl-a. Finally, the accuracy of our method was tested with an independent dataset collected by the local Fisheries Research Institute, and the results revealed that the switching algorithm with the recalculated Rrs reduced the RMSE of MODIS Chl-a from 0.412 of the standard to 0.335.","10.3390/rs10091335","35104","0.2759124"
"101","cold_topic_cluster_no_2","rank_1","840","Nighttime ozone variability in the high latitude winter mesosphere","We use satellite observations and a numerical model to investigate polar nighttime ozone at the secondary maximum, around 90-95 km. Observations from the MIPAS and SABER satellite instruments indicate that the highest ozone mixing ratios are seen during the late fall to early winter period in both hemispheres and for all years examined. Simulations using the Whole Atmosphere Community Climate Model (WACCM) find qualitatively the same seasonal evolution. Analysis of WACCM results shows that the high ozone concentration is due in part to the relatively quiet dynamical conditions in early winter. The mean circulation, which brings warmer temperatures and higher concentrations of H, is weaker in early winter than during middle and late winter. H in the late fall to early winter period drops to the lowest levels seen during the year due to lack of a source from photochemistry, weak transport into the region by the mean circulation, and continual loss due to diffusive separation. The low concentration of H leads to higher ozone.","10.1002/2014JD021987","68306","0.1986842"
"102","cold_topic_cluster_no_2","rank_1","840","Trends and Variability of North Pacific Polar Lows","The 6-hourly 1948-2010 NCEP 1 reanalyses have been dynamically downscaled for the region of the North Pacific. With a detecting-and-tracking algorithm, the climatology of North Pacific Polar Lows has been constructed. This derived climatology is consistent with the limited observational evidence in terms of frequency and spatial distribution. The climatology exhibits strong year-to-year variability but weak decadal variability and a small positive trend. A canonical correlation analysis describes the conditioning of the formation of Polar Lows by characteristic seasonal mean flow regimes, which favor, or limit, cold air outbreaks and upper air troughs.","10.1155/2013/170387","75615","0.2000000"
"103","cold_topic_cluster_no_2","rank_1","840","Modelling multidecadal variability in the South Indian Ocean region: local forcing or a near-global mode?","Evidence for decadal to multidecadal climate variability has been found for many Southern Hemisphere regions, including the South Indian Ocean. Here we apply atmospheric and ocean general circulation models (GCM) to investigate the mechanisms potentially associated with a mode of multidecadal climate variability over the South Indian Ocean region. This mode is characterized by a strengthening and weakening of the South Indian Ocean anticyclone and basin-scale sea-surface temperature (SST) anomalies that reverse sign between the southern midlatitudes and the subtropics. It is suggested that the SST variability most likely results from forcing by local atmospheric changes, whereas the latter may arise as the manifestation over the Indian Ocean of a near-global multidecadal mode.","","91171","0.2020000"
"104","cold_topic_cluster_no_2","rank_1","840","Effect of tropospheric temperature change on the zonal mean circulation and SH winter extratropical cyclones","This study aims to understand the mechanisms which cause an overall reduction of SH extratropical cyclone activity with a slight increase in the high latitudes in a warmer climate simulated in general circulation models (GCMs) with increasing CO2. For this purpose, we conducted idealized model experiments by forcing warm temperature anomalies to the areas where climate change models exhibit local maximum warming-the tropics in the upper troposphere and the polar regions in the lower troposphere-simultaneously and separately. The Melbourne University atmospheric GCM (R21) coupled with prescribed SST was utilized for the experiments. Our results demonstrate that the reduction of SH extratropical cyclone frequency and depth in the midlatitudes but the slight increase in the high latitudes suggested in climate change models result essentially from the tropical upper tropospheric warming. With this tropical warming, the enhanced static stability which decreases baroclinicity in the low and midlatitudes turns out to be a major contributor to the decrease of cyclone activity equatorward of 45A degrees S whereas the increased meridional temperature gradient in the high latitudes seems an important mechanism for the increase of cyclone activity over 50A degrees aEuro""60A degrees S.","10.1007/s00382-008-0444-0","83871","0.2272727"
"105","cold_topic_cluster_no_2","rank_1","840","Enhanced poleward propagation of storms under climate change","Earth's midlatitudes are dominated by regions of large atmospheric weather variability-often referred to as storm tracks-which influence the distribution of temperature, precipitation and wind in the extratropics. Comprehensive climate models forced by increased greenhouse gas emissions suggest that under global warming the storm tracks shift poleward. While the poleward shift is a robust response across most models, there is currently no consensus on what the underlying dynamical mechanism is. Here we present a new perspective on the poleward shift, which is based on a Lagrangian view of the storm tracks. We show that in addition to a poleward shift in the genesis latitude of the storms, associated with the shift in baroclinicity, the latitudinal displacement of cyclonic storms increases under global warming. This is achieved by applying a storm-tracking algorithm to an ensemble of CMIP5 models. The increased latitudinal propagation in a warmer climate is shown to be a result of stronger upper-level winds and increased atmospheric water vapour. These changes in the propagation characteristics of the storms can have a significant impact on midlatitude climate.","10.1038/s41561-017-0001-8","43182","0.2480000"
"106","cold_topic_cluster_no_2","rank_2","597","Comparative Assessment of Volume Change in Kolahoi and Chhota Shigri Glaciers, Western Himalayas, Using Empirical Techniques","Volume change in glaciers across the world is an established fact; however, response of Himalayan glaciers to climate change is not well understood. Looking at the importance of volumetric information of glaciers, glaciologists use different methods of volume estimation e. g., geophysical methods including GPR surveys, seismic profiling and tomography; glaciological methods by installing stakes and empirically established scaling methods. However, considering the difficulty in field measurements associated with Himalayan glaciers, indirect assessment techniques are being used widely. The objective of this study was to compare the response of two most studied western Himalayan glaciers i. e. Kolahoi glacier and Chhota Shigri glacier over the last three to four decades using empirical relationships of glacier volume estimation. The factors such as glacier length, area and slope were extracted from satellite data and digital elevation models. Glacier area change for Kolahoi glacier and Chhota Shigri glacier was mapped from 1980 to 2015 using Landsat images. The analysis of data showed that the areal extent of glaciers has receded from 1980 to 2015. Three scaling laws applied in this study yield different volume estimates for the glaciers from two adjoining sub-basins of the Indus basin, western Himalaya. The glaciers reported in this study show -10% (Kolahoi glacier) and -2.6% (Chhota Shigri glacier) change in their length and -13.5% (Kolahoi glacier) and -2.1% changes in the areal surface during the study period of 35 years. Cumulative change in volume estimated using different methods was found to be comparable i. e., -18% for Kolahoi and -3% for Chhota Shigri glacier, during the study period. However, progressive change in volume of two glaciers shows slight difference-Kolahoi glacier is declining rapidly as compared to Chhota Shigri glacier. Incorporation of field measurements into satellite data analysis, and their comparison strengthens the estimates of pattern in ice volume variability. The study tried to identify parallels and differences in the melt response of two most studied glaciers in Indus basin, based on available satellite images.","10.3233/JCC-170004","49683","0.3669118"
"107","cold_topic_cluster_no_2","rank_2","597","Balanced conditions or slight mass gain of glaciers in the Lahaul and Spiti region (northern India, Himalaya) during the nineties preceded recent mass loss","The volume change of the Chhota Shigri Glacier (India, 32 degrees 20 N, 77 degrees 30' E) between 1988 and 2010 has been determined using in situ geodetic measurements. This glacier has experienced only a slight mass loss between 1988 and 2010 (-3.8 +/- 2.0mw.e. (water equivalent) corresponding to -0.17 pm 0.09mw.e. yr(-1)). Using satellite digital elevation models (DEM) differencing and field measurements, we measure a negative mass balance (MB) between 1999 and 2010 (-4.8 +/- 1.8mw.e. corresponding to -0.44 +/- 0.16mw.e. yr(-1)). Thus, we deduce a slightly positive or near-zero MB between 1988 and 1999 (+1.0 +/- 2.7mw.e. corresponding to +0.09 +/- 0.24mw.e. yr(-1)). Furthermore, satellite DEM differencing reveals that the MB of the Chhota Shigri Glacier (-0.39 pm 0.15mw.e. yr(-1)) has been only slightly less negative than the MB of a 2110 km(2) glaciarized area in the Lahaul and Spiti region (-0.44 +/- 0.09mw.e. yr(-1)) during 1999-2011. Hence, we conclude that the ice wastage is probably moderate in this region over the last 22 yr, with near equilibrium conditions during the nineties, and an ice mass loss after. The turning point from balanced to negative mass budget is not known but lies probably in the late nineties and at the latest in 1999. This positive or near-zero MB for Chhota Shigri Glacier (and probably for the surrounding glaciers of the Lahaul and Spiti region) during at least part of the 1990s contrasts with a recent compilation of MB data in the Himalayan range that indicated ice wastage since 1975. However, in agreement with this compilation, we confirm more negative balances since the beginning of the 21st century.","10.5194/tc-7-569-2013","75702","0.3704545"
"108","cold_topic_cluster_no_2","rank_2","597","Heterogeneous Influence of Glacier Morphology on the Mass Balance Variability in High Mountain Asia","We investigate the control of the morphological variables on the 2000-2016 glacier-wide mass balances of 6,470 individual glaciers of High Mountain Asia. We separate the data set into 12 regions assumed to be climatically homogeneous. We find that the slope of the glacier tongue, mean glacier elevation, percentage of supraglacial debris cover, and avalanche contributing area all together explain a maximum of 48% and a minimum of 8% of the glacier-wide mass balance variability, within a given region. The best predictors of the glacier-wide mass balance are the slope of the glacier tongue and the mean glacier elevation for most regions, with the notable exception of the inner Tibetan Plateau. Glacier-wide mass balances do not differ significantly between debris-free and debris-covered glaciers in 7 of the 12 regions analyzed. Lake-terminating glaciers have more negative mass balances than the regional averages, the influence of lakes being stronger on small glaciers than on large glaciers.","10.1029/2018JF004838","25548","0.3855072"
"109","cold_topic_cluster_no_2","rank_2","597","Assessment of Changes in Mass Balance of the Tuyuksu Group of Glaciers, Northern Tien Shan, Between 1958 and 2016 Using Ground-Based Observations and Pleiades Satellite Imagery","Continuous measurements of glaciological mass balance have been conducted at the Central Tuyuksu glacier, Tuyuksu group of glaciers, Ile Alatau, northern Tien Shan since 1957, showing that cumulative mass balance was negative since the 1970s. Geodetic mass balance was calculated for the 1958-1998 and 1998-2016 periods using multi-temporal digital elevation models derived from the historic photogrammetric surveys from 1958 and 1998 and the high-resolution Pleiades satellite stereo imagery from 2016. The geodetic measurements revealed a mean surface lowering of 23.2 +/- 2.2 m (0.40 +/- 0.04 m a(-1)) and a reduction in volume of (67.7 +/- 6.7) x 10(6)m(3)in 1958-2016 at the Central Tuyuksu glacier, yielding a geodetic mass balance of -21.8 +/- 2.6 m w.e. Similar trends were observed at other glaciers of the Tuyuksu group, which lost in total 83.4 x 10(6)m(3)of ice. The mass balance annual rates have not changed significantly from 1958-1998 (-0.39 +/- 0.05 m w.e. a(-1)) to 1998-2016 (-0.35 +/- 0.18 m w.e. a(-1)) at the Central Tuyuksu and at other glaciers of the Tuyuksu group whose maximum elevations exceed 4,000 m a.s.l. While glacier thinning intensified in the ablation zone and affected a larger area in 1998-2016, extending to 3,600-3,700 m a.s.l., the accumulation increased at higher elevations in 1998-2016. Geodetic mass balance was more negative in 1998-2016 than in 1958-1998 at the smaller glaciers with lower maximum elevations. At the Central Tuyuksu, the geodetic mass balance was in close agreement with the glaciological mass balance, particularly in 1958-1998 when the difference between the geodetic and the cumulative glaciological mass balance values did not exceed 5%. During 1998-2016, this difference increased to 14%, with the glaciological method producing a more negative mass balance. This discrepancy was attributed to a systematic bias introduced by the lack of stakes in the accumulation zone of Central Tuyuksu whose contribution to uncertainty increased in 1998-2016 in line with an increase in accumulation. The negative mass balance of the Tuyuksu group of glaciers was attributed to a continuing increase in summer temperatures and a low accumulation observed in the 1970-1980s and at the turn of the century.","10.3389/feart.2020.00259"," 7764","0.3951220"
"110","cold_topic_cluster_no_2","rank_2","597","Glacier changes in Glacier Bay, Alaska, during 2000-2012","This article presents measurements of glacier surface areas, mean snow line altitude (MSLA) values, mean snow accumulation area ratio (MAAR) values, and elevation changes in the Glacier Bay, Alaska, using Landsat Thematic Mapper (TM)/Landsat Enhanced Thematic Mapper Plus (ETM+) images and digital elevation models (DEMs) from Shuttle Radar Topography Mission (SRTM) and interferometric synthetic aperture radar (IFSAR) data during 2000-2012. Glacier area estimation results showed that Desolation glacier and Fairweather glacier have lost 2.6% and 2.2% of the glacier area, respectively. Only minor surface area changes were seen in Cascade glacier, Crillon glacier, and Lituya glacier during the study period. The results of MSLA and MAAR showed that the MSLA of Fairweather glacier, Lituya glacier, and Desolation glacier increased by about 120-289 m and the MAAR of Fairweather glacier, Lituya glacier, and Desolation glacier decreased by about 3-6%. In contrast, MSLA and MAAR of Crillon glacier decreased by about 70 m and increased by about 1%, respectively. Glacier elevation change results showed that 7.7 m, 4.6 m and 1.5 m of mean thinning change were observed, respectively, on Fairweather glacier, Lituya glacier, and Desolation glacier. However, 7 m and 0.65 m of mean thickening were, respectively, experienced on Cascade glacier and Crillon glacier in the same period. Results from the study indicated that glacier retreat (Fairweather glacier, Lituya glacier, and Desolation glacier) affected by higher temperatures probably dominates with over-increased precipitation. However, increasing debris cover on the glacier surface can also modify the glacier dynamic, resulting in a different response to global warming.","10.1080/01431161.2016.1207267","61676","0.4686869"
"111","cold_topic_cluster_no_2","rank_3","82","Lateral variation in sandstone lithofacies from conventional core, Scotian Basin: implications for reservoir quality and connectivity","Facies analysis in outcrops on land is strongly dependent on the lateral variability of lithofacies. Interpretation of conventional core in wells relies principally on the vertical succession of lithofacies. To better understand depositional environments and reservoir sandstone connectivity, the lithofacies of reservoir sandstones sampled in conventional core were correlated laterally through two sets of closely spaced wells in the Scotian Basin: in the Barremian-Albian succession around the Panuke-Cohasset field and in the Late Jurassic succession west of the Venture field. Regional correlation by gamma logs is confirmed by lithologically similar transgressive units including shelly mudstones or coals. A standard scheme of lithofacies and recognition of three types of parasequences were used for comparisons between wells. Some transgressive surfaces are of limited extent and may represent delta distributary switching and subsidence rather than regional changes in sea level. Major sandstone packets extend at least 10-30 km laterally, but commonly show lateral changes in lithofacies, and some are bounded by the margins of incised valleys. Such packages show poor correlation of lithofacies with porosity and permeability, probably because of the variable effects of diagenesis. Lateral transitions from tidal estuary sandstones in Panuke B-90 to thick-bedded river-mouth turbidites in Lawrence D-14, over a distance of 15 km, demonstrates the scale of delta lobes and confirms that sharp-based sandstone beds are turbidites related to river floods, not storm deposits. Similar lateral transitions in the Venture field are on a similar scale and pass distally into prodeltaic muddy landslide deposits.","10.1139/e2012-064","76150","0.2007752"
"112","cold_topic_cluster_no_2","rank_3","82","Sea level change along the Black Sea coast from satellite altimetry, tide gauge and GPS observations","Sea level change affects human living conditions, particularly ocean coasts. However, sea level change is still unclear along the Black Sea coast due to lack of in-situ measurements and low resolution satellite data. In this paper, sea level change along the Black Sea coast is investigated from joint satellite altimetry, tide gauge (TG) and Global Positioning System (GPS) observations. The linear trend and seasonal components of sea level change are estimated at 8 TG stations (Amasra, Igneada, Trabzon-II, Sinop, Sile, Poti, Tuapse, and Batumi) located along the Black Sea coast, which are compared with Satellite Altimetry and GPS. At the tide gauge stations with long-term records such as Poti (about 21 years) and Tuapse (about 19 years), the results obtained from the satellite altimetry and tide gauge observations show a remarkably good agreement. While some big differences are existed between Satellite Altimetry and TG at other stations, after adding vertical motion from GPS, correlation coefficients of the trend have been greatly improved from 0.37 to 0.99 at 3 co-located GPS and TG stations (Trabzon-II, Sinop and Sile). ","10.1016/j.geog.2016.03.005","62317","0.2153846"
"113","cold_topic_cluster_no_2","rank_3","82","Detection of nonstationarities in geological time series: Wavelet transform of chaotic and cyclic sequences","Wavelet analysis is used to detect and localize unconformities, events, or chaotic and periodic-cyclic sequences in marine sedimentary successions. Computer modeling using a single nonlinear algorithm provides a quantitative approach to a better understanding of chaotic, cyclic and monotonous sedimentation, and the preservation of unconformities. To model cyclic and chaotic sequences, we consider nonlinear and cyclic fluctuations in the rate of sedimentary supply, simple climatic feedback processes, variations of the Earth's orbit and sea-level changes. Controlled by superimposed sinusoidal fluctuations of sea-level, four stages of deterministic sedimentary pattern may be detectable: (1) chaotic sedimentation at relatively low sea-level and early transgression; (2) stable sedimentation with well-pronounced cyclicity or monotonous sequences at relatively high sea-level during late transgression and early regression; (3) a pre-chaotic zone with absence of significant cyclicity during decreasing sea-level; and (4) chaotic signals in the sea-level lowstand. These patterns have also been found in a Cretaceous sedimentary succession. Additionally, the exact localization of transitions and abrupt changes (events and unconformities) in the sequences permit a sufficient and simple zonation of the datasets. These transitions can be correlated to stratigraphical boundaries or changes in the depositional environment as well as to changes of the sedimentation rate. ","10.1016/S0098-3004(96)00054-4","92103","0.2161905"
"114","cold_topic_cluster_no_2","rank_3","82","GPS-controlled tide gauges in Indonesia - a German contribution to Indonesia's Tsunami Early Warning System","Coastal tide gauges do not only play a central role in the study of climate-related sea level changes but also in tsunami warning systems. Over the past five years, ten GPS-controlled tide gauge systems have been installed by the German Research Centre for Geosciences (GFZ) in Indonesia to assist the development of the Indonesian Tsunami Early Warning System (InaTEWS). These stations are mainly installed at the Indonesian coastline facing the Indian Ocean. The tide gauge systems deliver information about the instantaneous sea level, vertical control information through GPS, and meteorological observations. A tidal analysis at the station's computer allows the detection of rapid changes in the local sea level (""sea level events""/SLE), thus indicating, for example, the arrival time of tsunamis. The technical implementation, communication issues, the operation and the sea level event detection algorithm, and some results from recent earthquakes and tsunamis are described in this paper.","10.5194/nhess-11-731-2011","80932","0.2242424"
"115","cold_topic_cluster_no_2","rank_3","82","RETHINKING THE INCISED-VALLEY FILL PARADIGM FOR CAMPANIAN BOOK CLIFFS STRATA, UTAH-COLORADO, USA: EVIDENCE FOR DISCRETE PARASEQUENCE-SCALE, SHOREFACE-INCISED CHANNEL FILLS","The conventional sequence stratigraphic model of large-scale incised-valley fills floored by third-order sequence boundaries is inappropriate for most of the Campanian Book Cliffs strata, Utah-Colorado, including the Desert Member to Lower Castlegate Sandstone stratigraphic interval. Shoreface-incised, parasequence-scale, deltaic river-dominated distributaries, tidally influenced distributaries, terminal distributaries, and/or tidal-estuarine channels are ubiquitous. These do not coalesce into sequence-scale incised-valley fills. There are no basinward shifts of facies across regionally mappable sequence boundaries, no sediment bypass with concomitant development of detached falling-stage and lowstand shoreface-deltaic sand bodies, and no incised-valley fill deposits in the Desert-Castlegate interval. Shoreface-incised channels occur in the uppermost proximal parts of most Desert-Castlegate parasequences, not just the parasequence-set top, as predicted by conventional models. Shoreface-incised channel fills are temporally and spatially linked to the laterally adjoining nearshore terrestrial and shallow marine facies belts, as demonstrated by correlation of coal beds and flooding surfaces. Parasequence-scale interfingering of nearshore terrestrial and shallow marine facies belts are common, with thin packages of channels, coal-bearing coastal plain with associated white-capped foreshore-shoreface sandstones, and flat-topped, rooted foreshore sandstones along most parasequence tops. Multiple, parasequence-scale, shoreface-incised channel fills are often lumped together and miscorrelated as sequence-scale incised-valley fills. Similar shoreface-incised channels occur at a variety of parasequence levels in all other members of the Blackhawk Formation. An alternative sequence stratigraphic model showing temporally and spatially linked, laterally adjoining, nearshore terrestrial and shallow marine facies belts is the best fit for the Campanian Book Cliffs strata. This model should be widely applicable to similar nearshore terrestrial and shallow marine settings worldwide. The alternative correlation style generates significantly different predictions of sand-body continuity, connectivity, and distribution. Applications to modeling and predicting subsurface hydrocarbon and fresh-water reservoirs are anticipated.","10.2110/jsr.2018.72","32407","0.2290155"
"116","cold_topic_cluster_no_2","rank_4","157","The role of larval dispersal in metapopulation gene flow: Local population dynamics matter","The degree of genetic connectivity among populations in a metapopulation has direct consequences for species evolution, development of disease resistance, and capacity of a metapopulation to adapt to climate change. This study used a metapopulation model that integrates population dynamics, dispersal, and genetics within an individual-based model framework to examine the mechanisms and dynamics of genetic connectivity within a metapopulation. The model was parameterized to simulate four populations of oysters (Crassostrea virginica) from Delaware Bay on the mid-Atlantic coast of the United States. Differences among the four populations include a strong spatial gradient in mortality, a spatial gradient in growth rates, and uneven population abundances. Simulations demonstrated a large difference in the magnitude of neutral allele transfer with changes in population abundance and mortality (on average between 14 and 25% depending on source population), whereas changes in larval dispersal were not effective in altering genetic connectivity (on average between 1 and 8%). Simulations also demonstrated large temporal changes in metapopulation genetic connectivity including shifts in genetic sources and sinks occurring between two regimes, the 1970s and 2000s. Although larval dispersal in a sessile marine population is the mechanism for gene transfer among populations, these simulations demonstrate the importance of local dynamics and characteristics of the adult component of the populations in the flow of neutral alleles within a metapopulation. In particular, differential adult mortality rates among populations exert a controlling influence on dispersal of alleles, an outcome of latent consequence for management of marine populations.","10.1357/002224012802851869","77900","0.2722689"
"117","cold_topic_cluster_no_2","rank_4","157","Influence of larval traits on dispersal and connectivity patterns of two exploited marine invertebrates in central Chile","Environmental variability can influence larval development rates and affect critical processes in the dynamics of natural populations, such as dispersal distances and connectivity, when modulated by different larval traits. Knowledge of connectivity patterns in marine populations is fundamental for defining population viability and progressing with management and conservation goals. Here, we developed a biophysical, individual-based larval dispersal model to assess the effect of oceanographic variability and biological traits (i.e. larval diel vertical migration [DVM] and temperature-dependent larval development [PLD]) on recruitment success, dispersal distance, and alongshore connectivity patterns. We selected 2 species exploited by Chilean artisanal fisheries: Loxechinus albus (PLD: 20 d) and Fissurella latimarginata (PLD: 5 d). A sensitivity analysis was used to examine the effect of intrinsic (DVM and PLD) and extrinsic (release depth, latitude, and timing) processes. Release location and timing of release explained respectively 24.30 and 5.54% (F. latimarginata) and 34.8 and 4.19% (L. albus) of the variability observed in recruitment success, and 23.80 and 6.94% (F. latimarginata) and 26.10 and 19.60% (L. albus) of the variability observed in dispersal distance. Most recruitment to local populations was allochthonous, presenting low levels of self-recruitment and local retention, including species with short PLD. Similar geographic patterns of source and destination strengths were observed in both species, showing a geographic mosaic of source and sink populations with relatively higher importance towards the northern region of the study area. Our findings allow us to identify primary determinants of recruitment success and dispersal distance for 2 important exploited species in Chile.","10.3354/meps12870","28244","0.2911290"
"118","cold_topic_cluster_no_2","rank_4","157","Patterns, causes, and consequences of marine larval dispersal","Quantifying the probability of larval exchange among marine populations is key to predicting local population dynamics and optimizing networks of marine protected areas. The pattern of connectivity among populations can be described by the measurement of a dispersal kernel. However, a statistically robust, empirical dispersal kernel has been lacking for any marine species. Here, we use genetic parentage analysis to quantify a dispersal kernel for the reef fish Elacatinus lori, demonstrating that dispersal declines exponentially with distance. The spatial scale of dispersal is an order of magnitude less than previous estimates-the median dispersal distance is just 1.7 km and no dispersal events exceed 16.4 km despite intensive sampling out to 30 km from source. Overlaid on this strong pattern is subtle spatial variation, but neither pelagic larval duration nor direction is associated with the probability of successful dispersal. Given the strong relationship between distance and dispersal, we show that distance-driven logistic models have strong power to predict dispersal probabilities. Moreover, connectivity matrices generated from these models are congruent with empirical estimates of spatial genetic structure, suggesting that the pattern of dispersal we uncovered reflects long-term patterns of gene flow. These results challenge assumptions regarding the spatial scale and presumed predictors of marine population connectivity. We conclude that if marine reserve networks aim to connect whole communities of fishes and conserve biodiversity broadly, then reserves that are close in space (<10 km) will accommodate those members of the community that are short-distance dispersers.","10.1073/pnas.1513754112","63181","0.3111111"
"119","cold_topic_cluster_no_2","rank_4","157","Development and characterization of ten highly polymorphic microsatellite markers for the demosponge Poecillastra laminaris (Sollas)","Poecillastra laminaris (Sollas) (Order Tetractinellida, Family Vulcanellidae) is a demosponge widely distributed on seamounts and other habitats around the New Zealand region. In order to investigate its genetic structure and connectivity, ten polymorphic microsatellite markers were developed and tested on all 54 individuals in the NIWA Invertebrate Collection (NIC), Wellington, using low-cost M13 labelling. High quality results were obtained for 34 individuals. The microsatellite loci were highly polymorphic, with ten to 27 alleles per locus (mean +/- SD of 16.2 +/- 5.1). Expected and observed heterozygosities of these loci were 0.786 to 0.952 and 0.550 to 0.926, respectively. These microsatellite markers will be used for population genetic studies of P. laminaris, in particular relating to the identification of metapopulations and barriers to gene flow. Results will inform spatial management planning for the protection of vulnerable marine ecosystems.","10.1007/s12526-016-0540-z","37403","0.3387755"
"120","cold_topic_cluster_no_2","rank_4","157","Fifteen novel microsatellite markers for two Amphiprion species (Amphiprion frenatus and Amphiprion perideraion) and cross-species amplification","Anemonefishes are popular model species for research on population connectivity via larval dispersal, and understanding this connectivity is beneficial when designing marine protected areas. We developed 15 microsatellite markers for two anemonefishes, Amphiprion frenatus and Amphiprion perideraion. Of these 15 markers, 10 worked well for both species, while three were specific for A. frenatus and two were specific for A. perideraion. For A. frenatus, the number of alleles at each locus ranged from 4 to 28, and the observed and expected heterozygosities ranged from 0.258 to 0.938 and from 0.285 to 0.953, respectively. For A. perideraion, the respective numbers were 4-14, 0.344-0.969, and 0.412-0.868. These microsatellite markers will be useful for the study of population connectivity in these two species.","10.1007/s12686-014-0182-z","69499","0.3781250"
"121","cold_topic_cluster_no_2","rank_5","655","Climate Change and the Distribution of Neotropical RedBellied Toads (Melanophryniscus, Anura, Amphibia): How to Prioritize Species and Populations?","We used species distribution modeling to investigate the potential effects of climate change on 24 species of Neotropical anurans of the genus Melanophryniscus. These toads are small, have limited mobility, and a high percentage are endangered or present restricted geographical distributions. We looked at the changes in the size of suitable climatic regions and in the numbers of known occurrence sites within the distribution limits of all species. We used the MaxEnt algorithm to project current and future suitable climatic areas (a consensus of IPCC scenarios A2a and B2a for 2020 and 2080) for each species. 40% of the species may lose over 50% of their potential distribution area by 2080, whereas 28% of species may lose less than 10%. Four species had over 40% of the currently known occurrence sites outside the predicted 2080 areas. The effect of climate change (decrease in climatic suitable areas) did not differ according to the present distribution area, major habitat type or phylogenetic group of the studied species. We used the estimated decrease in specific suitable climatic range to set a conservation priority rank for Melanophryniscus species. Four species were set to high conservation priority: M. montevidensis, (100% of its original suitable range and all known occurrence points potentially lost by 2080), M. sp.2, M. cambaraensis, and M. tumifrons. Three species (M. spectabilis, M. stelzneri, and M. sp.3) were set between high to intermediate priority (more than 60% decrease in area predicted by 2080); nine species were ranked as intermediate priority, while eight species were ranked as low conservation priority. We suggest that monitoring and conservation actions should be focused primarily on those species and populations that are likely to lose the largest area of suitable climate and the largest number of known populations in the short-term.","10.1371/journal.pone.0094625","70758","0.2053097"
"122","cold_topic_cluster_no_2","rank_5","655","Reassessment of the conservation status and protected area coverage of Taiwanese birds: How distribution modelling can help species conservation","Taiwan has 145 breeding bird species, but so far no comprehensive attempt has been made to model their distributions. For the first time, we bring together various datasets to model the distributions of the 116 bird species with sufficient sampling coverage. We improved on previous limited modelling efforts by using ensemble modelling, based on five well-performing modelling pproaches: multiple discriminant analysis, logistic regression, genetic algorithm for rule-set production, ecological niche factor analysis and maximum-entropy. We then used these ensemble models to improve our knowledge of the status of each bird species by (1) calculating each species's coverage of Taiwan, (2) calculating each species's coverage by Taiwan's protected area network, and (3) comparing these two conservation-relevant measures with already established measures to highlight those species whose status may need to be reassessed. We categorised each species's coverage of the entire study area as measured by their modelled distributions into four quartiles, thus establishing a new measure of rarity called 'range quartile' which we used to highlight the 22 species with a limited distribution on mainland Taiwan. We also calculated that overall, 29.8% of the distribution ranges of the 116 modelled species are covered by Taiwanese protected areas. We then identified those species whose status may need to be reassessed because of possible conflicts between the respective conservation-relevant measures. Thus we identified 10 species which are first-quartile species < 5% of whose distributions are protected, of which only five are considered threatened. We also identified another 12 species with limited distributions, 30 species with limited protection and 19 species whose status may need to be reassessed for various reasons. We recommend that range quartile and protected area coverage be incorporated into future assessments of the conservation status and protected area coverage of Taiwanese birds.","10.1017/S0959270913000336","70332","0.2223140"
"123","cold_topic_cluster_no_2","rank_5","655","Biological reserves, rare species and the trade-off between species abundance and species diversity","The preservation of species diversity generally suggests protection of either the greatest number of species possible or all species. Requiring representation of each species in at least one parcel in the system and seeking the minimum number of parcels in the reserve system to achieve this requirement is termed the Species Set Covering Problem (SSCP). Nonetheless, it is important, as well, to consider the rarest of species, as their populations are the most in need of protection to assure their survival. This paper uses 0-1 programming models and an existing data set to study species protection, rarity, species abundance and species diversity. We employ for this purpose an integer programming model that uses the SSCP format to require at least one representation of each and every species, but that seeks in addition protection of the rarest species. This is achieved by maximizing redundant coverage of those species designated as rare. Results are then compared to those of the SSCP. Recognizing that resources available for conservation purchases could well be insufficient to represent all species at least once, we structure a model comparing coverage of the greatest number of species and redundant coverage of rare species. We develop a trade-off curve for this multi-objective problem in order to evaluate the opportunity cost of covering more species as redundant coverage of rare species decreases-and vice versa. Finally, various possible rarity sets and various budget proxies are considered along with their impacts on conservation policies, Pareto optimality and species diversity. ","10.1016/j.ecolecon.2005.03.009","87820","0.2350000"
"124","cold_topic_cluster_no_2","rank_5","655","Comparing machine learning classifiers in potential distribution modelling","Species' potential distribution modelling consists of building a representation of the fundamental ecological requirements of a species from biotic and abiotic conditions where the species is known to occur. Such models can be valuable tools to understand the biogeography of species and to support the prediction of its presence/absence considering a particular environment scenario. This paper investigates the use of different supervised machine learning techniques to model the potential distribution of 35 plant species from Latin America. Each technique was able to extract a different representation of the relations between the environmental conditions and the distribution profile of the species. The experimental results highlight the good performance of random trees classifiers, indicating this particular technique as a promising candidate for modelling species' potential distribution. ","10.1016/j.eswa.2010.10.031","80250","0.2396226"
"125","cold_topic_cluster_no_2","rank_5","655","ANGIOSPERMS IN NARSINGDI DISTRICT OF BANGLADESH: CLASS LILIOPSIDA","This study provides the taxonomic data on 168 plant species belonging to 96 genera and 23 families of Liliopsida (monocotyledons) extant in Narsingdi district of Bangladesh. These species are mostly comprised of herbs (90.48%), followed by trees and shrubs (4.76% each). Poaceae with 66 species under 37 genera is the best represented family, followed by Cyperaceae with 26 species of seven genera, Araceae with 16 species of 11 genera, Commelinaceae with 11 species of four genera and Arecaceae with 10 species of eight genera. Cyperus with 13 species appears as the largest genus, which is followed by Panicum with nine species, Digitaria with six species, and Commelina and Dioscorea with five species each. The six upazilas of this district are 39.77% similar in the species composition of their Liliopsida, but the similarity between the pairs of upazilas varies from 6.45% to 32.31%. Roadside and fallow land habitats share the highest similarity (36.84%) in species composition. Total 117 species are distinguished as economically useful. This study suggests for implementation of necessary measures in order to minimize the major threats to this plant group and to favor its sustainable development in the study area.",""," 1193","0.2558824"
"126","cold_topic_cluster_no_3","rank_1","534","Optimization of the Activated Sludge Process","This paper presents a multiobjective model for optimization of the activated sludge process (ASP) in a wastewater-treatment plant (WWTP). To minimize the energy consumption of the activated sludge process and maximize the quality of the effluent, three different objective functions are modeled [i.e., the airflow rate, the carbonaceous biochemical oxygen demand (CBOD) of the effluent, and the total suspended solids (TSS) of the effluent]. These models are developed using a multilayer perceptron (MLP) neural network based on industrial data. Dissolved oxygen (DO) is the controlled variable in these objectives. A multiobjective model that included these objectives is solved with a multiobjective particle swarm optimization (MOPSO) algorithm. Computation results are reported for three trade-offs between energy savings and the quality of the effluent. A 15% reduction in airflow can be achieved by optimal settings of dissolved oxygen, provided that energy savings take precedence over the quality of the effluent. DOI: 10.1061/(ASCE)EY.1943-7897.0000092. ","10.1061/(ASCE)EY.1943-7897.0000092","75099","0.2208333"
"127","cold_topic_cluster_no_3","rank_1","534","Haar orthogonal functions based parameter identification of wastewater treatment process with distributed parameters","The most often used system in aerobic biological wastewater treatment is the system ""biological reservoir - sedimentor"". The necessary condition for obtaining a good operative control is an adequate process model to be available. The aim of this paper is modelling of a process, carrying out a system ""biological reservoir - sedimentor"", and consequent parameter identification. In order to obtain a model with higher degree of accuracy, the process of wastewater treatment is considered as object with distributed parameters. Shifted two - dimensional Haar orthogonal functions are used for parameter identification of the process. The implementation of these orthogonal functions reduces the problem to a computationally convenient form. The algorithm is efficient and simple in form.","","88324","0.2608696"
"128","cold_topic_cluster_no_3","rank_1","534","Automation of an anaerobic-aerobic wastewater treatment process","In this paper, the treatment capacity and effluent quality of an anaerobic-aerobic sewage treatment system composed of a upflow anaerobic sludge blanket unit in series with a sequencing batch reactor (SBR) is evaluated. The aerobic part of the system (the SBR) is automated and the concentration of dissolved oxygen is used as a control parameter for the aeration period.","10.1109/TIM.2003.814359","89753","0.2695652"
"129","cold_topic_cluster_no_3","rank_1","534","Designing Control Strategies of Aeration System in Biological WWTP","The paper presents the complete design processes of novel aeration control systems in the SBR (sequencing batch reactor) wastewater treatment plant (WWTP). Due to large energy expense and a high influence on biological processes, the aeration system plays a key role in WWTP operation. The paper considers the aeration system for a biological WWTP located in the northeast of Poland. This system consists of blowers, the main collector pipeline, three aeration lines with different diameters and lengths, and diffusers. Classical control systems applied for this type of installation are based on PID (proportional-integral-derivative) controllers, the settings of which are often found experimentally. The article presents the optimization of these settings and the design of an alternative control algorithm-the fuzzy controller.","10.3390/en13143619"," 8525","0.2708333"
"130","cold_topic_cluster_no_3","rank_1","534","Adaptive linearizing control of activated sludge processes","This paper deals with the control of biological wastewater treatment processes. More specifically it concentrates on the control of the biological oxygen demand (BOD) and of the dissolved oxygen in activated sludge processes. The control algorithm design is based on dynamic mass balance equations of the process and includes the on-line estimation of uncertain parameters (specific growth rate). Its performance is illustrated by simulation.","","92324","0.2862069"
"131","cold_topic_cluster_no_3","rank_2","774","ADAPTIVE CONTROL OF TEMPERATURE FOR MINIMIZATION OPERATING COSTS OF INDUSTRIAL METHANE TANK PROCESS","An algorithm for adaptive control of temperature in methane tank is developed in order to minimize operating costs under variation of the methane tank process conditions. The adaptation is based on process model and on-line minimization of objective function that relates operating costs to the process variables. Mathematical model for prediction of biogas production and simulation of the temperature control process is identified using experimental data from the waste water treatment plant. The temperature adaptive control system is investigated via computer simulation of the control system performance under operating conditions of real methane tank process during one year period. The process operating costs by optimal control of temperature are compared to those of temperature control at constant set-point. The model identification and the control system simulation results are presented and discussed.","","83015","0.2161290"
"132","cold_topic_cluster_no_3","rank_2","774","Optimal Control Method for HVAC Systems in Offices with a Control Algorithm Based on Thermal Environment","This study examined a method to reduce energy consumption in office buildings. Correspondingly, an optimal control method was proposed for heating, ventilation, and air conditioning (HVAC) systems via two control algorithms that considered the indoor thermal environment. The control algorithms were developed by considering temperature and humidity as the factors of the indoor thermal environment that influence the control of HVAC systems and the predicted mean vote comfort ranges. Furthermore, an experiment was performed using office equipment that incorporated the two control algorithms for HVAC systems, and the correlation between changes in the thermal environment within the office and the occupant's comfort levels was estimated via an actual survey. The results demonstrated that the proposed control method for HVAC systems, which considered the comfort ranges of temperature and humidity and the thermal adaptation capability, can efficiently maintain the occupant's comfort with lower energy usage compared with conventional HVAC systems. Thus, the use of the control method contributes to the reduction of total energy consumption in buildings with HVAC systems.","10.3390/buildings10050095","11529","0.2238806"
"133","cold_topic_cluster_no_3","rank_2","774","A novel operation approach for the energy efficiency improvement of the HVAC system in office spaces through real-time big data analytics","Since a traditional centralized control system (e.g., building energy management system) with a fixed schedule and manual control is not appropriate to irregularly occupied rooms, it is expected to have a large amount of energy saving potential in operating the HVAC system. To overcome this challenge, this study aimed to develop a novel operation approach for the energy efficiency improvement of the HVAC system in office spaces. The real-time indoor environmental indicators were collected and analyzed to evaluate the current operation status of the HVAC system as well as to propose a novel control strategy in two ways. The significant findings can be illustrated as follows. First, it could be stated that occupants would tend to establish a lower set-point temperature for a cooler indoor environment. To solve this issue, a basic control strategy was proposed to detect the anomaly detection of the HVAC system and to automatically adjust the indoor temperature within a preferred range. Second, it could be evaluated that the HVAC system would be kept operating since occupants would forget to turn off the HVAC system after the meetings. To solve this issue, an advanced control strategy was proposed to operate the automatic on/off control of the HVAC system by considering the indoor temperature and CO2 concentration in real time. The proposed strategies can contribute to a large amount of energy savings in operating the HVAC system of irregularly occupied spaces.","10.1016/j.rser.2020.109885"," 9027","0.2404494"
"134","cold_topic_cluster_no_3","rank_2","774","Study on application of a neuro-fuzzy models in air conditioning systems","In this paper, we present the application of neuro-fuzzy system-ANFIS for air conditioning systems to reduce electricity consumption. The current buildings have automation systems that provide data on lighting, electrical system, air conditioning system, etc. We studied the air conditioning system, in particular, to reduce energy consumption as air conditioning has a high consumption value. Our main goal in this study is the application of neuro-fuzzy system-ANFIS with the adjustment of the rules made by a decision tree-CART algorithm in air conditioning system. We compared the results of the application of the ANFIS-CART system with the application of PID controllers and fuzzy control system for a central air conditioning.","10.1007/s00500-014-1431-5","66134","0.2483871"
"135","cold_topic_cluster_no_3","rank_2","774","Emulation environment for automobile automatic climate control","This study presents the development of a simulation platform for evaluating climate control strategies for automobiles. Unified modeling language-based use-case diagrams and statechart diagrams are employed to model the functionalities and dynamic behaviors, respectively, of a typical automatic air-conditioning system. With these two modeling diagrams as blueprints of the simulation target, a general simulation platform is implemented on Matlab/Simulink. Given a tested climate control algorithm and environment conditions, simulation results in terms of temperature and relative humidity in the car compartment can be visually presented on an implemented graphic user interface of a psychrometric chart. With its flexibility to replace control algorithms and to change testing parameters, the implemented platform has been utilized to test the control results of an enthalpy-based control algorithm, robustness of the same control algorithm at different initial conditions, and comparison of two different control algorithms.","10.1080/02533839.2012.734676","75603","0.2647059"
"136","cold_topic_cluster_no_3","rank_3","7","Using common-pool resource design principles to assess the viability of community-based fisheries co-management systems in American Samoa and Hawai'i","Community-based fisheries co-management provides a promising path to improve both environmental sustainability and social justice, but the outcomes of co-management programs to date have been mixed. This paper examines two fisheries co-management programs, American Samoa's Community-based Fisheries Management Program (CFMP) and Hawai'i's Community-based Subsistence Fisheries Area (CBSFA) legislation, using a framework of design principles derived from common property theory. The two programs provide an interesting contrast; while American Samoa's CFMP program has been successfully implemented in eleven communities, the Hawai'i program has struggled, with no CBSFA fully implemented for over 20 years. The purpose of this analysis is threefold: to utilize the design principles to assess the likelihood that a given fisheries co-management program will have a successful outcome; to test the design principles to determine how well these principles can serve as predictors for the outcomes seen in these two different programs; and to develop recommendations for how these design principles might be used to strengthen these two programs, as well as improve other fisheries co-management programs and policies in the future. The analysis reveals that the design principles provide a useful framework through which to assess co-management programs. Our analysis reveals that the principles can provide important insights for understanding the differential outcomes of the two programs as well as provide guidance for how these and other programs can be strengthened in the future. ","10.1016/j.marpol.2015.08.019","62754","0.1666667"
"137","cold_topic_cluster_no_3","rank_3","7","Developing the eCPP: Adapting an Evidence-Based Parent Training Program for Digital Delivery in Primary Care Settings","BackgroundDeveloping innovative delivery methods is needed to overcome time and logistic barriers to in-person participation in evidence-based parent training (PT) programs. PurposeThe purpose of this paper is to (a) describe the systematic process for adapting an evidence-based group PT program (the Chicago Parent Program) to a tablet-based delivery format, (b) present the adapted program, and (c) discuss opportunities and challenges of adapting evidence-based programs for alternative delivery methods. MethodsTo ensure consistency with the original program and relevance to the intended program recipients, three groupsparents (n = 10), CPP developers (n = 3), and digital delivery experts were engaged throughout the systematic steps of the delivery adaptation of the Chicago Parent Program (eCPP). Group meetings were used to identify the program's core components, develop the adaptation program model, assess potential mismatches for the new delivery context, and adapt the original program model and materials. ResultsThe final eCPP is a six-module Internet-based intervention that includes: interactive activities, video examples and explanations of parenting strategies, reflection questions, assessment of parent knowledge with feedback, and module practice assignments. Linking Evidence to ActionDeveloping innovative delivery approaches for evidenced-based interventions are promising to increase intervention sustainability and participant access and engagement. It is critical that these adaptations are systematic and developed with expert consultation and community input.","10.1111/wvn.12074","66831","0.1706422"
"138","cold_topic_cluster_no_3","rank_3","7","Health and Big Data: An Ethical Framework for Health Information Collection by Corporate Wellness Programs","This essay details the resurgence of wellness program as employed by large corporations with the aim of reducing healthcare costs. The essay narrows in on a discussion of how Big Data collection practices are being utilized in wellness programs and the potential negative impact on the worker in regards to privacy and employment discrimination. The essay offers an ethical framework to be adopted by wellness program vendors in order to conduct wellness programs that would achieve cost-saving goals without undue burdens on the worker. The essay also offers some innovative approaches to wellness that may well better serve the goals of healthcare cost reduction.","10.1177/1073110516667943","57606","0.1764706"
"139","cold_topic_cluster_no_3","rank_3","7","Applying spatial analysis to forest policy evaluation: case study of the Illinois Forestry Development Act","Economic incentive programs are increasingly promoted in the United States and Europe as a method to encourage sustainable forest management practices on privately owned lands. This paper employs spatial analysis to evaluate the effectiveness of one such program created in Illinois through passage of the Illinois Forestry Development Act (IFDA). The Act establishes economic incentives available to private landowners who develop forest management plans and/or establish tree plantings. This paper provides (1) a description of the Illinois Forestry Development Act and its associated incentive program and (2) an evaluation of the program's land enrollments in respect to their potential for improving surface water quality. In order to evaluate the legislation in terms of water quality benefits, this paper develops a novel analytical approach utilizing a geographic information system (GIS) and high-resolution digital satellite imagery to locate and characterize land parcels enrolled in the IFDA landowner assistance program. The program has done well in enrolling lands located on steep slopes and has been moderately successful at enrolling lands within environmentally impaired watersheds. There is room for improvement in enrolling lands within the riparian areas of impaired waterways. However, the ability to enroll new lands in the program is limited by the program budget. ","10.1016/j.envsci.2006.01.002","88038","0.1775510"
"140","cold_topic_cluster_no_3","rank_3","7","Effectiveness of school-based drug prevention programs for marijuana use","Thirty-seven evaluations of universal drug use prevention programs implemented in American schools between grades 6 and 12 were quantitatively synthesized by coding program characteristics and calculating weighted effect sizes (WES) for marijuana use. Programs were divided into two types, Interactive and Non-Interactive, based on a combination of content and delivery method. To determine the characteristics of programs that most effectively reduce, delay or prevent marijuana use, a weighted least squares multiple regression analysis was performed using the WES of marijuana use as the dependent variable, type of program as the predictor and sample size as an additional covariate. Program type and sample size were found to be significant predictors of program effectiveness. Non-Interactive lecture-oriented prevention programs that stressed knowledge about drugs or affective development of students showed minimal reductions in marijuana use. Interactive programs that fostered the development of social competencies showed greater reductions in marijuana use. Larger implementations of both program types showed substantial decreases in effectiveness, although the larger Interactive programs were significantly superior to the larger Non-Interactive ones. The primary finding for prevention program planners is that interactive cultivation of social skills reduces marijuana use.","10.1177/0143034399201008","91499","0.1969697"
"141","cold_topic_cluster_no_3","rank_4","148","Synchronous machine parameter estimation using the Hartley series","This paper presents a navel alternative to estimate armature circuit parameters of large utility generators using real time operating data. The proposed approach uses the Hartley series for fitting operating data (voltage and currents measurements). The essence of the method is the use of linear state estimation to identify the coefficients of the Hartley series. The approach is tested for noise corruption likely to be found in measurements. The method is found to be suitable for the processing of digital fault recorder data to identify synchronous machine parameters.","10.1109/60.911403","90759","0.1432432"
"142","cold_topic_cluster_no_3","rank_4","148","Monitoring digitisation: lessons from previous experiences","Purpose - The purpose of this paper is to evaluate current approaches to assessing digitisation activities in memory institutions. Design/methodology/approach - Qualitative and quantitative analysis of digitisation surveys were performed. Analysis concentrated on several themes: general methodological solutions, digitisation objectives, users and usage of digitised content, budgeting and costs of digitisation, and volume and growth of digitised collections. Findings - Analysis revealed an absence of sound methodology solutions, issues of constructing a sample, the split between strategic and resource management approaches to digitisation, low visibility of user related evaluation criteria, and problems in developing quantitative measures. Research limitations/implications - Approaches to evaluating digitisation are not restricted to digitisation surveys and to provide a more comprehensive analysis these should be complemented by other data (e.g. inter-views of digitisation experts). The identification of surveys was limited by subjective factors such as knowledge of national experts, visibility of reports on the web, and language of publication. Practical implications - The paper assists in the development of digitisation surveys by highlighting previous gaps and achievements. Originality/value - The paper is a first attempt to comprehend approaches to monitoring digitisation internationally. Gaps and issues identified in the research can guide studies on developing indicators and measures for specific digitisation activities.","10.1108/00220410910983100","84719","0.1473118"
"143","cold_topic_cluster_no_3","rank_4","148","Compliance of Electronic Bill of Lading Regulation in Korea with Model Law on Electronic Transferable Records","Purpose -The UNCITRAL Model Law on Electronic Transferable Records (Model Law) is based on the principles of non-discrimination against the use of electronic means, functional equivalence, and technology neutrality underpinning all UNCITRAL texts on electronic commerce. Investigating the disagreements between the Model Law and the Koran Commercial Act (KC Act), including the B/ L Regulation, and suggesting the revision of the KC Act including the B/ L Regulation, could be a valuable study. The purpose of this paper is to contribute to the harmonization of Korean legislation regarding electronic bill of lading in compliance with the Model Law. Design/ methodology -The Model Law is flexible to accommodate the use of all technologies and models, such as registries, tokens, and distributed ledgers: that is, blockchain. In 2007, the KC Act was revised to regulate electronic bills of lading to promote the widespread legal use of electronic bills of lading. In addition, The Regulation on Implementation of the Provisions of the Commercial Act Regarding Electronic Bills of Lading (the B/ L Regulation) was enacted to regulate the detailed procedures in using electronic bills of lading in 2008. This paper employs a legal analysis by which this paper does find differences between two rules in light of technology neutrality and global standard of electronic bills of lading model. Findings -The main findings are as follows: i) the Korean registry agency has characteristics of a closed system. ii) The KC Act has no provision regarding control. iii) The KC Act discriminates other electronic bills of lading on the ground that it was issued or used abroad. Moreover, this study does comprehensive analysis of Korean Acts in comparison with the Model Law and, in particular, this study analyzes the differences between the KC Act and the Model Law by comparing article by article in view of the harmonization of the two rules. Originality/ value -The subject of previous several studies was draft provisions on Electronic Transferable Records before completion of the Model Law; thus, these studies did not take into consideration the character of the Model Law as the Model Law was chosen at the final stage of legislation. This study is aimed at the final version of the Model Law. So, this study is meaningful by finding the suggestion and directions for the Korean government to revise the KC Act and the B/ L Regulation in line with the Model Law.","10.35611/jkt.2019.23.3.68","30723","0.1481481"
"144","cold_topic_cluster_no_3","rank_4","148","WHEN COPYRIGHTS MEET HUMAN RIGHTS: ""CYBERSPACE ARTICLE 23"" AND HONG KONG'S COPYRIGHT PROTECTION IN THE DIGITAL ERA","Hong Kong's current Copyright Bill has been criticised as a ""Cyberspace Article 23"". There is a fear that its introduction would hamper freedom of speech, because its all-embracing and exclusive right of communication to copyright holders leaves no room for satire or parody of copyright works. With reference to domestic and international jurisprudence, this article argues that the apparent conflict between copyright and freedom of speech is misleading because copyright's protection of the expression of ideas forms an essential part of freedom of speech. The article argues that Hong Kong's model is very unique, contrary to public perception: the Bill not only accommodates various individual rights, but also combines statutory law with a non-statutory Code of Practice to define Online Service Providers' liability through its ""one regime two systems"" framework. The Hong Kong model provides insight for other jurisdictions for balancing conflicting interests in adapting ","","78336","0.1687500"
"145","cold_topic_cluster_no_3","rank_4","148","The impact of technological change on employment: The case of press digitisation","Based on an exhaustive dataset of all journalists in France, this article investigates the impact of digitisation on the employment of journalists in the press industry. In particular, focus is put on the effect played by the level of digitisation of newspapers and magazines, some of which have resisted digitisation, while others have embraced it. We find that greater levels of digitisation tend to increase the likelihood of job creation and reduce the probability of job destruction. Likewise, higher level of digitisation leads, on average, to higher earnings for journalists. At the same time, though, higher digitisation also increases sharply the likelihood that jobs created are of casual contractual natures, as opposed to regular permanent contracts. Yet, we find that digitisation also has a positive impact on the earnings of journalists on a casual contact (though, far less than for 'tenured' journalists). More surprisingly, we show that digitisation also reduces job instability of those journalists on a casual contract, as a greater level of digitisation reduces the likelihood of job destruction, even for casual jobs. Though, while digitisation tends to change the contractual nature of job created, embracing digitisation appears to be a 'lesser evil' than resisting technological change.","10.1016/j.techfore.2017.10.015","39514","0.1797753"
"146","cold_topic_cluster_no_3","rank_5","542","Scalable Safety-Preserving Robust Control Synthesis for Continuous-Time Linear Systems","We present a scalable set-valued safety-preserving hybrid controller for constrained continuous-time linear time-invariant (LTI) systems subject to additive disturbance/uncertainty. The approach relies on a conservative approximation of the discriminating kernel using a piecewise ellipsoidal algorithm with polynomial complexity. This precomputed approximation is used online to synthesize a permissive state-feedback control law that guarantees the satisfaction of all constraints despite potentially conflicting performance objectives. We show the results on a flight envelope protection problem for a quadrotor with actuation saturation and unknown wind disturbances.","10.1109/TAC.2015.2411872","63431","0.1627451"
"147","cold_topic_cluster_no_3","rank_5","542","Energy efficient bipedal robots walking in resonance","This contribution presents a method to improve the energy efficiency of walking bipedal robots over 50% in a range of speed from 0.3 to 2.3 m/s by the use of constant elastic couplings. The method consists of modeling the robot as underactuated system - so that it is making use of its natural dynamics instead of fighting against it - controlling its joint-angle trajectories with input-output feedback linearization and optimizing the joint-angle trajectories as well as the elastic couplings numerically. The mechanism of minimizing energy expenditure consists of reducing impact losses by choosing smaller steps, which gets favorable by a higher natural frequency due to elastic couplings. The method is applied to a planar robot with upper body, two stiff legs, two actuators in the hip joints and one simple rotational spring between the legs as elastic coupling. The mechanism of energy expenditure is investigated for the robot with and without elastic coupling between legs in detail. ","10.1002/zamm.201300245","68802","0.1885714"
"148","cold_topic_cluster_no_3","rank_5","542","Combining series elastic actuation and magneto-rheological damping for the control of agile locomotion","All-terrain robot locomotion is an active topic of research. Search and rescue maneuvers and exploratory missions could benefit from robots with the abilities of real animals. However, technological barriers exist to ultimately achieving the actuation system, which is able to meet the exigent requirements of these robots. This paper describes the locomotion control of a leg prototype, designed and developed to make a quadruped walk dynamically while exhibiting compliant interaction with the environment. The actuation system of the leg is based on the hybrid use of series elasticity and magneto-theological dampers, which provide variable compliance for natural-looking motion and improved interaction with the ground. The locomotion control architecture has been proposed to exploit natural leg dynamics in order to improve energy efficiency. Results show that the controller achieves a significant reduction in energy consumption during the leg swing phase thanks to the exploitation of inherent leg dynamics. Added to this, experiments with the real leg prototype show that the combined use of series elasticity and magneto-rheological damping at the knee provide a 20% reduction in the energy wasted in braking the knee during its extension in the leg stance phase. ","10.1016/j.robot.2011.06.006","79463","0.1904762"
"149","cold_topic_cluster_no_3","rank_5","542","Control of micro-satellite orientation using bounded-input, fully-reversed MEMS actuators","We present a novel technique for controlling the attitude of microsatellites using MEMS-based microactuators. In addition to being restricted in the magnitudes of inputs, most microactuators need to be returned to their original inactive state at the end of the attitude control maneuver in order to reduce energy consumption. This type of bounded and fully-reversed actuation on a free rigid body leads to a special type of system in which nonholonomic effects have been incorporated by design. We discuss issues of controllability and develop motion planning algorithms for such a system considering the nonintegrable nature of rigid body rotations. We use approximate methods to develop two algorithms for controlling satellite orientation. The first method uses a series of pair-wise actuation steps and is guaranteed to converge to the desired orientation; the second uses fewer steps, and in certain cases uses less input power We also present preliminary results of a practical implementation using electro-thermal microactuation.","10.1177/027836402128964440","90311","0.2000000"
"150","cold_topic_cluster_no_3","rank_5","542","High-level motion planning for CPG-driven modular robots","Modular robots may become candidates for search and rescue operations or even for future space missions, as they can change their structure to adapt to terrain conditions and to better fulfill a given task. A core problem in such missions is the ability to visit distant places in rough terrain. Traditionally, the motion of modular robots is modeled using locomotion generators that can provide various gaits, e.g. crawling or walking. However, pure locomotion generation cannot ensure that desired places in a complex environment with obstacles will in fact be reached. These cases require several locomotion generators providing motion primitives that are switched using a planning process that takes the obstacles into account. In this paper, we present a novel motion planning method for modular robots equipped with elementary motion primitives. The utilization of primitives significantly reduces the complexity of the motion planning which enables plans to be created for robots of arbitrary shapes. The primitives used here do not need to cope with environmental changes, which can therefore be realized using simple locomotion generators that are scalable, i.e., the primitives can provide motion for robots with many modules. As the motion primitives are realized using locomotion generators, no reconfiguration is required and the proposed approach can thus be used even for modular robots without self-reconfiguration capabilities. The performance of the proposed algorithm has been experimentally verified in various environments, in physical simulations and also in hardware experiments. ","10.1016/j.robot.2015.01.006","65519","0.2019417"
"151","cold_topic_cluster_no_4","rank_1","700","Estimating Summer Precipitation Over the Tibetan Plateau With Geostatistics and Remote Sensing","Although precipitation is important to climatology, hydrology, and agricultural research, the spatial pattern of precipitation over the Tibetan Plateau is difficult to determine because of complex surface conditions and a sparse rain gauge network. In the present article, a method we named FETCH_OCK-based on a combination of Yin et al's Fetch method (2008) and ordinary cokriging (OCK)-is proposed; it was used to estimate monthly summer precipitation over the Tibetan Plateau, which has limited rain gauge observations and a restricted satellite precipitation dataset. First, the monthly ground observations measured by rain gauges were interpolated using OCK, with a digital elevation model (DEM) as the covariant. Second, the spatial variability of the precipitation monitored by satellite was extracted from the Climate Prediction Center morphing (CMORPH) satellite precipitation dataset by calculating a parameter (FETCH) developed from Yin et al's Fetch parameter. Finally, the precipitation datasets estimated by OCK were corrected by the FETCH parameter derived from the CMORPH satellite precipitation dataset. Summer (June to August) precipitation over the Tibetan Plateau from 2005 to 2009 was estimated using this model. The precipitation datasets estimated by FETCH_OCK were tested using ground observations from 55 independent rain gauges. The results indicate that the FETCH_OCK model not only is an improvement compared with the input precipitation datasets (OCK and CMORPH) but also performs better than other widely used precipitation datasets, including universal kriging with DEM as a covariant and Tropical Rainfall Measuring Mission 3B43. The present study aims to correct the smoothing effect of kriging interpolation models and to provide a more accurate precipitation dataset for the Tibetan Plateau.","10.1659/MRD-JOURNAL-D-13-00033.1","72954","0.2504587"
"152","cold_topic_cluster_no_4","rank_1","700","Recent global performance of the Climate Hazards group Infrared Precipitation (CHIRP) with Stations (CHIRPS)","The Climate Hazards group Infrared Precipitation (CHIRP) and with Stations (CHIRPS) datasets are two new quasi-global (50 degrees S-50 degrees N), high-resolution (0.05 degrees x 0.05 degrees), long-term (1981-present) precipitation estimates based on infrared Cold Cloud Duration (CCD) observations. This study investigates, for the first time, the global performance of CHIRP and CHIRPS against the gauge-based GPCC (Global Precipitation Climatology Centre) data at monthly scale using 36 complete years of data record (1981-2016). Global assessment results indicate that both CHIRP and CHIRPS have negative biases (- 5.93% for CHIRP and - 2.01% for CHIRPS) before 2000, while this systematic underestimation was effectively removed after 2000. Global analyses also show that the gauge-adjusted CHIRPS estimates generally represent a substantial improvement over CHIRP due to gauge-based bias correction. With respect to regional statistics, temporal analysis and intensity distribution, the gauge-adjusted CHIRPS estimates agree well with GPCC and outperforms CHIRP over most regions, such as the United States, Europe, Africa, Australia and South America. However, southeast China is an exception. Over this region, CHIRPS has a systematic overestimation of 5.55% against GPCC during 2000-2016, especially for spring and summer months, while such positive biases were not found for the pure satellite-derived CHIRP. Possible causes for the discrepancy between these two satellite products over the global and regional scales were further discussed and analyzed. The results reported here will both provide the algorithm developers of CHIRP and CHIRPS with some valuable information and offer the hydrometeorological users a better understanding of their error characteristics and potential limits for various hydrological applications from the global perspective.","10.1016/j.jhydrol.2020.125284","  985","0.2516393"
"153","cold_topic_cluster_no_4","rank_1","700","Statistical properties of precipitation as observed by the TRMM precipitation radar","The statistical properties of tropic-subtropic precipitation are revealed with 13years of Tropical Rainfall Measuring Mission (TRMM) precipitation radar (PR) measurements. About 3% of PR observations are raining pixels. The average daily rainfall over 37.5 degrees N-37.5 degrees S is 1.28, 1.18, and 2.46mmd(-1) for convective, stratiform, and total rain, respectively, indicating 51.85% from convective rain and 48.09% from stratiform rain. The related values are 1.300, 1.272, and 2.573mmd(-1) over ocean and 1.22, 0.97, and 2.19mmd(-1) over land, indicating a convective rain fraction of 50.51% over ocean and 55.77% over land. The 92% (93%) and 73% (55%) of rain events over ocean (land) are from stratiform and convective rain <5mmh(-1), respectively, while the associated rainfall contributions in stratiform and convective rain are 62% (68%) and 27% (15%) over ocean (land). Results demonstrate that contributions from large rain intensity events are very importation in total precipitation, especially over land. The rainfall missed by TRMM PR is mostly light rain and does not significantly impact large-scale statistics of convective and stratiform rain amount. Light rain will increase the total precipitation by about 10% and, if considered a separate category, decrease the observed convective and stratiform rain contributions about 10% over the PR domain. These statistical properties of precipitation could be utilized as a baseline in the assessment of precipitation from operational numerical weather prediction and climate models.","10.1002/2014GL060683","69629","0.2674419"
"154","cold_topic_cluster_no_4","rank_1","700","Correction of TRMM 3B42V7 Based on Linear Regression Models over China","High temporal-spatial precipitation is necessary for hydrological simulation and water resource management, and remotely sensed precipitation products (RSPPs) play a key role in supporting high temporal-spatial precipitation, especially in sparse gauge regions. TRMM 3B42V7 data (TRMM precipitation) is an essential RSPP outperforming other RSPPs. Yet the utilization of TRMM precipitation is still limited by the inaccuracy and low spatial resolution at regional scale. In this paper, linear regression models (LRMs) have been constructed to correct and downscale the TRMM precipitation based on the gauge precipitation at 2257 stations over China from 1998 to 2013. Then, the corrected TRMM precipitation was validated by gauge precipitation at 839 out of 2257 stations in 2014 at station and grid scales. The results show that both monthly and annual LRMs have obviously improved the accuracy of corrected TRMM precipitation with acceptable error, and monthly LRM performs slightly better than annual LRM in Mideastern China. Although the performance of corrected TRMM precipitation from the LRMs has been increased in Northwest China and Tibetan plateau, the error of corrected TRMM precipitation is still significant due to the large deviation between TRMM precipitation and low-density gauge precipitation.","10.1155/2016/3103749","61353","0.3146067"
"155","cold_topic_cluster_no_4","rank_1","700","Performance assessment of CHIRPS, MSWEP, SM2RAIN-CCI, and TMPA precipitation products across India","Accurate long-term estimates of precipitation at fine spatiotemporal resolution are vital for several applications ranging from hydrometeorology to climatology. The availability of a good network of rain gauges, and high precipitation variability associated with two annual monsoon systems and complex topography make India a suitable test-bed to assess the performance of any satellite-based precipitation product This study assesses the performance of latest versions of four multi-satellite precipitation products: (i) Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS), (ii) Multi-Source Weighted-Ensemble Precipitation (MSWEP), (iii) SM2RAIN-Climate Change Initiative (SM2RAIN-CCI), and (iv) TRMM Multisatellite Precipitation Analysis (TMPA) across India using gauge-based observations for the period of 1998-2015 at monthly scale. These four multi-satellite precipitation products are essentially based on different algorithms and input data sets. Among these multi-satellite precipitation products, SM2RAIN-CCI is the only product that does not use rain gauge observations for bias adjustment. Results indicate that CHIRPS and TMPA are comparable to gauge-based precipitation estimates at all-India and sub-regional scales followed by MSWEP estimates. However, SM2RAIN-CCI largely underestimates precipitation across the country as compared to gauge-based observations. The systematic error component in SM2RAIN-CCI dominates as compared to random error component, which suggests the need of a suitable bias correction to SM2RAIN-CCI before integrating it in any application. The overall results indicate that CHIRPS data set could be used for long-term precipitation analyses with rather higher confidence.","10.1016/j.jhydrol.2019.01.036","27822","0.3362069"
"156","cold_topic_cluster_no_4","rank_2","846","Climatology of the Aerosol Extinction-to-Backscatter Ratio from Sun-Photometric Measurements","The elastic lidar equation contains two unknown atmospheric parameters, namely, the particulate optical extinction and backscatter coefficients, which are related through the lidar ratio (i.e., the particulate-extinction-to-backscatter ratio). So far, independent inversion of the lidar signal has been carried out by means of Raman lidars (usually limited to night-time measurements), high-spectral-resolution lidars, or scanning elastic lidars under the assumption of a homogeneously vertically stratified atmosphere. In this paper, we present a procedure to obtain the lidar ratio at 532 nm by a combined Sun-photometer-aerosol-model inversion, where the viability of the solution is largely reinforced by assimilating categorized air-mass back-trajectory information. Thus, iterative lidar-ratio tuning to reconstruct the Sun-photometric aerosol optical depth (AOD) is additionally constrained by the air-mass back trajectories provided by the hybrid single-particle Lagrangian integrated-trajectory model. The retrieved lidar ratios are validated with inversions of lidar data based on the Klett-Fernald-Sasano algorithm and with the Aerosol Robotic Network (AERONET)retrieved lidar ratios. The estimated lidar ratios concur with the AERONET-retrieved lidar ratios and with those of the well-known KFS inversion constrained with Sun-photometric AOD values and embedded single-scattering models. The proposed method can be applied to routinely extract climatological values of the lidar ratio using measurements of direct solar irradiance (more numerous than those of sky radiance).","10.1109/TGRS.2009.2027699","82991","0.2171171"
"157","cold_topic_cluster_no_4","rank_2","846","Transpacific transport and evolution of the optical properties of Asian dust","Five years of CALIPSO lidar layer products are used to study transpacific transport of Asian dust. We focus on possible changes to dust intrinsic optical properties during the course of transport, with specific emphasis on changes to particulate depolarization ratio (PDR). PDR distributions for Asian dust transported across the Pacific are compared to previously reported PDR distributions for African dust transported across the Atlantic. African dust shows a slight decreasing trend in PDR during westward transport across the Atlantic during its most active long-range transport season in summer. Asian dust, on the other hand, shows some spatial variability in PDR over the Pacific during its most active long-range transport season in spring. The dust PDR is generally smaller over the ocean than over the Tarim basin and nearby downwind regions. PDR also shows a decreasing trend with latitude moving northward toward the Arctic, together with an increasing trend in the dust aerosol optical depth (AOD) when passing over polluted Asian regions. Possible explanations include (i) the mixing of dust externally or internally with other types of aerosol over the heavily developed industrial regions in East Asia, and (ii) the downstream mixing of dust plumes from different source regions (i.e., Tarim and Gobi). Dust from different source regions exhibits relatively large differences in PDR, with mean values of 0.34 +/- 0.07, 0.28 +/- 0.06, and 0.30 +/- 0.08, respectively, over the Tarim basin, Gobi Desert and Northwest African source regions. Different transport mechanisms are seen for African dust and Asian dust. Asian dust transport is originated by cold fronts and driven by westerly jets. In contrast, summer African transatlantic dust transport is driven by trade winds and is generally well confined in altitude in the free troposphere throughout the tropics and subtropics. ","10.1016/j.jqsrt.2012.11.011","75304","0.2218750"
"158","cold_topic_cluster_no_4","rank_2","846","Operating an environmentally sustainable city using fine dust level big data measured at individual elementary schools","As the problem of fine dust pollution becomes increasingly serious in South Korea, the country is becoming more interested in obtaining information on fine dust levels. Fine dust level data are sufficiently local to make regional forecasting meaningless. Thus, this study proposes an alternative measurement technique to minimize differences between published and perceived levels of fine dusts. Owing to the large variations in the fine dust levels within urban areas, it is very difficult to provide measurements that are sufficiently area-representative. Because infants and elementary school students are more sensitive to fine dust than adults, it is useful to construct large data sets of measurements of fine dust levels at elementary schools. In Korea, the distribution of elementary schools is consistent with population density, which is useful for analyzing local differences in the fine dust levels in urban areas. This study will provide a basis for big data application to public health policy and infographics using color fuzzy model.","10.1016/j.scs.2017.10.019","39879","0.2280000"
"159","cold_topic_cluster_no_4","rank_2","846","Characteristics of the planetary boundary layer above Wuhan, China based on CALIPSO","Planetary boundary layer height (PBLH) has important implications for human health, weather forecasting, ecology, and climate change. This study aims to investigate the characteristics of the PBLH above Wuhan, China. We propose a new procedure (wavelet covariance and the ideal curve-fitting algorithm) to reveal PBLHs based on the Cloud-Aerosol LIDAR and Infrared Pathfinder Satellite Observations (CALIPSO) attenuated backscatter ratio. Under cloud situation, the results of PBLHs revealed from CALIPSO show a relatively low correlation (R-2 = 0.55) with PBLHs determined using a thermodynamic method. And the results show a significant correlation coefficient (R-2 = 0.86) when the cloudy scenarios are eliminated. Because CALIPSO could have mistakenly classified cloud tops as PBLHs during the formation of stratocumulus clouds. Characteristics of annual and seasonal variations of the PBLH for all sky conditions from June 2006 to September 2013 were also studied. Because of the climatic and geographic characteristics of Wuhan City, the PBLHs display clear annual and seasonal variations. Warmer seasons have deeper PBLHs, while colder seasons are characterized by shallower PBLHs. Over 90% of daytime PBLHs in Wuhan are between 400 and 1800 m, while over 90% of nocturnal PBLHs clustered between 200 m and 1000 m. This research will contribute to improving PBLH input parameters for numerical models and enhance the understanding of the urban planetary boundary layer.","10.1016/j.atmosres.2018.07.024","32773","0.2415730"
"160","cold_topic_cluster_no_4","rank_2","846","Vertical aerosol distribution in the southern hemispheric midlatitudes as observed with lidar in Punta Arenas, Chile (53.2 degrees S and 70.9 degrees W), during ALPACA","Within this publication, lidar observations of the vertical aerosol distribution above Punta Arenas, Chile (53.2 degrees S and 70.9 degrees W), which have been performed with the Raman lidar Polly(XT) from December 2009 to April 2010, are presented. Pristine marine aerosol conditions related to the prevailing westerly circulation dominated the measurements. Lofted aerosol layers could only be observed eight times during the whole measurement period. Two case studies are presented showing long-range transport of smoke from biomass burning in Australia and regionally transported dust from the Patagonian Desert, respectively. The aerosol sources are identified by trajectory analyses with the Hybrid Single-Particle Lagrangian Integrated Trajectory (HYS-PLIT) and FLEXible PARTicle dispersion model (FLEX-PART). However, seven of the eight analysed cases with lofted layers show an aerosol optical thickness of less than 0.05. From the lidar observations, a mean planetary boundary layer (PBL) top height of 1150 +/- 350 m was determined. An analysis of particle backscatter coefficients confirms that the majority of the aerosol is attributed to the PBL, while the free troposphere is characterized by a very low background aerosol concentration. The ground-based lidar observations at 532 and 1064 nm are supplemented by the Aerosol Robotic Network (AERONET) Sun photometers and the space-borne Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) aboard the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO). The averaged aerosol optical thickness (AOT) determined by CALIOP was 0.02 +/- 0.01 in Punta Arenas from 2009 to 2010.","10.5194/acp-19-6217-2019","26375","0.2489796"
"161","cold_topic_cluster_no_4","rank_3","103","The mean properties and variations of the Southern Hemisphere subpolar gyres estimated by Simple Ocean Data Assimilation (SODA) products","Based on the Simple Ocean Data Assimilation (SODA) products, we study the mean properties and variations of the Southern Hemisphere subpolar gyres (SHSGs) in this paper. The results show that the gyre strengths in the SODA estimates are (55.9 +/- 9.8)x10(6) m(3)/s for the Weddell Gyre (WG), (37.0 +/- 6.4)x10(6) m(3)/s for the Ross Gyre (RG), and (27.5 +/- 8.2)x10(6) m(3)/s for the Australian-Antarctic Gyre (AG), respectively. There exists distinct connectivity between the adjacent gyres and then forms an oceanic super gyre structure in the southern subpolar oceans. And the interior exchanges are about (8.0 +/- 3.2)x10(6) m(3)/s at around 70A degrees E and (4.3 +/- 3.1)x10(6) m(3)/s at around 140A degrees E. The most pronounced variation for all three SHSGs occurs on the seasonal time scale, with generally stronger (weaker) SHSGs during austral winter (summer). And the seasonal changes of the gyre structures show that the eastern boundary of the WG and AG extends considerably further east during winter and the interior exchange in the super gyre structure increases accordingly. The WG and RG also show significant semi-annual changes. The correlation analyses confirm that the variations of the gyre strengths are strongly correlated with the changes in the local wind forcing on the semi-annual and seasonal time scales.","10.1007/s13131-016-0901-2","58444","0.1872340"
"162","cold_topic_cluster_no_4","rank_3","103","Air-sea interaction mechanisms and low-frequency climate variability in the South Indian Ocean region","Long-term observations indicate that the Indian Ocean displays significant low-frequency variability in mean sea-level pressure, near-surface wind, cloud and sea-surface temperature (SST). A general circulation model is used to study the response of the atmosphere to an idealized SST anomaly pattern (warm in southern mid-latitudes, cool in southern tropics) that captures the essence of observed multidecadal SST variability as well as that associated with ENSO in the South Indian Ocean. The major objectives are to investigate air-sea interaction mechanisms potentially associated with the variability and whether the atmospheric response to the SST is likely to lead to maintenance or damping of the original SST anomaly pattern, and on what time scale. Two types of experiment are performed to tackle these objectives. An ensemble of roughly 1-year-long integrations suggests that the seasonal-scale response of the atmosphere to the imposed SST anomaly includes reduced genesis and density of cyclones in the mid-to higher latitudes, and an indication of a shift in their tracks relative to climatology. It is argued that these changes together with those to the near-surface winds could be expected to lead to variations in surface fluxes that would tend to reinforce the original SST anomaly pattern on seasonal scales. A 21 year integration of the model with the SST anomaly pattern imposed throughout indicates that a low is generated near, and downstream of, the warm mid-latitude anomaly. On decadal/multidecadal scales, the associated changes to the surface winds are argued as being likely to lead to changes in surface fluxes and in the strength of the South Indian subtropical gyre that would oppose the original anomaly. The current and previous model results together with the observations then support the idea that the observed multidecadal variability in atmospheric circulation and SST of the South Indian Ocean during the past century may have arisen through a combination of basin scale atmosphere-ocean interaction and a remotely forced component. ","10.1002/(SICI)1097-0088(19980330)18:4<391::AID-JOC253>3.0.CO;2-C","91779","0.1892857"
"163","cold_topic_cluster_no_4","rank_3","103","Aerosol Optical Depth and Burden From Large Sea Salt Particles","Sea salt aerosol (SSA) particles are natural aerosols which exert a large impact on various geo-chemical and geophysical processes and on Earth's climate. Emissions of SSA are mainly influenced by the wind near ocean surface. By so far, most models do not consider SSA with dry radius r(d)>10m. For example, the cutoff size of coarse SSA's r(d) in the Community Earth System Model is about 10m. However, laboratory experiments have provided the evidence that numerous large droplets are not accounted for in theoretical estimations of sea spray generation, especially in high-wind conditions, which consequently affect the large SSA emissions. This study assesses the impact of large SSA on aerosol optical depth (AOD) and aerosol burden in the atmosphere by using Community Earth System Model but with a new parameterization of large SSA emissions in high-wind conditions. Our simulations show that considering the large SSA in the model, the simulated AOD over midlatitude and high-latitude oceans are more consistent with the observation from both Moderate Resolution Imaging Spectroradiometer and Aerosol Robotic Network. And we also found that despite dry deposition, large SSA with r(d)>10m can contribute more than 15% to the total AOD over midlatitude and high-latitude oceans, particularly over oceans between 40 and 65 degrees S, and about 28.5% to the total global SSA mass burden. Plain Language Summary One large size sea salt aerosol mode is introduced in model emission to supplement the high-wind conditions. Aerosol optical depth and burden of sea salt aerosol with the large size mode are studied in Community Earth System Model. Simulated aerosol optical depth with the large size mode improve the agreements with observations, especially in high-wind regions.","10.1029/2018JD029814","29113","0.2344000"
"164","cold_topic_cluster_no_4","rank_3","103","High-Resolution Numerical Model for Predicting the Transport and Dispersal of Oil Spilled in the Black Sea","The coupled DieCAST-SSBOM (Shirshov-Stony Brook Oil spill transport Model) circulation-oil spill transport model is used to predict the transport and dispersal of contaminants resulting from representative hypothetical oil spills in the Black Sea. With climatological forcing, the DieCAST model realistically simulates many of the dominant mesoscale features of seasonally-varying large scale circulation and meso-scale features of the circulation including the Rim Current, anticyclonic coastal eddies, headland eddy shedding and vertical stratification. The oil spill model ingests DieCAST surface currents and employs a Lagrangian tracking algorithm to predict the motion of a large number of seeded particles, the sum of which (similar to 1 million) form the oil plume. Basic processes affecting the transport of oil and its fate (advection, sinking, evaporation, etc.) are included as parameters. Various scenarios of hypothetical blowouts of oil in different regions of the sea are simulated and discussed as to their structure, transport and likelihood of coastal contamination. The ultimate objective is to develop an operational oil slick model forced with synoptic winds and air-sea interaction.","10.3319/TAO.2009.04.24.01(IWNOP)","82778","0.2356322"
"165","cold_topic_cluster_no_4","rank_3","103","Estimating pCO(2) from sea surface temperatures in the Atlantic gyres","A simple one-dimensional model, validated with observations from ship of opportunity programs, was ran at different locations in the North and South Atlantic gyres to produce seasonal partial pressure of CO2 (pCO(2))-sea surface temperature (SST) relationships. The pCO(2)-SST relationships obtained at different locations in the North Atlantic gyre can be approximated by two regression lines, one from February to July and another from August to January. An algorithm including SST, latitude, longitude and atmospheric pCO(2) was constructed for each period. The robustness of these relationships was tested along several transects in the North Atlantic gyre and found to be in good agreement with the observations. The same approach was used in the South Atlantic gyre, but more observations are required in this region. In both gyres, the pCO(2)-SST relationships are close to 4%/degreesC, which is higher than the pCO(2)-SST relationships deduced from a CO2 climatology. ","10.1016/S0967-0637(01)00064-4","90366","0.3136364"
"166","cold_topic_cluster_no_4","rank_4","562","Multivariate Spatial Data Fusion for Very Large Remote Sensing Datasets","Global maps of total-column carbon dioxide (CO2) mole fraction (in units of parts per million) are important tools for climate research since they provide insights into the spatial distribution of carbon intake and emissions as well as their seasonal and annual evolutions. Currently, two main remote sensing instruments for total-column CO2 are the Orbiting Carbon Observatory-2 (OCO-2) and the Greenhouse gases Observing SATellite (GOSAT), both of which produce estimates of CO2 concentration, called profiles, at 20 different pressure levels. Operationally, each profile estimate is then convolved into a single estimate of column-averaged CO2 using a linear pressure weighting function. This total-column CO2 is then used for subsequent analyses such as Level 3 map generation and colocation for validation. In principle, total-column CO2 in these applications may be more efficiently estimated by making optimal estimates of the vector-valued CO2 profiles and applying the pressure weighting function afterwards. These estimates will be more efficient if there is multivariate dependence between CO2 values in the profile. In this article, we describe a methodology that uses a modified Spatial Random Effects model to account for the multivariate nature of the data fusion of OCO-2 and GOSAT. We show that multivariate fusion of the profiles has improved mean squared error relative to scalar fusion of the column-averaged CO2 values from OCO-2 and GOSAT. The computations scale linearly with the number of data points, making it suitable for the typically massive remote sensing datasets. Furthermore, the methodology properly accounts for differences in instrument footprint, measurement-error characteristics, and data coverages.","10.3390/rs9020142","48407","0.1561404"
"167","cold_topic_cluster_no_4","rank_4","562","RECONSIDERATION OF THE CAUSE OF DRY AIR IN THE SOUTHERN MIDDLE LATITUDE STRATOSPHERE","A previous comparison of data from the Halogen Occultation Experiment (HALOE) and of output from the NCAR Community Climate Model (CCM2) in austral spring overstated the effect of dehydration in the southern polar vortex on lower stratospheric water vapor in part due to shortcomings in the HALOE retrieval algorithm and in part to remnants of the initially dry stratosphere in the CCM2 run. A new HALOE retrieval and a longer CCM2 run both show more water vapor than previously, but HALOE still shows less in the southern hemisphere than in the northern in September-October 1992. A ''dried-air'' tracer in the CCM2 suggests that polar dehydration influences middle latitudes only below about 100 hPa, and seasonal variations of RALOE H2O imply no influence north of 50 degrees S.","10.1029/95GL01847","92373","0.1774194"
"168","cold_topic_cluster_no_4","rank_4","562","Tropospheric nitrogen dioxide column retrieval from ground-based zenith-sky DOAS observations","We present an algorithm for retrieving tropospheric nitrogen dioxide (NO2) vertical column densities (VCDs) from ground-based zenith-sky (ZS) measurements of scattered sunlight. The method is based on a four-step approach consisting of (1) the differential optical absorption spectroscopy (DOAS) analysis of ZS radiance spectra using a fixed reference spectrum corresponding to low NO2 absorption, (2) the determination of the residual amount in the reference spectrum using a Langley-plot-type method, (3) the removal of the stratospheric content from the daytime total measured slant column based on stratospheric VCDs measured at sunrise and sunset, and simulation of the rapid NO2 diurnal variation, (4) the retrieval of tropospheric VCDs by dividing the resulting tropospheric slant columns by appropriate air mass factors (AMFs). These steps are fully characterized and recommendations are given for each of them. The retrieval algorithm is applied on a ZS data set acquired with a multi-axis (MAX-) DOAS instrument during the Cabauw (51.97A degrees N, 4.93A degrees E, sea level) Intercomparison campaign for Nitrogen Dioxide measuring Instruments (CINDI) held from 10 June to 21 July 2009 in the Netherlands. A median value of 7.9 x 10(15) molec cm(-2) is found for the retrieved tropospheric NO2 VCDs, with maxima up to 6.0 x 10(16) molec cm(-2). The error budget assessment indicates that the overall error Sigma(TVCD) on the column values is less than 28%. In the case of low tropospheric contribution, Sigma(TVCD) is estimated to be around 39% and is dominated by uncertainties in the determination of the residual amount in the reference spectrum. For strong tropospheric pollution events, Sigma(TVCD) drops to approximately 22% with the largest uncertainties on the determination of the stratospheric NO2 abundance and tropospheric AMFs. The tropospheric VCD amounts derived from ZS observations are compared to VCDs retrieved from off-axis and direct-sun measurements of the same MAX-DOAS instrument as well as to data from a co-located SystSme d'Analyse par Observations Z,nithales (SAOZ) spectrometer. The retrieved tropospheric VCDs are in good agreement with the different data sets with correlation coefficients and slopes close to or larger than 0.9. The potential of the presented ZS retrieval algorithm is further demonstrated by its successful application on a 2-year data set, acquired at the NDACC (Network for the Detection of Atmospheric Composition Change) station Observatoire de Haute Provence (OHP; Southern France).","10.5194/amt-8-2417-2015","67451","0.1786096"
"169","cold_topic_cluster_no_4","rank_4","562","Evaluation of optimization of lidar temperature analysis algorithms using simulated data","Temperature lidar data have been simulated in order to test the Jet Propulsion Laboratory (JPL) SO3ANL version 3.2 and Service d'Aeronomie du Centre National de la Recherche Scientifique (CNRS/SA) TEMPER version 2.1 lidar temperature analysis software. Assuming known atmospheric temperature-pressure-density profiles, theoretical raw-photons lidar profiles have been calculated using the actual characteristics of two JPL lidar instruments, located at the Table Mountain Facility (TMF) in California and the Mauna Loa Observatory (MLO), Hawaii, and the CNRS/SA Rayleigh lidar, located at the Observatoire de Haute-Provence (OHP) in France. The simulations were performed for an initial climatological profile taken from the CIRA-86 model and for various profiles derived from this model including realistic atmospheric disturbances. Comparisons between the original and retrieved temperature profiles revealed errors of several kelvins for both the JPL and the CNRS/SA programs. By varying parameters in the simulation, it was possible to determine both the source and the magnitude of these errors. Once identified, the errors were corrected, and the analysis programs were optimized, leading to new operational versions of these programs (SO3ANL version 3.6 and TEMPER version 2.2), An accurate accounting of the temperature lidar analysis errors, before and after this work, is presented.","10.1029/97JD03494","91780","0.1881720"
"170","cold_topic_cluster_no_4","rank_4","562","Intercomparison of Stratospheric Aerosol and Gas Experiment (SAGE) with Umkehr[64] and Umkehr[92] ozone profiles and time series: 1979-1991","Analyzing coincident observations of ozone profiles at 15 Umkehr stations with Stratospheric Aerosol and Gas Experiment (SAGE) I and SAGE II measurements within 1000 km and 12 hours in low-aerosol conditions between 1979 and 1991, we find improved agreement between Umkehr and SAGE retrievals using the new (1992) Umkehr algorithm compared to the previous(1964) algorithm, but some significant differences remain. The column ozone amounts in layers 4 through 10 for both old Umkehr[64] (after adjustment for the scale change from the International Ozone Commission/World Meteorological Organization 1968 scale to the Bass and Parts [1985] scale) and new Umkehr[92] retrievals are approximately 5-6% lower than SAGE column amounts. The structure of the aerosol-corrected Umkehr[92] profiles compares much more favorably to SAGE than did the Umkehr[64] profiles in layers 4 to 7 (20-35 km), with considerable consistency in the vertical structure of differences across most sites. The layer-ozone differences, however, increase from zero in layer 4 to -15% (Umkehr[92] low) in layer 8. Belsk and Sapporo are the only 2 sites of the 15 analyzed here that exhibit somewhat dissimilar vertical structure in their differences versus SAGE. The Umkehr[92] a priori climatology contains less ozone in the lower layers (2-5) than does SAGE and somewhat more in the upper layers (7-9). However, these differences in a priori climatology do not significantly affect the broad altitude structure in the SAGE-Umkehr layer-mean ozone differences. On average, the Umkehr[92] profiles possess a correlation between 0.3 and 0.5 (higher at some stations) with the SAGE-measured ozone in individual layers 4 to 8. The time series of the Umkehr[92] and SAGE measurements typically exhibit similar trends except for discontinuous changes noted at Mauna Loa and Kagoshima.","10.1029/98JD02440","91573","0.2165354"
"171","cold_topic_cluster_no_4","rank_5","797","Assessing the effects of slope gradient and land use change on soil quality degradation through digital mapping of soil quality indices and soil loss rate","Slope gradient and land use change are known to influence soil quality and the assessment of soil quality is important in determining sustainable land-use and soil-management practices. In this study, soil quality indices (SQIs) were developed by quantifying several soil properties to discriminate the effects of slope gradient and land use change on soil quality in 480 km(2) of agricultural land in Kurdistan Province, Iran. Three soil quality indices (SQIs) were used. Each of the soil quality indices was calculated using two linear and non-linear scoring methods and two soil indicator selection approaches, a Total Data Set (TDS) and a Minimum Data Set (MDS). Nine soil quality indicators: pH, Electrical Conductivity (EC), Organic Carbon (OC), Cation Exchange Capacity (CEC), Total Naturalized Value (TNV), Soil Erodibility (K), Porosity (P), Mean Weight Diameter (MWD), and Bulk Density (BD) and soil loss rate were measured for 110 soil samples (0-30 cm depth). Soil quality indices maps were developed using digital soil mapping methods. The > 10% slope class had the highest soil loss rate and highest percentage of soils with very low quality (grade V) based on all SQIs. The results showed that soil quality was better estimated using the Weighted Additive Soil Quality Index (SQI(w)) (r(2) = 0.78) compared to SQI(a) (the Additive Soil Quality Index) and SQI(n) (the Nemoro Soil Quality Index). The agreement values of all SQIs for the non-linear scoring method were higher than the linear scoring method. The mean values of all SQIs and the soil loss rate were higher and lower in rangeland than cropland, respectively, but they were not significantly different because of intensive grazing. Slopes with a large gradient and where land use was converted to agriculture were characterized by low values of SQIs, suggesting a recovery of soil quality through changing to sustainable practices and abandoning over grazing in these areas.","10.1016/j.geoderma.2017.12.024","37680","0.2506667"
"172","cold_topic_cluster_no_4","rank_5","797","Comparison of soil variability in a detailed and a reconnaissance soil map in central Iran","Quality of soil maps is a prerequisite for rational land use and soil management. To assess the accuracy of the existing reconnaissance soil map (1:50,000) of the Chaharmahal Bakhtiari province, Iran, a second-order soil survey was conducted. Using aerial photo interpretation, profile description, and field observations, a detailed soil map (1:20,000) was prepared. Then, 85 pedons were described and samples were taken for laboratory analyses. The location of pedons was determined with a Global Positioning System (GPS) and then marked on both soil maps using ILWIS software. Properties of the surface horizons including percentage of gravel, clay, sand, silt, CaCO3 equivalent, organic matter, and the thickness of A horizon were determined. Although detailed soil map units were more homogenous, purity of map units at family and series levels was lower than those expected in American Soil Survey Manual. Taxonomic purity at series level was 18.75% to 33.3% and 0% to 1.3% for detailed and reconnaissance soil maps, respectively. With respect to similar soils, interpretive purity could improve to 75% in detailed soil map. For all the surface soil properties except organic matter, the detailed soil map had within-map unit variances that were smaller than the total variance, whereas reconnaissance soil map variance remained high. Soil classification in both the detailed and the reconnaissance mapping units accounted for 5% to 45% and 1.1% to 27% of the total variance for these properties. Sampling demand to predict mean values was much larger in the reconnaissance soil map for most of the topsoil properties. Laboratory studies showed that the detailed soil map units in similar physiographic units could have different soils and vice versa. It appears that the definition of mapping units based on guidelines by Soil Survey Manual is rather optimistic and does not have sufficient reliability for using in sustainable agriculture. ","10.1016/S0016-7061(02)00252-5","89984","0.2553191"
"173","cold_topic_cluster_no_4","rank_5","797","Assessment for soil loss by using a scheme of alterative sub-models based on the RUSLE in a Karst Basin of Southwest China","Accurate assessment of soil loss caused by rainfall is essential for natural and agricultural resources management. Soil erosion directly affects the environment and human sustainability. In this work, the empirical and contemporary model of revised universal soil loss equation (RUSLE) was applied for simulating the soil erosion rate in a karst catchment using remote sensing data and geographical information systems. A scheme of alterative""sub-models was adopted to calculate the rainfall erosivity (R), soil erodibility (K), slope length and steepness (LS), cover management (C) and conservation practice (P) factors in the geographic information system (GIS) environment. A map showing the potential of soil erosion rate was produced by the RUSLE and it indicated the severe soil erosion in the study area. Six classes of erosion rate are distinguished from the map: 1) minimal, 2) low, 3) medium, 4) high, 5) very high, and 6) extremely high. The RUSLE gave a mean annual erosion rate of 30.24 Mg ha(-1) yr(-1) from the 1980s to 2000s. The mean annual erosion rate obtained using RUSLE is consistent with the result of previous research based on in situ measurement from 1980 to 2009. The high performance of the RUSLE model indicates the reliability of the sub-models and possibility of applying the RUSLE on quantitative estimation. The result of the RUSLE model is sensitive to the slope steepness, slope length, vegetation factors and digital elevation model (DEM) resolution. The study suggests that attention should be given to the topographic factors and DEM resolution when applying the RUSLE on quantitative estimation of soil loss.","10.1016/S2095-3119(16)61507-1","55386","0.2708029"
"174","cold_topic_cluster_no_4","rank_5","797","Efficiency of wheat straw mulching in reducing soil and water losses from three typical soils of the Loess Plateau, China","Mulching the soil surface with a layer of plant residue is considered an effective method of conserving water and soil because it increases water infiltration into the soil, reduces surface runoff and the soil erosion, and reduces flow velocity and the sediment carrying capacity of overland flow. However, application of plant residues increases operational costs and so optimal levels of mulch in order to prevent soil and/or water losses should be used according to the soil type and rainfall and slope conditions. In this study, the effect of wheat straw mulch rate on the total runoff and total soil losses from 60-mm simulated rainstorms was assessed for two intensive rainfalls (90 and 180 mm h(-1)) on three slope gradients typical conditions on the Loess Plateau of China and elsewhere. For short slopes (1 m), the optimal mulch rate to save water for a silt loam and a loam soil was 0.4 kg m(-2). However, for a clay loam soil the mulch rate of 0.4 kg m(-2) would be optimal only under the 90 mm h(-1) rainfall; 0.8 kg m(-2) was required for the 180 mm h(-1). In order to save soil, a mulch rate of 0.2 kg m(-2) on the silt loam slopes prevented 60%-80% of the soil losses. For the loam soil, mulch at the rate of 0.4 kg m(-2) was essential in most cases in order to reduce soil losses substantially. For the clay loam, 0.4 kg m(-2) may be optimal under the 90 mm h(-1) rain, but 0.8 kg m(-2) may be required for the 180 mm h(-1) rainstorm. These optimal values would also need to be considered alongside other factors since the mulch may have value if used elsewhere. Hence doubling the optimal mulch rate for the silt loam soil from 0.2 kg m(-2) or the clay loam soil under 90 mm h(-1) rainfall from 0.4 kg m(-2) in order to achieve a further 10% reduction in soil loss needs to be assessed in that context. Therefore, Optimal mulch rate can be an effective approach to virtually reduce costs or to maximize the area that can be treated. Meantime, soil conservationist should be aware that levels of mulch for short slopes might not be suitable for long slopes. ","10.1016/j.iswcr.2019.08.003","19815","0.2710145"
"175","cold_topic_cluster_no_4","rank_5","797","Quantification of Soil Losses along the Coastal Protected Areas in Kenya","Monitoring of improper soil erosion empowered by water is constantly adding more risk to the natural resource mitigation scenarios, especially in developing countries. The demographical pattern and the rate of growth, in addition to the impairments of the rainfall pattern, are consequently disposed to adverse environmental disturbances. The current research goal is to evaluate soil erosion triggered by water in the coastal area of Kenya on the district level, and also in protected areas. The Revised Universal Soil Loss Equation (RUSLE) model was exercised to estimate the soil loss in the designated study area. RUSLE input parameters were functionally realized in terms of rainfall and runoff erosivity factor (R), soil erodibility factor (K), slope length and gradient factor (LS), land cover management factor (C) and slope factor (P). The realization of RUSLE input parameters was carried out using different dataset sources, including meteorological data, soil/geology maps, the Digital Elevation Model (DEM) and processing of satellite imagery. Out of 26 districts in coastal area, eight districts were projected to have mean annual soil loss rates of >10 t.ha(-1).y(-1): Kololenli (19.709 t.ha(-1).y(-1)), Kubo (14.36 t.ha(-1).y(-1)), Matuga (19.32 t.ha(-1).y(-1)), Changamwe (26.7 t.ha(-1).y(-1)), Kisauni (16.23 t.ha(-1).y(-1)), Likoni (27.9 t.ha(-1).y(-1)), Mwatate (15.9 t.ha(-1).y(-1)) and Wundanyi (26.51 t.ha(-1).y(-1)). Out of 34 protected areas at the coastal areas, only four were projected to have high soil loss estimation rates >10 t.ha(-1).y(-1): Taita Hills (11.12 t.ha(-1).y(-1)), Gonja (18.52 t.ha(-1).y(-1)), Mailuganji (13.75.74 t.ha(-1).y(-1)), and Shimba Hills (15.06 t.ha(-1).y(-1)). In order to mitigate soil erosion in Kenya's coastal areas, it is crucial to regulate the anthropogenic disturbances embedded mainly in deforestation of the timberlands, in addition to the natural deforestation process caused by the wildfires.","10.3390/land9050137","11390","0.2838235"
"176","cold_topic_cluster_no_5","rank_1","760","Aerosol optical properties from columnar data during the second Aerosol Characterization Experiment on the south coast of Portugal","[1] Measurements of aerosol optical properties over the atmospheric column (aerosol optical thickness, spectral angular sky radiance (sky brightness), and downwelling hemispheric flux) have been used to derive climate-relevant aerosol parameters such as the phase function, the broadband single-scattering albedo, and the refractive index. These parameters are needed to estimate the direct short-wave radiative forcing by aerosols and to validate aerosol models in the satellite retrieval algorithms. Values of the broadband single-scattering albedo obtained in this study range between omega(0) = 0.98 (marine aerosols) and 0.90 (continental pollution aerosols). The columnar ambient broadband refractive index is found to be m = 1.39 +/- 0.044 - i (< 0.003) for marine conditions and m = 1.48 +/- 0.058 - i (0.01 +/- 0.003) for polluted continental aerosols. Nonsphericity is shown to be important in the case of marine aerosols. Moreover, aerosol nonsphericity gives an additional contribution to the negative short-wave radiative forcing of marine aerosols under clear-sky conditions, which can be estimated as being 30 up to 50% of the radiative forcing estimated for spherical marine aerosols. In the case of continental polluted aerosols the optical properties can be represented by spherical particles, and no additional shape effect has to be considered. However, the aerosol absorption leads to an increase of about 40% of the radiative forcing estimated for nonabsorbing aerosol of the same size distribution.","10.1029/2002JD002196","90107","0.3318681"
"177","cold_topic_cluster_no_5","rank_1","760","Comparison of moderate resolution Imaging spectroradiometer (MODIS) and aerosol robotic network (AERONET) remote-sensing retrievals of aerosol fine mode fraction over ocean","[1] Aerosol particle size is one of the fundamental quantities needed to determine the role of aerosols in forcing climate, modifying the hydrological cycle, and affecting human health and to separate natural from man-made aerosol components. Aerosol size information can be retrieved from remote-sensing instruments including satellite sensors such as Moderate Resolution Imaging Spectroradiometer ( MODIS) and ground-based radiometers such as Aerosol Robotic Network (AERONET). Both satellite and ground-based instruments measure the total column ambient aerosol characteristics. Aerosol size can be characterized by a variety of parameters. Here we compare remote-sensing retrievals of aerosol fine mode fraction over ocean. AERONET retrieves fine mode fraction using two methods: the Dubovik inversion of sky radiances and the O'Neill inversion of spectral Sun measurements. Relative to the Dubovik inversion of AERONET sky measurements, MODIS slightly overestimates fine fraction for dust-dominated aerosols and underestimates in smoke-and pollution-dominated aerosol conditions. Both MODIS and the Dubovik inversion overestimate fine fraction for dust aerosols by 0.1 - 0.2 relative to the O'Neill method of inverting AERONET aerosol optical depth spectra. Differences between the two AERONET methods are principally the result of the different definitions of fine and coarse mode employed in their computational methodologies. These two methods should come into better agreement as a dynamic radius cutoff for fine and coarse mode is implemented for the Dubovik inversion. MODIS overestimation in dust-dominated aerosol conditions should decrease significantly with the inclusion of a nonspherical model.","10.1029/2005JD005760","88191","0.3341880"
"178","cold_topic_cluster_no_5","rank_1","760","Aerosol type over east Asian retrieval using total and polarized remote Sensing","Satellite observation in East Asia is important because aerosols of this region are very complex, not only from anthropogenic but also from natural sources, which have been recognized as a major source of regional and global air pollution. However, retrieving aerosols properties over land is difficult because of surface reflection, complex aerosol composition, and aerosol absorption. In this study, a new aerosol retrieval method including two steps was developed to invert AOT and aerosol type using PARASOL TOA measurements. Both reflectance (490 nm) and polarized reflectance (670 nm and 865 nm) are used in the algorithm to improve the accuracy of retrieved AOT and aerosol type. AOT was determined by comparing the aerosol reflectance and polarized reflectance with pre-calculated LUTs of six aerosols clustered from the inversion products of AERONET sites in East Asia. The comparison between PARASOL and AERONET showed improvement in total AOT using the suggested methodology. Three case studies show inverted aerosol AOT and types agreed well with specific aerosol types. ","10.1016/j.jqsrt.2013.05.028","73121","0.3567164"
"179","cold_topic_cluster_no_5","rank_1","760","Radiative signature of absorbing aerosol over the eastern Mediterranean basin","The effects of absorbing aerosols on the atmospheric radiation budget and dynamics over the eastern Mediterranean region are studied using satellites and ground-based observations, and radiative transfer model calculations, under summer conditions. Climatology of aerosol optical depth (AOD), single scattering albedo (SSA) and size parameters were analyzed using multiyear (1999-2012) observations from Moderate Resolution Imaging Spectroradiometer (MODIS), Multi-angle Imaging SpectroRadiometer (MISR) and AErosol RObotic NET-work (AERONET). Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP)-derived aerosol vertical distributions and their classifications are used to calculate the AOD of four dominant aerosol types: dust, polluted dust, polluted continental, and marine aerosol over the region. The seasonal mean (June-August 2010) AODs are 0.22 +/- 0.02, 0.11 +/- 0.04, 0.10 +/- 0.04 and 0.06 +/- 0.01 for polluted dust, polluted continental, dust and marine aerosol, respectively. Changes in the atmospheric temperature profile as a function of absorbing aerosol loading were derived for the same period using observations from the AIRS satellite. We inferred heating rates in the aerosol layer of similar to 1.7 +/- 0.8 K day(-1) between 925 and 850 hPa, which is attributed to aerosol absorption of incoming solar radiation. Radiative transfer model (RTM) calculations show significant atmospheric warming for dominant absorbing aerosol over the region. A maximum atmospheric forcing of +16.7 +/- 7.9 W m(-2) is calculated in the case of polluted dust, followed by dust (+9.4 +/- 4.9 W m(-2)) and polluted continental (+6.4 +/- 4.5 W m(-2)). RTM-derived heating rate profiles for dominant absorbing aerosol show warming of 0.1-0.9 K day(-1) in the aerosol layer (<3.0 km altitudes), which primarily depend on AODs of the different aerosol types. Diabatic heating due to absorbing aerosol stabilizes the lower atmosphere, which could significantly reduce the atmospheric ventilation. These conditions can enhance the ""pollution pool"" over the eastern Mediterranean.","10.5194/acp-14-7213-2014","72332","0.3686567"
"180","cold_topic_cluster_no_5","rank_1","760","Global aerosol optical properties and application to Moderate Resolution Imaging Spectroradiometer aerosol retrieval over land","As more information about global aerosol properties has become available from remotely sensed retrievals and in situ measurements, it is prudent to evaluate this new information, both on its own and in the context of satellite retrieval algorithms. Using the climatology of almucantur retrievals from global Aerosol Robotic Network (AERONET) Sun photometer sites, we perform cluster analysis to determine aerosol type as a function of location and season. We find that three spherical-derived types (describing fine-sized dominated aerosol) and one spheroid-derived types (describing coarse-sized dominated aerosol, presumably dust) generally describe the range of AERONET observed global aerosol properties. The fine-dominated types are separated mainly by their single scattering albedo (omega(0)), ranging from nonabsorbing aerosol (omega(0)similar to 0.95) in developed urban/industrial regions, to moderately absorbing aerosol (omega(0)similar to 0.90) in forest fire burning and developing industrial regions, to absorbing aerosol (omega(0)similar to 0.85) in regions of savanna/grassland burning. We identify the dominant aerosol type at each site, and extrapolate to create seasonal 1 degrees x1 degrees maps of expected aerosol types. Each aerosol type is bilognormal, with dynamic (function of optical depth) size parameters (radius, standard deviation, volume distribution) and complex refractive index. Not only are these parameters interesting in their own right, they can also be applied to aerosol retrieval algorithms, such as to aerosol retrieval over land from Moderate Resolution Imaging Spectroradiometer. Independent direct-Sun AERONET observations of spectral aerosol optical depth (tau) are consistent the spectral dependence of the models, indicating that our derived aerosol models are relevant.","10.1029/2006JD007815","86644","0.3696721"
"181","cold_topic_cluster_no_5","rank_2","624","Objective identification and tracking of multicentre cyclones in the ERA-Interim reanalysis dataset","We present a novel cyclone identification and tracking method that explicitly recognizes multicentre cyclones (MCCs), defined as a cyclonic system with two or three sea-level pressure minima within its outermost contour. The method allows for the recognition of cyclone merger and splitting events in a natural way, and provides a consistent measure of the cyclone extent. Using the ERA-Interim reanalysis dataset, we compute a climatology using this method and show that MCCs occur in about 32 of all cyclone tracks and are much more prevalent in more intense storms. We also show that the method permits reconnection of tracks that would have been spuriously split using a conventional method. We present spatial maps of cyclone mergers, splitting, genesis and lysis using the method and also compute statistics of precipitation falling within cyclones, showing that it is strongly concentrated in the most intense cyclones. ","10.1002/qj.948","77836","0.2629630"
"182","cold_topic_cluster_no_5","rank_2","624","PATTERNS OF CYCLONE ACTIVITY INFLUENCING CHANGJIANG RIVER-HUAIHE RIVER VALLEYS IN SPRING REVEALED USING THE CYCLONE AREA ALGORITHM","Due to the topography and local nonuniform distribution of heating, extratropical cyclones in the lower troposphere usually have complex shapes and structures, and there remain some uncertainties in identifying them. Using a modified cyclone area automatic objective recognition algorithm for cyclones, we investigated the patterns of spring cyclone activities affecting Changjiang River-Huaihe River valleys (CHV) of China during the previous 37 years. The results indicated that the algorithm performs well in reproducing the cyclogenesis, movement, and cyclolysis of cyclones in CHV. Three new perspectives were found. (1) Most influential cyclones have relatively short travel distances and lifetimes, and they are typically excluded when conducting synoptic-scale cyclone tracking. (2) The cyclogenesis location of influential cyclones in spring displays multi-source characteristics. In particular, the influential cyclones originated in northern China account for 43% with more marked mobility compared to the locally generated cyclones, although most of their centers do not enter CHV. (3) Multi-center cyclones appear mainly in Da Xingan Mountains which is on the east side of the Mongolian Plateau and over the East China Sea. These cyclones are relatively large in horizontal scale and have relatively long lifetimes.","10.16555/j.1006-8775.2019.04.006","19497","0.2750000"
"183","cold_topic_cluster_no_5","rank_2","624","A 10-yr Climatology of Diabatic Rossby Waves in the Northern Hemisphere","Diabatic Rossby waves (DRWs) are low-tropospheric positive potential vorticity (PV) anomalies in moist and sufficiently baroclinic regions. They regenerate continuously by moist-diabatic processes and potentially develop into explosively intensifying cyclones. In this study a specific DRW-tracking algorithm is developed and applied to operational ECMWF analyses to compile a first climatology of DRWs in the Northern Hemisphere for the years 2001-10. DRWs are more frequent over the North Pacific than over the North Atlantic with on average 81 and 43 systems per year, respectively. Less than 15% of these systems intensify explosively, on average 12 per year over the Pacific and 5 over the Atlantic. DRWs are most frequent in summer but most of the explosively intensifying DRWs occur in autumn and winter. DRWs are generated typically between 30 degrees-50 degrees N over the eastern parts of the continents and the western/central parts of the oceans. They propagate fairly zonally along the midlatitude baroclinic zone. The generation of the initial low-tropospheric PV anomalies goes along with precipitation processes in characteristic flow patterns, which correspond to 1) flow around the subtropical high against the midlatitude baroclinic zone, 2) flow induced by an upper-level cutoff or a (tropical) cyclone against the baroclinic zone, 3) upper-level trough-induced ascent at the baroclinic zone, and 4) PV remnants of a tropical cyclone or a mesoscale convective system that are advected into the baroclinic zone where they start propagating as a DRW. In most cases, explosive intensification of DRWs occurs through interaction with a preexisting upper-level trough.","10.1175/MWR-D-12-00012.1","75170","0.2758333"
"184","cold_topic_cluster_no_5","rank_2","624","A PV Perspective on the Vertical Structure of Mature Midlatitude Cyclones in the Northern Hemisphere","Development of extratropical cyclones can be seen as an interplay of three positive potential vorticity anomalies: an upper-level stratospheric intrusion, low-tropospheric diabatically produced potential vorticity (PV), and a warm anomaly at the surface acting as a surrogate PV anomaly. This study, based on the interim ECMWF Re-Analysis (ERA-Interim) dataset, quantifies the amplitude of the PV anomalies of mature extratropical cyclones in different regions in the Northern Hemisphere on a climatological basis. A tracking algorithm is applied to sea level pressure (SLP) fields to identify cyclone tracks. Surface potential temperature anomalies Delta theta and vertical profiles of PV anomalies Delta PV are calculated at the time of the cyclones' minimum SLP in a vertical cylinder around the surface cyclone center. To compare the cyclones' characteristics they are grouped according to their location and intensity. Composite Delta PV profiles are calculated for each region and intensity class at the time of minimum SLP and during the cyclone intensification phase. In the mature stage all three anomalies are on average larger for intense than for weak winter cyclones [e.g., 0.6 versus 0.2 potential vorticity units (PVU; 1 PVU = 10(-6) K kg(-1) m(2) s(-1)) at lower levels, and 1.5 versus 0.5 PVU at upper levels]. The regional variability of the cyclones' vertical structure and the profile evolution is prominent (cyclones in some regions are more sensitive to the amplitude of a particular anomaly than in other regions). Values of Delta theta and low-level Delta PV are on average larger in the western parts of the oceans than in the eastern parts. Results for summer are qualitatively similar, except for distinctively weaker surface Delta theta values.","10.1175/JAS-D-11-050.1","78230","0.3014599"
"185","cold_topic_cluster_no_5","rank_2","624","High-stability algorithm for the three-pattern decomposition of global atmospheric circulation","In order to study the atmospheric circulation from a global-wide perspective, the three-pattern decomposition of global atmospheric circulation (TPDGAC) has been proposed in our previous studies. In this work, to easily and accurately apply the TPDGAC in the diagnostic analysis of atmospheric circulation, a high-stability algorithm of the TPDGAC has been presented. By using the TPDGAC, the global atmospheric circulation is decomposed into the three-dimensional (3D) horizontal, meridional, and zonal circulations (three-pattern circulations). In particular, the global zonal mean meridional circulation is essentially the three-cell meridional circulation. To demonstrate the rationality and correctness of the proposed numerical algorithm, the climatology of the three-pattern circulations and the evolution characteristics of the strength and meridional width of the Hadley circulation during 1979-2015 have been investigated using five reanalysis datasets. Our findings reveal that the three-pattern circulations capture the main features of the Rossby, Hadley, and Walker circulations. The Hadley circulation shows a significant intensification during boreal winter in the Northern Hemisphere and shifts significantly poleward during boreal (austral) summer and autumn in the Northern (Southern) Hemisphere.","10.1007/s00704-017-2226-2","36120","0.3164384"
"186","cold_topic_cluster_no_5","rank_3","111","The surface downward longwave radiation in the ECMWF forecast system","The surface downward longwave radiation, computed by the European Centre for Medium-Range Weather Forecasts (ECMWF) forecast system used for the ECMWF 40-yr reanalysis, is compared with surface radiation measurements for the April-May 1999 period, available as part of the Baseline Surface Radiation Network (BSRN), Surface Radiation (SURFRAD), and Atmospheric Radiation Measurement (ARM) programs. Emphasis is put on comparisons on a 1-h basis, as this allows discrepancies to be more easily linked to differences between model description and observations of temperature, humidity, and clouds. It is also possible to compare the model and observed temporal variability in the surface radiation fluxes. Comparisons are first carried out at locations for which the spectral model orography differs from the actual station height. Sensitivity of the model fluxes to various algorithms to correct for this discrepancy is explored. A simple interpolation-extrapolation scheme for pressure, temperature, and specific humidity allows the improvement of model calculations of the longwave surface fluxes in most cases. Intercomparisons of surface longwave radiation are presented for the various longwave radiation schemes operational since the 15-yr ECMWF Re-Analysis (ERA-15) was performed. The Rapid Radiation Transfer Model of Mlawer et al., now operational at ECMWF, is shown to correct for the major underestimation in clear-sky downward longwave radiation seen in ERA-15. Sensitivity calculations are also carried out to explore the role of the cloud optical properties, cloud effective particle size, and aerosols in the representation of the surface downward longwave radiation.","10.1175/1520-0442(2002)015<1875:TSDLRI>2.0.CO;2","90248","0.1782178"
"187","cold_topic_cluster_no_5","rank_3","111","Consistent retrieval methods to estimate land surface shortwave and longwave radiative flux components under clear-sky conditions","Shortwave (0.3-3 mu m) and longwave (3-50 mu m) surface radiative flux components have been widely used in numerical prediction, meteorology, hydrology, biomass estimation, surface energy circulation and climate change studies, etc. However, during past decades, these components were usually estimated independently using different methods, possibly causing inconsistent estimation biases due to different atmospheric parameters and algorithms, especially for net surface fluxes. Two methods have been proposed in this paper to simultaneously derive surface shortwave (or longwave) radiative flux components based on MODIS products using an artificial neural network (ANN). The validation results show that the maximum root-mean-square error for downward and net shortwave radiative fluxes is less than 45 W/m(2), about 60 W/m(2) for direct solar radiation and 25 W/m(2) for all longwave fluxes, which are comparable or even better than existing algorithms, thus demonstrating their feasibility and efficacy. The ANN-based models are then applied over the Tibetan Plateau region and the characteristics of the surface radiative flux components over such areas are analyzed. ","10.1016/j.rse.2012.04.026","76797","0.1833333"
"188","cold_topic_cluster_no_5","rank_3","111","Development of a 10-year (2001-2010) 0.1 degrees data set of land-surface energy balance for mainland China","In the absence of high-resolution estimates of the components of surface energy balance for China, we developed an algorithm based on the surface energy balance system (SEBS) to generate a data set of land-surface energy and water fluxes on a monthly timescale from 2001 to 2010 at a 0.1 x 0.1 degrees spatial resolution by using multi-satellite and meteorological forcing data. A remote-sensing-based method was developed to estimate canopy height, which was used to calculate roughness length and flux dynamics The land-surface flux data set was validated against ""ground-truth"" observations from 11 flux tower stations in China The estimated fluxes correlate well with the stations' measurements for different vegetation types and climatic conditions (average bias = 11.2 Wm(-2), RMSE = 22.7 Wm(-2)). The quality of the data product was also assessed against the GLDAS data set. The results show that our method is efficient for producing a high-resolution data set of surface energy flux for the Chinese landmass from satellite data. The validation results demonstrate that more accurate downward long-wave radiation data sets are needed to be able to estimate turbulent fluxes and evapotranspiration accurately when using the surface energy balance model. Trend analysis of land-surface radiation and energy exchange fluxes revealed that the Tibetan Plateau has undergone relatively stronger climatic change than other parts of China during the last 10 years. The capability of the data set to provide spatial and temporal information on water-cycle and land atmosphere interactions for the Chinese landmass is examined. The product is free to download for studies of the water cycle and environmental change in China","10.5194/acp-14-13097-2014","71741","0.1870690"
"189","cold_topic_cluster_no_5","rank_3","111","Surface solar irradiance from SCIAMACHY measurements: algorithm and validation","Broadband surface solar irradiances (SSI) are, for the first time, derived from SCIAMACHY (SCanning Imaging Absorption spectroMeter for Atmospheric CartograpHY) satellite measurements. The retrieval algorithm, called FRESCO (Fast REtrieval Scheme for Clouds from the Oxygen A band) SSI, is similar to the Heliosat method. In contrast to the standard Heliosat method, the cloud index is replaced by the effective cloud fraction derived from the FRESCO cloud algorithm. The MAGIC (Mesoscale Atmospheric Global Irradiance Code) algorithm is used to calculate clear-sky SSI. The SCIAMACHY SSI product is validated against globally distributed BSRN (Baseline Surface Radiation Network) measurements and compared with ISCCP-FD (International Satellite Cloud Climatology Project Flux Dataset) surface shortwave downwelling fluxes (SDF). For one year of data in 2008, the mean difference between the instantaneous SCIAMACHY SSI and the hourly mean BSRN global irradiances is -4 Wm(-2) (-1%) with a standard deviation of 101 Wm(-2) (20%). The mean difference between the globally monthly mean SCIAMACHY SSI and ISCCP-FD SDF is less than -12 Wm(-2) (-2%) for every month in 2006 and the standard deviation is 62 Wm(-2) (12%). The correlation coefficient is 0.93 between SCIAMACHY SSI and BSRN global irradiances and is greater than 0.96 between SCIAMACHY SSI and ISCCP-FD SDF. The evaluation results suggest that the SCIAMACHY SSI product achieves similar mean bias error and root mean square error as the surface solar irradiances derived from polar orbiting satellites with higher spatial resolution.","10.5194/amt-4-875-2011","80900","0.1908333"
"190","cold_topic_cluster_no_5","rank_3","111","Evaluation of broadband surface solar irradiance derived from the Ozone Monitoring Instrument","Surface solar irradiance (SSI) data are important for planning and estimating the production of solar power plants. Long-term high quality surface solar radiation data are needed for monitoring climate change. This paper presents a new surface solar irradiance dataset, the broadband (0.2-4 mu m) surface solar irradiance product derived from the Ozone Monitoring Instrument (OMI). The OMI SSI algorithm is based on the Heliosat method and uses the OMI O-2-O-2 cloud product as main input. The OMI SSI data are validated against the globally distributed Baseline Surface Radiation Network (BSRN) measurements at 19 stations for the year 2008. Furthermore, the monthly mean OMI SSI data are compared to independent surface solar irradiance products from International Satellite Cloud Climatology Project Flux Data (ISCCP-FD) and Clouds and the Earth's Radiant Energy System (CERES) data for the year 2005. The mean difference between OMI SSI and BSRN global (direct + diffuse) irradiances is - 1.2W m(-2) (-0.2%), the root mean square error is 100.1 W m(-2) (18.1%), and the mean absolute error is 67.8W m(-2) (12.2%). The differences between OMI SSI and BSRN global irradiances are smaller over continental and coastal sites and larger over deserts and islands. OMI SSI has a good agreement with the CERES shortwave (SW) model B surface downward flux (SDF) product. The correlation coefficient and index of agreement between monthly mean 1-degree gridded OMI SSI and CERES SW SDF are >0.99. OMI SSI is lower than CERES SW SDF which is partly due to the solar zenith angle. On average, OMI SSI is 13.5 W m(-2) (2.5%) lower than the ISCCPFD SW surface downward flux and the correlation coefficient and index of agreement are >0.98 for every month. ","10.1016/j.rse.2014.03.036","70289","0.2172662"
"191","cold_topic_cluster_no_5","rank_4","592","Interannual Variability of Young Ice in the Arctic Estimated Between 2002 and 2009","The recently observed reduction in perennial ice in the Arctic has given rise to a corresponding increase in seasonal ice, which includes young ice (YI). This type of ice has a major impact on the weather and climate systems. However, only a limited number of studies have been dedicated to explore its spatial coverage and duration. This is mainly due to the lack of remote sensing tools that can identify it. This study uses an ice type and concentration retrieval algorithm, namely, Environment Canada's Ice Concentration Extractor, to study YI distribution and duration in the Arctic during seven ice formation seasons: 2002-03 to 2008-09. Results on the YI area, peak period, duration, and its interannual variability are presented in six regions covering the Arctic Basin. Duration is presented in terms of two parameters that describe the peak period and the number of days when YI concentration exceeds 50%. Probability distribution of the latter parameter shows that YI survives very few days before it grows into first-year ice. The summer of the minimum ice record in 2007 did not leave a remarkable impact on the subsequent YI area or its duration, although a delay in ice formation is observed. YI in the North Water polynya is also studied and shows no particular trend, although it varies between years. Anomalies are explained in terms of modeled surface temperature and wind.","10.1109/TGRS.2012.2225432","74277","0.2631579"
"192","cold_topic_cluster_no_5","rank_4","592","Antarctic sea ice change based on a new sea ice dataset from 1992 to 2008","The sea ice concentration dataset (covering the period 1992-2008) used in this study is a new dataset based on the Sea Ice Climate Change Initiative (SICCI) algorithm. We investigate whether the SICCI dataset is on a par with other datasets for studying sea ice cover changes in the Southern Ocean. We then examine spatiotemporal variations in sea ice derived from the SICCI dataset over the Southern Ocean, and analyse relationships of sea ice with sea surface temperature (SST). The results indicate that there is no significant difference between the SICCI dataset and the NASA Team dataset, and therefore the former can also be used for studying sea ice changes. Both sea ice extent (SIE) and sea ice area (SIA) derived from the SICCI dataset over the Southern Ocean increased slightly from 1992 to 2008, at rates of (17.75 +/- 11.50) x 10(3) and (17.37 +/- 9.51) x 10(3) km(2) yr(-1), respectively. Antarctic sea ice has significant seasonal variations; all seasonally averaged SIE and SIA show an increase, with spring showing the largest positive changing rate. The Weddell Sea, Ross Sea, and Indian Ocean have positive yearly changing rates in SIE and SIA, while the Bellingshausen/Amundsen seas and western Pacific Ocean have negative yearly changing rates. However, overall sea ice over the Southern Ocean has a slight positive trend, which is the same as the sea ice change pattern derived from the NASA Team dataset. This indicates that the contributions to the change in sea ice over the whole Southern Ocean due to the Weddell Sea, Ross Sea, and Indian Ocean dominate over those by the Bellingshausen/Amundsen seas and western Pacific Ocean. Further analysis shows that both SIE and SIA are negatively correlated with SST in the Southern Ocean or each of the 5 longitudinal sectors, and sea ice is more sensitive to SST in spring and autumn.","10.3354/cr01436","55449","0.2732283"
"193","cold_topic_cluster_no_5","rank_4","592","The impact of snow depth, snow density and ice density on sea ice thickness retrieval from satellite radar altimetry: results from the ESA-CCI Sea Ice ECV Project Round Robin Exercise","We assess different methods and input parameters, namely snow depth, snow density and ice density, used in freeboard-to-thickness conversion of Arctic sea ice. This conversion is an important part of sea ice thickness retrieval from spaceborne altimetry. A data base is created comprising sea ice freeboard derived from satellite radar altimetry between 1993 and 2012 and co-locate observations of total (sea ice + snow) and sea ice freeboard from the Operation Ice Bridge (OIB) and CryoSat Validation Experiment (CryoVEx) airborne campaigns, of sea ice draft from moored and submarine upward looking sonar (ULS), and of snow depth from OIB campaigns, Advanced Microwave Scanning Radiometer (AMSR-E) and the Warren climatology (Warren et al., 1999). We compare the different data sets in spatiotemporal scales where satellite radar altimetry yields meaningful results. An inter-comparison of the snow depth data sets emphasizes the limited usefulness of Warren climatology snow depth for freeboard-to-thickness conversion under current Arctic Ocean conditions reported in other studies. We test different freeboard-to-thickness and freeboard-to-draft conversion approaches. The mean observed ULS sea ice draft agrees with the mean sea ice draft derived from radar altimetry within the uncertainty bounds of the data sets involved. However, none of the approaches are able to reproduce the seasonal cycle in sea ice draft observed by moored ULS. A sensitivity analysis of the freeboard-to-thickness conversion suggests that sea ice density is as important as snow depth.","10.5194/tc-9-37-2015","67591","0.3256410"
"194","cold_topic_cluster_no_5","rank_4","592","The spatiotemporal patterns of sea ice in the Bohai Sea during the winter seasons of 2000-2016","In this study, sea ice thickness (SIT) and sea ice extent (SIE) in the Bohai Sea from 2000 to 2016 were investigated. A surface heat balance equation was applied to calculate SIT using ice surface temperatures estimated from the Moderate Resolution Imaging Spectroradiometer (MODIS) data with input from air temperature and wind speed from reanalyzing weather data. No trend was found in SIT during 2000-2016. The mean SIT and SIE during this period were 5.58 +/- 0.86 cm and 23x10(3)+/- 8x10(3) km(2), respectively. The largest SIT and SIE periods were observed during the second half of January and the first half of February, respectively. The Spearman correlation coefficient between mean ice thickness and average air temperature from 21 automatic weather stations around the Bohai Sea was -0.94 (P < .005), and the coefficient between median ice extent and negative accumulated temperature was -0.503 (P < .001). The rate of increase in air temperature around the Bohai Sea is 0.271 degrees C per decade in winter for 1979-2016 (P < .05), which is much lower than that in northern polar area (0.648 degrees C per decade). This rate has not resulted in a decreasing trend in SIT and SIE for the past 16 years in the Bohai Sea.","10.1080/17538947.2017.1365957","23516","0.3345679"
"195","cold_topic_cluster_no_5","rank_4","592","Simultaneous estimation of wintertime sea ice thickness and snow depth from space-borne freeboard measurements","A method of simultaneously estimating snow depth and sea ice thickness using satellite-based freeboard measurements over the Arctic Ocean during winter was proposed. The ratio of snow depth to ice thickness (referred to as alpha) was defined and used in constraining the conversion from the freeboard to ice thickness in satellite altimetry without prior knowledge of snow depth. Then alpha was empirically determined using the ratio of temperature difference of the snow layer to the difference of the ice layer to allow the determination of alpha from satellite-derived snow surface temperature and snow-ice interface temperature. The proposed method was evaluated against NASA's Operation IceBridge measurements, and results indicated that the algorithm adequately retrieves snow depth and ice thickness simultaneously; retrieved ice thickness was found to be better than the methods relying on the use of snow depth climatology as input in terms of mean bias. The application of the proposed method to CryoSat-2 radar freeboard measurements yields similar results. In conclusion, the developed alpha-based method has the capacity to derive ice thickness and snow depth without relying on the snow depth information as input for the buoyancy equation or the radar penetration correction for converting freeboard to ice thickness.","10.5194/tc-14-3761-2020"," 1986","0.3564706"
"196","cold_topic_cluster_no_5","rank_5","664","Effects of past and future land conversions on forest connectivity in the Argentine Chaco","Land-use change is the main driver of habitat loss and fragmentation worldwide. The rate of dry forest loss in the South American Chaco is among the highest in the world, mainly due to the expansion of soybean production and cattle ranching. Argentina recently implemented a national zoning plan (i.e., the Forest Law) to reduce further forest loss. However, it is unclear how the effects of past deforestation and the implementation of the Forest Law will affect forest connectivity in the Chaco. Our main goal was to evaluate the potential effect of the Forest Law on forest fragmentation and connectivity in the Argentine Chaco. We studied changes in the extent, fragmentation, and connectivity of forests between 1977 and 2010, by combining agricultural expansion and forest cover maps, and for the future in a scenario analysis. Past agricultural expansion translated into an overall loss of 22.5 % of the Argentine Chaco's forests, with deforestation rates in 2000-2010 up to three times higher than in the 1980s. Forest fragmentation and connectivity loss were highest in 1977-1992, when road construction fragmented large forest patches. Our future scenario analysis showed that if the Forest Law will be implemented as planned, forest area and connectivity in the region will decline drastically. Land-use planning designed to protect stepping stones could substantially mitigate connectivity loss due to deforestation, with the co-benefit of preserving the greatest amount of biodiversity priority areas across all evaluated scenarios. Including scenario analyses that assess forest fragmentation and connectivity at the ecoregion scale is thus important in upcoming revisions of the Argentine Forest Law, and, more generally, in debates about sustainable resource use.","10.1007/s10980-014-0147-3","65758","0.2466165"
"197","cold_topic_cluster_no_5","rank_5","664","Relationships between burned area, forest cover loss, and land cover change in the Brazilian Amazon based on satellite data","Fires are used as a tool in the deforestation process. Yet, the relationship between fire and deforestation may vary temporally and spatially depending on the type of deforestation and climatic conditions. This study evaluates spatiotemporal dynamics of deforestation and fire represented by burned area over the 2002-2012 period in the Brazilian Legal Amazon. As a first step, we compared newly available Landsat-based maps of gross forest cover loss from the Global Forest Change (GFC) project with maps of deforestation extent from the Amazon Deforestation Monitoring Project (PRODES) produced by the Brazilian National Institute for Space Research (INPE). As a second step, we rescaled the Landsat-based data to the 500m resolution of the Moderate Resolution Imaging Spectroradiometer (MODIS) burned area data (MCD64A1) and stratified this using MODIS land cover data to study the role of burned area in forest cover loss and deforestation. We found that while GFC forest cover loss and PRODES deforestation generally agreed on spatial and temporal dynamics, there were several key differences between the data sets. Both showed a decrease in the extent of forest cover loss or deforestation after 2004, but the drop was larger and more continuous in PRODES than in GFC. The observed decrease in forest cover loss or deforestation rates over our study period was mainly due to lower clearing rates in the evergreen broadleaf forests in the states of Mato Grosso, Para, and Rondonia. GFC indicated anomalously high forest cover loss in the years 2007 and 2010, which was not reported by PRODES. The burned area data indicated that this was predominantly related to increased burned area occurring outside of the tropical forest area during these dry years, mainly in Para. This indicated that fire and forest loss dynamics in woodlands or secondary forests may be equally important as deforestation in regulating atmospheric CO2 concentrations. In addition to the decrease in forest cover loss rates, we also found that post-deforestation fire use declined; burned area within 5 years after forest cover loss decreased from 54 to 39% during our study period.","10.5194/bg-12-6033-2015","67297","0.2648485"
"198","cold_topic_cluster_no_5","rank_5","664","Remote sensing and GIS based forest cover change detection study in Kalrayan hills, Tamil Nadu","The present study focuses on the role of remote sensing and geographic information system (GIS) in assessment of changes in forest cover, between 1931 and 2001, in the Kalrayan hills, Tamil Nadu. The trend of forest cover changes over the time span of 70 years, was precisely analysed using high resolution Satellite data. The study revealed that the forest cover was 275.6, 481.7 and 266.5 sq. km in 1931, 1971 and 2001 respectively It was noticed that forest cover has increased between 1931 and 1971, because of the implementation of various afforestation schemes by the forest department and scared grooves. It also revealed that the forest cover loss between 1971 and 2001 could be due to Shifting cultivation and illegal encroachments by villagers; and the forest cover drastically decreased on plateau areas due to human population pressure. The study analyses the forest cover change in the tropical deciduous forest region of the Eastern Ghats of India. It is envisaged that the study would prove the usefulness of Remote Sensing and GIS in forest restoration planning.","","81906","0.2716418"
"199","cold_topic_cluster_no_5","rank_5","664","Assessment of spatial changes in forest cover and deforestation rate in Eastern Ghats Highlands of Odisha, India","Aim : The spatial changes in forest cover and deforestation rate over eight decades in Koraput district of Odisha, a mountainous part of Eastern Ghats Highland region was studied using remote sensing and GIS. The dynamics of forest fragmentation was also quantified using patch analysis. Methodology : The multi-source and multi-date mapping was carried out using Survey of India topographical maps (1930's), Landsat MSS (1973), Landsat TM (1990), IRS P6 LISS III (2004 and 2013) satellite images. Radiometric and contrast correction was done to the images using digital image processing software. On-screen visual interpretation of forest cover was done which was aided by unsupervised classification. Ground truthing was done to determine the classification accuracy. Patch analysis was done to quantify forest fragmentation. Results : The mapping accuracy varied between 71.8% and 93.3 % for different years under study. The results for 1932, 1973, 1990, 2004 and 2013 indicate that the forest cover for the mentioned years were 4413.4 km(2), 3706.0 km(2), 3051.1 km, 2554.4 km(2) and 2284.5 km(2), which were 52.7%, 44.2%, 36.4%, 27.3% and 25.8% of the geographical area of the district, respectively. The deforestation rate was 0.38% per year during 1932-1973, 2.04% per year during 1973-1990, 1.71% per year during 1990-2004 and 0.63% per year during 2004-2013. The decline in overall rate of deforestation in recent years indicates increased emphasis on forest conservation. The number of fragmented forest patches was 398 in 1932, 645 in 1973, 688 in 1990, 697 in 2004 and 702 in 2013, which indicates ongoing anthropogenic pressure on the forests. The mean forest patch size decreased from 111 km(2) in 1932 to 65.8 km(2) in 2013. Interpretation : This study indicates the effectiveness of remote sensing in indentifying the forest and non-forest area. The change analysis of deforestation provides a decisive component for conservation and helpful in long term rational management of the remaining forests of the district for eco-restoration and sustainable development.","10.22438/jeb/39/2/MRN-429","39424","0.2945946"
"200","cold_topic_cluster_no_5","rank_5","664","The Human Impact on Changes in the Forest Range of the Silesian Beskids (Western Carpathians)","Changes in forest range are caused by human activity in many regions of the world. The aim of this paper is an attempt to determine the impact of pastoral and forest management on changes in forest cover and their fragmentation in the Silesian Beskids (southern Poland) in 1848-2015. Historical maps and landscape metrics were used to study changes in forest cover. Using a digital map of forests, analyses of the distribution of forest communities, site types and their condition were conducted. Since 1848 the forest area has increased by 11.8%, while the area of forest core zones has increased by 16.2%, accompanied by a 4.5% reduction in the forest's internal buffer zone. From the mid-nineteenth century, the forest range has been systematically growing from 82.1 to 93.9% because of the pastureland abandonment and forest regeneration, despite temporary logging resulting in forest fragmentation. Minor changes in core area index (CAI) from 80.41 to 87.55 indicate that pastoral economy did not result in considerable fragmentation of forests. The impact of forest management was greater as the sites characterised by natural condition occupy only 28% of the forest land and anthropogenically transformed ones dominate occupying over 50%. An artificial spruce monoculture was died-off and large felling areas were created at the beginning of the twenty-first century covering almost 40% of the study area.","10.3390/resources9120141","  930","0.3073684"
"201","cold_topic_cluster_no_6","rank_1","766","Sustainability of a Firm's Reputation for Information Technology Capability: The Role of Senior IT Executives","This study investigates the development and sustainability of a firm's information technology (IT) capability reputation from an IT executive's standpoint. Building on institutional theory, we argue that IT executives will try to achieve external legitimacy (i.e., project an image of superior IT capability to external stakeholders) in the hope that the top management team and board members will reciprocate by elevating the internal legitimacy of IT executives. Firms that develop such a culture of reciprocity with their IT executives are more likely to sustain their IT capability reputation. Econometric results based on panel data for 1,326 large U. S. firms from a wide spectrum of industries over a 13-year period (1997-2009) validate these predictions. More specifically, we find that IT executives with greater structural power (e. g., higher job titles) or IT-related expert power (e. g., IT-related education or experience) are more likely to attract public recognition for their firm's IT capability. Firms that build such an IT capability reputation are more likely to promote their IT executives, and IT executives who are promoted are more likely to stay longer with their firms. This continuity in IT strategic leadership is positively associated with the firm's ability to sustain its IT capability reputation. Our findings have important practical implications related to a firm's IT reputation strategy as well as the motivation and career of IT executives. Firms wanting to develop and sustain their IT capability reputation would do well to foster the creation of a cycle of positive reciprocity with their IT executives. IT executives hoping to increase their power within their firm's top management team and improve the legitimacy of the firm's IT organization need to project an image of IT superiority to external stakeholders.","10.2753/MIS0742-1222300102","74254","0.2592593"
"202","cold_topic_cluster_no_6","rank_1","766","Information technology literacy: Implications on teaching and learning","This paper aims to discuss the role and impact that information technology (IT) has on the future and existing style of learning and teaching. It highlights the importance of acquiring computer skills and being literate in IT. The focus is put on certain areas related to IT and education which include pedagogy and training to build IT literacy among both educators and learners. Particularly, it covers the current trends in IT development and how it has started to change and will further influence the way learning and teaching will take place in the future. This paper also discusses various theoretical frameworks and methodologies designed to cope with progress in IT. In summary, this paper delivers a message that IT literacy is the key to today's empowerment and that education is the best foundation for it.","","87150","0.2611111"
"203","cold_topic_cluster_no_6","rank_1","766","BIMODAL IT: BEYOND THE HYPE WITH THE CONSTRUCTAL LAW?","Several models of Information Technology (IT) and digital products delivery organization have waxed and waned during the last five years Two Speed IT, Bimodal IT, Multi-speed IT, Right-Speed IT, etc. In this article, we review and briefly compare some of the main models proposed, mainly by large IT research or consultmg firms. In a second part, and since IT and Information Systems (IS) can be viewed as complex flow systems, we will show how the constructal theory can help to discriminate between these, often competing, IT organizational models. More particularly, we will show how the constructal theory of the origin of S-curve fits with Wardley's PST triple-modes model, which is rooted in the analysis of a generic IT product lifecycle logistic curve. We thus illustrate here a first approach to integrate the principles of the constructal theory into the domain of corporate and IT organization and processes.","","41529","0.2681818"
"204","cold_topic_cluster_no_6","rank_1","766","Conceptualising an IT mindset and its relationship to IT knowledge and intention to explore IT in the workplace","Purpose An ""IT mindset"" significantly influences public sector information technology (IT) adoption in least developed countries (LDCs). The purpose of this paper is to explore the IT mindset concept and its relationship with IT knowledge and intention to explore IT in the workplace. Design/methodology/approach The research used a mixed-methods approach in two phases. Qualitative work was conducted to formulate the conceptual framework and hypotheses, followed by a survey of 228 public sector officials in Bangladesh to test the hypotheses. Findings The study showed that an IT mindset can be conceptualised as comprising personal innovativeness with IT and IT beliefs. The IT mindset was significantly related to intention to explore IT use in the workplace and its components were influenced by an individual's IT skills and IT awareness. Research limitations/implications - Future research could further explore the IT mindset concept and its antecedents and consequences in LDCs, where it is often related to successful IT adoption, and also in public and private organisations elsewhere. Practical implications - The study furthers understanding of barriers to IT adoption in LDCs' public sectors. Building IT knowledge through IT skills and awareness is required to orient mindsets to IT adoption. Social implications Improved efficiency, productivity and transparency in the public sector through IT use have flow-on societal and economic benefits. The paper provides insights into greater facilitation of e-government and IT in the public sector. Originality/value The study is theoretically significant because the IT mindset concept has lacked in-depth study and requires clarification of its nature and role.","10.1108/ITP-04-2017-0115","19077","0.2703704"
"205","cold_topic_cluster_no_6","rank_1","766","Making a technopolis in Hyderabad, India: the role of government IT policy","This paper studies the unusual growth of the information technology (IT) industry in the Indian city of Hyderabad during the last 15 years. It examines the high-technology industry in Hyderabad using registration data from the Software Technology Park (STP) of India and the state's IT policies meant to enhance the growth of regional industry. The paper outlines factors that are required for sustained growth of IT regions and evaluates the Hyderabad IT industry and the state government's IT policy against them. ","10.1016/j.techfore.2004.01.009","88993","0.2756098"
"206","cold_topic_cluster_no_6","rank_2","877","Users of Internet Health Information: Differences by Health Status","Background: Millions of consumers have accessed health information online. However, little is known about their health status. Objective: To explore use of Internet health information among those who were sicker (fair/poor general health status) compared with those reported being healthier. Methods: A national, random-digit telephone survey by the Pew Internet & American Life Project identified 521 Internet users who go online for health care information. Our primary independent variable was general health status rated as excellent, good, fair, or poor. Patterns of Internet use, and types of information searched were assessed. Results: Among the 521 users, 64% were female, most (87%) were white, and median age was 42 years. Most individuals indicated that they learned something new online (81%) and indicated that they believe most information on the Internet (52%). Compared with those with excellent/good health, those with fair/poor health (N = 59) were relative newcomers to the Internet but tended to use the Internet more frequently, were more likely to use online chats, were less likely to search for someone other than themselves, and were more likely to talk about the new information with their physician (odds ratio 3.3 [95% confidence interval 1.8-6.3]), after adjustment for age, education and income. Conclusions: Health care professionals should be aware that their sicker patients are more likely to ask them about information they found online. Physicians, public health professionals, and eHealth developers should work together to educate patients about searching for health information online and to provide tools for them to navigate to the highest quality information.","10.2196/jmir.4.2.e7","90468","0.2584158"
"207","cold_topic_cluster_no_6","rank_2","877","Use of computers and the Internet for health information by patients with epilepsy","The purpose of this study was to describe computer and Internet use among an online group and a clinic-based group of people with epilepsy. Greater than 95% of the online group and 60% of the clinic group have access to computers and the Internet. More than 99% of the online group and 57% of the clinic group used the Internet to find health information. A majority of people reported being likely to employ an Internet-based self-management program to control their epilepsy. About 43% reported searching for general information on epilepsy, 30% for medication, 23% for specific types of epilepsy, and 20% for treatment. This study found that people with epilepsy have access to computers and the Internet, desire epilepsy-specific information, and are receptive to online health information on how to manage their epilepsy. ","10.1016/j.yebeh.2007.07.013","85977","0.3000000"
"208","cold_topic_cluster_no_6","rank_2","877","Internet access and Internet use for health information among people living with HIV-AIDS","Widespread access to the Internet has the potential to improve the health care and quality of life of people with chronic illnesses, including people living with HIV-AIDS. However, the Internet is not equally accessible to all persons. We surveyed 96 men and 51 women living with HIV-AIDS regarding their experiences using the Internet. Results showed that persons with 12 or fewer years of education were significantly less likely to have used the Internet and were less likely to have been instructed in Internet use. A broad range of health-related Internet activities was reported including searching for health, AIDS-specific information, and using the Internet to communicate with providers. Among current Internet users, individuals who had an Internet connection in their home reported significantly more experiences using the Internet, including Internet use for interpersonal communication and search functions. A digital divide therefore exists among people living with HIV-AIDS, and the benefits of the Internet appear better achieved with home access. ","10.1016/S0738-3991(01)00134-3","90422","0.3000000"
"209","cold_topic_cluster_no_6","rank_2","877","A review of Web searching studies and a framework for future research","Research on Web searching is at an incipient stage. This aspect provides a unique opportunity to review the current state of research in the field, identify common trends, develop a methodological framework, and define terminology for future Web searching studies, In this article, the results from published studies of Web searching are reviewed to present the current state of research. The analysis of the limited Web searching studies available indicates that research methods and terminology are already diverging. A framework is proposed for future studies that will facilitate comparison of results, The advantages of such a framework are presented, and the implications for the design of Web information retrieval systems studies are discussed. Additionally, the searching characteristics of Web users are compared and contrasted with users of traditional information retrieval and online public access systems to discover if there is a need for more studies that focus predominantly or exclusively on Web searching, The comparison indicates that Web searching differs from searching in other environments.","10.1002/1097-4571(2000)9999:9999<::AID-ASI1607>3.3.CO;2-6","90798","0.3174603"
"210","cold_topic_cluster_no_6","rank_2","877","Competent information search in the World Wide Web: Development and evaluation of a web training for pupils","This paper describes the development and empirical evaluation of a web training for pupils (CIS-WEB, Competent Information Search in the World Wide WEB) which aims to convey prerequisite knowledge and skills that are necessary for a competent search for information on the web. The web training focuses on competent information handling and is based on two theoretical analyses. First, a conceptual analysis of information search from the perspective of media literacy research and information retrieval research was conducted and yielded a set of five pivotal content aspects that need to be covered by a web training. Each of these content aspects is characterized by declarative and procedural knowledge components which are necessary for the pursuit of,I competent search for information on the web. Second, we conducted a task analysis which conceptualizes the search for information on the web as,I problem-solving process and which allows to systematically distinguish between different types of information problems. In the empirical part of the paper two classroom Studies are reported. III Study 1, the widespread training concept of a technically oriented Internet training for pupils was evaluated and it was shown that no substantial improvement of web searching skills can be expected from this type of treatment. III Study 2, it was shown that the web training CIS-WEB improves pupils' declarative knowledge of the web as well as their search performance, thereby outperforming the conventional Internet training used in Study 1. ","10.1016/j.chb.2007.01.029","85584","0.3500000"
"211","cold_topic_cluster_no_6","rank_3","21","Is There a Rise in the Importance of Socioemotional Skills in the Labor Market? Evidence From a Trend Study Among College Graduates","In this study, we examine whether socioemotional skills have become more important in the labor market within the past 14 years. To this end, we analyze data from a unique dataset on recent graduates from Dutch professional colleges (N= 67,000). Two different indicators of skill change are investigated, namely changes in the skill level required in the labor market and changes in the wage returns to these skills. The results indicate that socioemotional skills related to knowledge and innovation such as logical reasoning and information gathering, as well as skills related to working to plan and collaboration, have undergone a significant increase in terms of labor market requirements. We also observe an increase in the required level of the work-related skills digital literacy and occupation-specific knowledge. However, significant increases in wage returns are only observed for socioemotional skills related to knowledge and innovation. The labor market importance of socioemotional skills appears to be only modestly affected by business cycle effects.","10.3389/fpsyg.2020.01710"," 7974","0.2424242"
"212","cold_topic_cluster_no_6","rank_3","21","The growth and valuation of computing and other generic skills","This paper describes a method for measuring job skills using survey data on detailed work activities, and using these measures examines whether the utilisation of skills is growing, and how they are valued in the labour market. We show that between 1997 and 2001 there was a growth in Britain in the utilisation of computing skills, literacy, numeracy, technical know-how, high-level communication skills, planning skills, client communication skills, horizontal communication skills, problem-solving, and checking skills. Computer skills utilisation was growing the fastest, and the use of computers was becoming more sophisticated. We re-evaluate the issue of whether computers have affected wages, taking into account existing critiques in the literature. We find that both computer skills and high-level communication skills carry positive wage premia, as shown both in cross-section hedonic wage equations that control for many detailed activities, and through a within-cohorts change analysis.","10.1093/oep/gpf049","89135","0.2567164"
"213","cold_topic_cluster_no_6","rank_3","21","Emerging Technologies in Households: Implications for Homemakers","The computer system and other ICT innovations are fast becoming very important tools in everyday life including family living. This paper focuses on the various areas of ICT/Computer application in the home and the associated challenges for the homemakers in the use of computer in the home. It discusses the need for homemakers to acquire computer literacy in order to keep pace with the computer revolution. It also proffers some recommendations, which would help acquire computer literacy.","","80821","0.2821429"
"214","cold_topic_cluster_no_6","rank_3","21","Digital Learning in Rural Ontario, Canada: An Evaluation of the Computer for Seniors Program","This article presents an evaluation of an educational intervention that provided computer skills training for a sample of n = 17 rural seniors in North Middlesex, Ontario, Canada. Due to the rural nature of the community, this cohort had limited access to and knowledge of computers. A total of N= 36 originally participated in a designed basic computer training intervention consisting of 8-week training sessions, offered sequentially in three smaller cohorts of 12 seniors. The evaluation included a pre and post-test using the Seniors Basic Computer Skills Scale (SBCSS) to assess the effectiveness of this training. The evaluation team developed the SBCSS to measure basic computer skills of seniors and pilot-tested it with the sample. Psychometric testing of the SBCSS showed exceptionally high degrees of reliability. The SBCSS measured 11 basic computer skills including: (a) talking about computers, (b) using computer technology, (c) using the Internet, (d) using an Internet resource such as Skype, (e) using a computer mouse, (f) using Web browsers, (g) manipulating the computer screen brightness and size, (h) using bookmarks, (i) sending and receiving emails with family and friends, (j) sharing photographs with family and friends, and (k) using social media such as Facebook. There were some encouraging results regarding increased computer skills for this sample of rural seniors. However, there were other areas where learning did not increase significantly. This article speaks to the need to identify and recruit rural seniors who may benefit from such targeted interventions to increase technological skills, and also to the need for a second-level enhancement of the municipally funded Computer for Seniors program (CSP). We contend that community workers have a unique role to play in such initiatives, as they may encourage local seniors to both explore their computer literacy in rural areas, and help to increase opportunities that are often missing in rural communities.","","16280","0.3340136"
"215","cold_topic_cluster_no_6","rank_3","21","Assessing Individual Differences in Basic Computer Skills Psychometric Characteristics of an Interactive Performance Measure","A definition of basic computer skills (BCS) is proposed and the psychometric properties of a newly developed BCS scale are investigated. BCS is defined as the ability and speed of performing basic actions in graphical user interfaces of computers to access, collect, and provide information. BCS is thus considered a basic component skill of the much broader construct of ICT literacy. Data from the German PISA 2009 field trial was used to determine the factor structure of the BCS scale as well as convergent and discriminant validity. The latent factor structure underlying the BCS scale was investigated by testing confirmatory factor analysis (CFA) models for response times and responses. CFA results suggest that there is one dimension of BCS speed and BCS ability, respectively. With respect to convergent validity, practical computer knowledge and skill in digital reading had strong associations with BCS speed and ability. With respect to discriminant validity, only moderate associations were found with lower level reading skills and self-reported computer skills. Differences between BCS speed and ability and further developments of the BCS scale are discussed.","10.1027/1015-5759/a000153","75520","0.3973913"
"216","cold_topic_cluster_no_6","rank_4","143","The impact of new information services on teaching, learning and research at the University of Zululand Library","The rapidly-changing academic environment demands innovative library products, facilities and services. Libraries have to navigate shrinking budgets, currency fluctuations, and the high cost of print and e-resources. Other challenges include the growing need for research support services, improved information literacy, information client support services, and marketing of library services using social media. At the University of Zululand (UNIZULU) Library, information librarians have played a pivotal role in the training of the academic community on modern information services for teaching, learning and research support. The purpose of this paper is to report on the perceived impact of the new information services initiated by information librarians and to establish how these services have made a difference in the teaching, learning and research activities at the university. A case study of UNIZULU Library was conducted using survey data collected from a sample population of academic staff and a random selection of evaluation forms that were completed by the students. The results of the survey showed a positive response from UNIZULU Library users to the newly-developed information services system. The paper concludes by noting the challenges to the services and suggesting areas for improvement.","10.7553/82-2-1620","61190","0.2954545"
"217","cold_topic_cluster_no_6","rank_4","143","Adoption of information and communication technology (ICT) in academic libraries - A strategy for library networking in Nigeria","Purpose - This study aims to investigate the extent of adoption of information and communication technology (ICT) in university libraries in Nigeria. Design/methodology/approach - Postal survey was the instrument used for data collection. Almost 60 per cent of the University Librarians out of the 29 university libraries surveyed completed their questionnaires. Findings - The results of the survey show that only six university libraries are fully ""computerized"", nine are ""about to be computerized""; seven of the surveyed libraries have installed local area networks, five have online public access catalogue and only four libraries provide internet service. The major obstacles that influence effective adoption of ICT in university libraries are inadequate funds and the poor state of electricity in Nigeria. Practical implications - The federal government should increase the present level of funding of Nigerian university to improve the library development fund, which is the major source of funding available to university libraries. The poor state of electricity in Nigeria should also be improved by the federal government for sustainable adoption of ICT by university libraries. Originality/value - The paper proposes that computer networking of university libraries is feasible and recommends the development of the Nigerian university libraries network and academic libraries network.","10.1108/02640470510635782","88772","0.3108696"
"218","cold_topic_cluster_no_6","rank_4","143","The Law School Library or the Library at the Law School? How Lessons from Other Types of Libraries Can Inform the Evolution of the Academic Law Library in the Digital Age","Academic law libraries must adapt to the new digital environment of reduced funds, smaller physical collections, and greater use of electronic resources. This article examines the historical development of academic law libraries and the current challenges they face, different approaches libraries take to change, and lessons about how to thrive from nonlaw libraries.","","43304","0.3160000"
"219","cold_topic_cluster_no_6","rank_4","143","Rights instruction for undergraduate students: Needs, trends, and resources","As librarians are increasingly identified as rights experts on campus, it is essential that they understand rights issues and how best to communicate them to a multitude of users. While scholarly literature contains many examples of librarians working with faculty on copyright issues regarding course materials, reserves, e-reserves, and intellectual property, there is far less documentation of the need for students, especially undergraduates, to understand their rights as creators and consumers of information. This article reviews relevant literature on librarian ","10.1080/10691316.2016.1275910","41704","0.3333333"
"220","cold_topic_cluster_no_6","rank_4","143","Web Content and Digitization Patterns of Tribal College Libraries Within the Great Plains Region","The Web content and digitization patterns of twenty-three tribal college libraries from the Great Plains region were examined to determine what types of services, programs, digital resources, and social media were promoted on their websites. The results of this study differed from a previous 1998 study by Kaya, which found that tribal libraries merely support the institutions and are 'not recognized as an integral part of the activities of a tribal college, that is, as an equal partner in the academic endeavor' (p.245). Kaya noted that the success of students enrolled in tribal colleges was not contingent upon libraries and the resources they provide. Findings of this study indicate that today, the library is an integral part of the tribal college and the surrounding communities they serve and they promote lifelong learning and literacy in the community.","","57419","0.3545455"
"221","cold_topic_cluster_no_6","rank_5","528","Are pictures worth a thousand words? The effect of information presentation type on citizen perceptions of government websites","With the increasing disclosure of public information and government data through information and communication technologies, along with the considerable privately generated data now available online, individuals have access to a huge volume of information. This ""disintermediation"" of (i.e., greater direct access to) public information may improve transparency and facilitate citizen engagement, but it may also overwhelm citizens not only with too much information but also by requiring them to take responsibility for gathering, assembling, and processing information. Despite the importance of effective information processing to successful use of available information, existing studies have not yet fully integrated this consideration into research on citizen use of egovernment and open government data. Based on information processing theory-according to which individuals have a finite information processing capacity, which is affected not only by the quantity and quality of information but also by one's preferences for how information is presented-this study examined the effects of information presentation type (infographic versus text) on perceived information overload, along with the consequent effect of information overload on perceived website usefulness. We also investigated whether individual information processing propensity (visual or verbal) moderated the effect of information presentation type on perceived information overload. Our results showed that textual information tended to cause greater information overload, especially for those with a propensity for visual information processing, and that higher information overload was associated with a lower perception of website usefulness. Moreover, individual information propensity moderated the effect of information type on perceived information overload; people with visual information processing propensity were more strongly affected by the presentation of textual information. We discuss the implications of our findings for improving the communication of policy information through government websites.","10.1016/j.giq.2020.101482"," 8455","0.1946970"
"222","cold_topic_cluster_no_6","rank_5","528","Eradicating information poverty: An agenda for research","Information poverty remains a critical issue for societies today. The literature of information poverty is reviewed tracking its origins in library and information science and the various approaches that have been taken to tackling information poverty, including international development programmes such as the Global Libraries Initiative, working response to the UN's Sustainable Development Goals, the importance of access to health information and so on. The paper sets out themes that emerged in a roundtable discussion of library and information science academics in 2017. Discussion centred on: definitions of information poverty which reflect the wide variety of ways in which it is possible to be information poor; literacy and information literacy; the ways in which information can reduce poverty and disadvantage; library and information science initiatives to tackle information poverty; and information poverty in the context of social justice. The group agreed that there was a major piece of work to be done in reframing the library and information science discipline in terms of information poverty. Four key dimensions of information poverty for collaborative future research are: (1) information as an agent to eradicate poverty; (2) the causal factors resulting in information poverty; (3) creation and production activities to combat information poverty; and (4) better understanding of areas of extreme disadvantage and aspects of information need. A list of the key causal factors in creating information poverty which came out of the discussion is presented. Further research initiatives are underway for setting up a partnership/consortium that would lay the foundations for a multidisciplinary network on information poverty, sharing expertise internationally.","10.1177/0961000618804589","10614","0.1991597"
"223","cold_topic_cluster_no_6","rank_5","528","On the crest of a wave: transforming the archival future","The profession of digital archivist is crystallising, fundamentally challenging traditional archival roles. The very nature of digital records also challenges the sustainability of archival systems and collections. Records that used to stay stable for decades in an analogue world now risk being lost or damaged within moments of creation. How should archivists react to these changes? Archivists have to lift ourselves out of our analogue environment and focus more effort on forging a new path, to reposition archives, archival institutions and archival practitioners more strategically for the future. To do this, archivists must resist the temptation to think that we and we alone - as people, as archivists or as today's archivists as opposed to yesterday's archivists - can come up with the ultimate solution to the world's recordkeeping problems. Archivists must keep innovating, absolutely. But we also need to be agile and flexible, remembering that anything we come up with today will be superseded at some point in the future - increasingly, in the very near future. Archivists need to forge links with archives, systems and people in order to come up with approaches to records and archives care that remain usable now and flexible well into the future.","10.1080/01576895.2017.1328696","49743","0.2062500"
"224","cold_topic_cluster_no_6","rank_5","528","""An intensity around information': the changing face of chemical information literacy","The changing nature of chemical information literacy over 50 years is examined by a comparison of a number of guides to chemical literature and information. It is concluded that: an understanding of the world of information is the sole aspect to have remained important and essentially unchanged over time; that knowledge of sources, ability to access information and ability to organize information have been of importance throughout, but have changed their nature dramatically; and that evaluation of information has gained in importance since the advent of the World Wide Web. The link between chemical structure and corresponding substance information is the most significant threshold concept. Information literacy in chemistry is strongly subject-specific.","10.1177/0165551515616919","48469","0.2083333"
"225","cold_topic_cluster_no_6","rank_5","528","Examining the factors influencing information poverty in western China","Purpose The purpose of this study is to explore the factors impacting information poverty in western China and investigate to what extent these determinants contribute to information poverty in these areas. Design/methodology/approach Structural equation modeling (SEM) technique was used to analyze 232 valid responses collected from a survey to examine the research model and hypotheses in this study. Findings The results indicate that information literacy, information supply and information and communication technologies (ICTs) positively and significantly affect information poverty, while social prejudice and information orientation had no significant effects on information poverty. Practical implications Public libraries, government departments and other institutions should pay attention to the significant impact of information literacy, information supply and ICTs on information poverty and formulate corresponding systems and policies to alleviate or reduce information poverty. Originality/values In the past, few studies have focused on information poverty in western China and most of the existing research on information poverty adopts qualitative research methods, such as interview, systematic literature review and so on, while quantitative research is rare. In addition, the focus of these research studies was on one or two aspects, and a few of them can systematically study the influencing factors of information poverty. Inspired by the theories of information literacy, social prejudice and information poverty, this study comprehensively used a questionnaire survey and SEM to investigate the influences of information literacy, social prejudice, information supply, information orientation and ICTs on information poverty.","10.1108/EL-04-2020-0095"," 1567","0.2358491"
"226","cold_topic_cluster_no_7","rank_1","553","Haar orthogonal functions based parameter identification of wastewater treatment process with distributed parameters","The most often used system in aerobic biological wastewater treatment is the system ""biological reservoir - sedimentor"". The necessary condition for obtaining a good operative control is an adequate process model to be available. The aim of this paper is modelling of a process, carrying out a system ""biological reservoir - sedimentor"", and consequent parameter identification. In order to obtain a model with higher degree of accuracy, the process of wastewater treatment is considered as object with distributed parameters. Shifted two - dimensional Haar orthogonal functions are used for parameter identification of the process. The implementation of these orthogonal functions reduces the problem to a computationally convenient form. The algorithm is efficient and simple in form.","","88324","0.2000000"
"227","cold_topic_cluster_no_7","rank_1","553","Energy-Economizing Optimization of Magnesium Alloy Hot Stamping Process","Reducing the mass of vehicles is an effective way to improve energy efficiency and mileage. Therefore, hot stamping is developed to manufacture lightweight materials used for vehicle production, such as magnesium and aluminum alloys. However, in comparison with traditional cold stamping, hot stamping is a high-energy-consumption process, because it requires heating sheet materials to a certain temperature before forming. Moreover, the process parameters of hot stamping considerably influence the product forming quality and energy consumption. In this work, the energy-economizing indices of hot stamping are established with multiobjective consideration of energy consumption and product forming quality to find a pathway by which to obtain optimal hot stamping process parameters. An energy consumption index is quantified by the developed models, and forming quality indices are calculated using a finite element model. Response surface models between the process parameters and energy-economizing indices are established by combining the Latin hypercube design and response surface methodology. The multiobjective problem is solved using a multiobjective genetic algorithm (NSGA-II) to obtain the Pareto frontier. ZK60 magnesium alloy hot stamping is applied as a case study to obtain an optimal combination of parameters, and compromise solutions are compared through stamping trials and numerical simulations. The obtained results may be used for guiding process optimization regarding energy saving and the method of manufacturing parameters selection.","10.3390/pr8020186","15080","0.2009901"
"228","cold_topic_cluster_no_7","rank_1","553","Inverse distributed modelling of streamflow and turbulent fluxes: A sensitivity and uncertainty analysis coupled with automatic optimization","The interactions of hydrological variables in the terrestrial hydrological cycle are complex. To better predict the variables, distributed and physically based models are used as they account for the complexity of interactions. In this study, we addressed the joint simulation of water- and energy fluxes and the potential benefit of flux measurements in the parameter estimation process. For this purpose, we applied the hydrological model GEOtop to a prealpine catchment in southern Germany (River Rott, 55 km(2)) over two recent summer episodes, as a test case. Due to its complexity, the model is computationally demanding and only a limited number of forward runs can be afforded in inverse modelling and parameter estimation. We applied the gradient-based nonlinear Gauss-Marquardt-Levenberg (GML) parameter estimation method and linked the GEOtop model to the Parameter ESTimation tool (PEST). Using this developed GEOtop-PEST interface, we particularly investigated the value added by including turbulent flux data in the parameter estimation process, and analyse the impact of the additional flux data on the uncertainty bounds of the parameters. To better understand the interplay of the model parameters and to identify the dominating parameters in the calibration process, we also conducted a Principal Component Analysis (PCA). We were able to identify a set of model parameters that reproduced both observed streamflow and turbulent heat fluxes reasonably well. The majority of the estimated parameters were highly sensitive to the considered variables. We showed that the confidence bounds of estimated parameters are narrowed significantly when considering not only streamflow observations but also turbulent flux measurements in the calibration process. In this manner, correlations between estimated parameters could also be reduced.","10.1016/j.jhydrol.2019.02.033","27824","0.2045872"
"229","cold_topic_cluster_no_7","rank_1","553","A Fuzzy Sequential Pattern Mining Algorithm Based on Independent Pruning Strategy for Parameters Optimization of Ball Mill Pulverizing System","This paper presents a fuzzy sequential pattern mining algorithm based on independent pruning strategy for parameters optimization of ball mill pulverizing system. Based on the Apriori-alike process, the proposed algorithm uses the independent pruning strategy to mine the fuzzy sequential patterns, which could enhance the efficiency of the algorithm. Then, the optimal values of the process variables are determined by a searching method with the mined sequential patterns. The improved fuzzy sequential pattern support and the fuzzy sequential pattern confidence are adopted to ensure the accuracy of the mined sequential patterns. Moreover, the sliding time window technique is used to ensure the completeness of mining results. The experimental results for parameters optimization of ball mill pulverizing system also verify that the proposed algorithm could determine the optimal values correctly and the running time is not long. In addition, the proposed algorithm has been put into practice successfully and the statistic data show that the pulverizing capability of ball mill pulverizing system is increased and the energy consumption would be reduced.","10.5755/j01.itc.43.3.5180","71841","0.2402597"
"230","cold_topic_cluster_no_7","rank_1","553","ENERGY-EFFICIENT FAST FOURIER TRANSFORMS FOR COGNITIVE RADIO SYSTEMS","AN ENERGY-EFFICIENT FAST FOURIER TRANSFORM (FFT) ALGORITHM FOR COGNITIVE RADIO COMMUNICATION SYSTEMS USES A HOMOGENEOUS MULTIPROCESSOR SYSTEM ON CHIP. THE ALGORITHM ALLOWS FOR PRUNING OF INPUTS SUCH THAT ALGORITHM COMPLEXITY CAN BE REDUCED WHENEVER SEVERAL OF THE FFT INPUTS ARE ZERO. RESULTS SHOW THAT THE PRUNING ALGORITHM SIGNIFICANTLY REDUCES ENERGY CONSUMPTION COMPARED TO A NONPRUNED VERSION.","10.1109/MM.2010.84","81621","0.2500000"
"231","cold_topic_cluster_no_7","rank_2","301","Feedback PID-like fuzzy controller for pH regulatory control near the equivalence point","In this research the use of a feedback PID-like fuzzy controller scheme for pH control is presented to deal with instability problems near the equivalence point in neutralization processes. State space analysis of the titration curves and a fuzzy clustering algorithm based on calculating a measure of potential derived from the square distance of the pH data are complementary applied to define the membership structure and the fuzzy sets of the controller. To test the performance of the controller, both simulated and experimental runs were used. The fuzzy controller was tested for compensating step-change perturbations of propionic acidic flow rates, propionic acid concentration, and buffering conditions. Stationary cycling behavior has been observed for large loads of acidic flow rates. It was found that though the rejection time was strongly dependent on the mean residence time of the liquid solutions, the proposed controller keep the neutralization process operating close to the specified set point of pH =7. ","10.1016/j.jprocont.2014.05.006","69973","0.2184211"
"232","cold_topic_cluster_no_7","rank_2","301","Controlling cold temperature partial nitritation in moving bed biofilm reactor","Mainstream partial nitritation was studied at 10 degrees C in a moving bed biofilm reactor treating synthetic wastewater containing both nitrogen ( approximate to 40 mg L-1) and organic carbon at COD/IN ratio ranging from 1.3 to 2.2. Three different control strategies were investigated to achieve partial nitritation. Initially, biofilm age was controlled by incorporating a media replacement strategy. Next, separately from the media replacement, oxygen limited conditions were investigated and finally pH control was incorporated together with oxygen limitation. Successful partial nitritation was achieved only by combining oxygen limitation with pH control. The average NH4-N concentration was equal to 16.0 +/- 1.6 mg L-1 and average NO2-N concentration was equal to 15.7 +/- 2.4 mg L-1 during steady state partial nitritation. The average residual NO3-N concentration was equal to 2.6 +/- 2.2 mg L-1. The results obtained from this study prove for the first time that partial nitritation can be successfully controlled in a biofilm reactor treating wastewater with low nitrogen concentration, relatively high COON ratio and at low temperature. An algorithm for dynamic process control of partial nitritation has been also developed. ","10.1016/j.chemosphere.2019.04.025","25092","0.2370370"
"233","cold_topic_cluster_no_7","rank_2","301","Application of adaptive heuristic criticism control (AHCC) to dye wastewater","This paper presents an experimental application of AHCC to study the coagulation process of wastewater treatment in a dye plant. Also this study includes a series of tests in which an AHCC control was used for pH control. The performance results of the AHCC controller are compared with the results obtained by using a conventional proportional-integral-derivative (PID) algorithm. It is useful to compare PID with AHCC to illustrate the extreme range of the nonlinearity of the dye wastewater treatment process. Although the removal of pollutants from wastewater is similar with AHCC and PID, our results show excellent AHCC performance in the region where conventional PID control fails. ","10.1016/j.jenvman.2006.06.018","86529","0.2457143"
"234","cold_topic_cluster_no_7","rank_2","301","Microbial electrolysis cell scale-up for combined wastewater treatment and hydrogen production","This study demonstrates microbial electrolysis cell (MEC) scale-up from a 50 mL to a 10 L cell. Initially, a 50 mL membraneless MEC with a gas diffusion cathode was operated on synthetic wastewater at different organic loads. It was concluded that process scale-up might be best accomplished using a ""reactor-in-series"" concept. Consequently, 855 mL and 10 L MECs were built and operated. By optimizing the hydraulic retention time (HRT) of the 855 mL MEC and individually controlling the applied voltages of three anodic compartments with a real-time optimization algorithm, a COD removal of 5.7 g L(R)(-1)d(-1) and a hydrogen production of 1.0-2.6 L L(R)(-1)d(-1) was achieved. Furthermore, a two MECs in series 10 L setup was constructed and operated on municipal wastewater. This test showed a COD removal rate of 0.5 g L(R)(-1)d(-1), a removal efficiency of 60-76%, and an energy consumption of 0.9 Wh per g of COD removed. Crown ","10.1016/j.biortech.2012.12.062","75253","0.2753846"
"235","cold_topic_cluster_no_7","rank_2","301","Investigation of the removal of cyanide from aqueous solutions using biomass Saccharomyces cerevisiae","The industrial revolution and the rise of factories and industrial towns caused the increasing production of wastewater that contains hazardous compounds in the aqueous ecosystems. The purpose of this study was to investigate the removal of cyanide from aqueous solutions using yeast of Saccharomyces cerevisiae biomass. In this experimental-interventional study, to measure the concentration of cyanide, the titrimetric method was used. After determining the concentration of cyanide in the samples exposed to the S. cerevisiae yeast, the removal rate was calculated. In the concentration of 5mg/l of cyanide and at 15, 30, 60, 90min of contact, yeast weight values of 0, 0.5, 1, 1.5g/l, at the pH of 5, 7, 9 were studied. The data were statistically analyzed using SPSS 16 software. The study showed that with increasing the contact time of the yeast, cyanide removal efficiency increased. Concentration of 0.5g/l of yeast at pH of 9 and contact time of 15min had the lowest percentage of removal of cyanide, while concentration of 1.5g/l of yeast at pH of 7 and contact time of 90min had the highest level of removal of cyanide. About 0g/l yeast concentration at all times showed a significant relationship (p<0.001). According to the findings, this yeast is a suitable adsorbent for the removal of cyanide ions from wastewater. The yeast is easily produced in the very cheap fermentation process medium. Therefore, by replacing the expensive and noneconomical treatment methods with yeast biomass, contaminated wastewater can be treated.","10.1080/19443994.2016.1172262","56086","0.2901099"
"236","cold_topic_cluster_no_7","rank_3","92","Optimal operation of reservoir systems with the symbiotic organisms search (SOS) algorithm","This work introduces the symbiotic organisms search (SOS) evolutionary algorithm to the optimization of reservoir operation. Unlike the genetic algorithm (GA) and the water cycle algorithm (WCA) the SOS does not require specification of algorithmic parameters. The solution effectiveness of the GA, SOS, and WCA was assessed with a single-reservoir and a multi-reservoir optimization problem. The SOS proved superior to the GA and the WCA in optimizing the objective functions of the two reservoir systems. In the single reservoir problem, with global optimum value of 1.213, the SOS, GA, and WCA determined 1.240, 1.535, and 1.262 as the optimal solutions, respectively. The superiority of SOS was also verified in a hypothetical four-reservoir optimization problem. In this case, the GA, WCA, and SOS in their best performance among 10 solution runs converged to 97.46%, 99.56%, and 99.86% of the global optimal solution. Besides its better performance in approximating optima, the SOS avoided premature convergence and produced lower standard deviation about optima.","10.2166/hydro.2017.085","45904","0.2244898"
"237","cold_topic_cluster_no_7","rank_3","92","Reservoir Operation by a New Evolutionary Algorithm: Kidney Algorithm","This article shows an application of a new algorithm, called kidney algorithm, for reservoir operation which employs three different operators, namely filtration, secretion, and excretion that lead to faster convergence and more accurate solutions. The kidney algorithm (KA) was used for generating the optimal operation of a reservoir namely; Aydoghmoush dam in eastern Azerbaijan province in Iran whose purpose was to decrease irrigation deficit downstream of the dam. Results from the algorithm were compared with those by other evolutionary algorithms, including bat (BA), genetic (GA), particle swarm (PSO), shark (SA), and weed algorithms (WA). The results showed that the kidney algorithm provided the best performance against the other evolutionary algorithms. For example, the computational time for the KA was 3s, 2s, 4s, 6s and 3s less than BA, SA, GA PSA and WA, respectively. Also, the objective function for the optimization problem was the minimization of the irrigation deficits and its value for the KA was 55%, 28%, 52%, 44 and 54% less than GA, SA, WA, BA and PSA, respectively. Also, the different performance indexes showed the superiority of the KA compared to the other algorithms. For example, the root mean square error for the KA was 74%, 61%, 68%, 33 and 54% less than GA, SA, WA, BA and PSA, respectively. Different multi criteria decision models were used to select the best models. The results showed that the KA achieved the first rank for the optimization problem and thus, it shows a high potential to be applied for different problems in the field of water resources management.","10.1007/s11269-018-2078-2","33698","0.2392857"
"238","cold_topic_cluster_no_7","rank_3","92","Product evolutionary design driven by environmental performance","This article is in terms of product environmental performance demand and proposes four structure evolutionary operation modes which include combined evolutionary method, decomposition evolutionary method, replacement evolutionary method, and material-changing evolutionary method to express the structure evolutionary process of products. Through the quotient space theory and proposed method combined with probability statistics, probability mapping from environmental performance to product structure is established and the evolutionary individuals with outstanding environmental performance are listed. Through the analysis to the specific conditions of the evolutionary individuals, the design constraints are extracted, and the objective function of environmental performance is established. This article presents an interactive genetic algorithm as evolutionary algorithm and combines it with four structure evolutionary operation modes to conduct corresponding gene manipulation and generates evolutionary product. Finally, the proposed methodology is successfully applied to engine gear chamber and the environmental impact is found to be better than before evolution.","10.1177/1063293X18805200","28631","0.2408451"
"239","cold_topic_cluster_no_7","rank_3","92","An efficient and robust artificial bee colony algorithm for numerical optimization","Artificial bee colony (ABC) algorithm has already shown more effective than other population-based algorithms. However, ABC is good at exploration but poor at exploitation, which results in an issue on convergence performance in some cases. To improve the convergence performance of ABC, an efficient and robust artificial bee colony (ERABC) algorithm is proposed. In ERABC, a combinatorial solution search equation is introduced to accelerate the search process. And in order to avoid being trapped in local minima, chaotic search technique is employed on scout bee phase. Meanwhile, to reach a kind of sustainable evolutionary ability, reverse selection based on roulette wheel is applied to keep the population diversity. In addition, to enhance the global convergence, chaotic initialization is used to produce initial population. Finally, experimental results tested on 23 benchmark functions show that ERABC has a very good performance when compared with two ABC-based algorithms. ","10.1016/j.cor.2012.12.006","74657","0.2562500"
"240","cold_topic_cluster_no_7","rank_3","92","Crow Algorithm for Irrigation Management: A Case Study","This study employed a new evolutionary algorithm namely, the crow algorithm (CA), to optimize reservoir operation and minimize irrigation water deficit. Comprehensive analysis have been carried out between the proposed CA algorithm and other algorithms such as Prticle Swarm optimization (PSO), Shark Algorithm (SA), Genetic Algorithm (GA), and Weed Algorithm (WA). In addition, in order to select the optimal optimization algorithm among all of the investigated ones, a Multi-Criteria Decision model has been utilized. The time of computation was 45 s for CA but was 65, 50, 78, and 99 s for SA, WA, PSO, and GA, respectively. The CA exhibited greater volumetric reliability and a lower vulnerability index over the other examined algorithms. Furthermore, the Root Mean Square Error (RMSE) between demand and water release was 1.11 x 10(6) m(3) for CA compared to 2.14 x 10(6) m(3), 3.33 x 10(6) m(3), 3.45 x 10(6) m(3), and 3.78 x 10(6) m(3) for SA, WA, PSO, and GA, respectively. Using a multi-criteria decision model based on different indices, including the vulnerability index, resiliency index and volumetric reliability index, CA was ranked first.","10.1007/s11269-020-02488-6","15150","0.3064935"
"241","cold_topic_cluster_no_7","rank_4","681","A noise-robust semi-supervised dimensionality reduction method for face recognition","Face recognition (FR) is a fundamental problem in a biometric identification system and has attracted much attention in pattern recognition and computer vision fields. Since human face images have high dimensionality, dimensionality reduction (DR) is often adapted for FR and a number of relevant methods are proposed, such as principal component analysis (PCA), linear discriminant analysis (LDA). However, face images are generally hard to be labeled and corrupted by noise in the collection phase. To address the problem, we present a novel semi-supervised DR method which is more robust to noise than the traditional DR methods. Our basic idea is that collaborative representation-based classification (CRC) can achieve better performance than the classifier trained by Euclidean distance when the face images are corrupted by noise. In our algorithm, we firstly employ CRC to compute a representation coefficient for each face image. We then construct a reconstruction error based regularization term through the obtained coefficient vector. Finally, we extend LDA to the semi-supervised framework by embedding the regularization term into LDA. To evaluate the effectiveness of our algorithm, we use four well-known face databases to conduct several experiments by comparing to unsupervised, supervised and semi-supervised DR methods. The results illustrate that our algorithm can always achieve the best performance as the noise corruption percent of noise images increases. ","10.1016/j.ijleo.2017.11.140","41056","0.1531915"
"242","cold_topic_cluster_no_7","rank_4","681","On weighted total least-squares adjustment for linear regression","The weighted total least-squares solution (WTLSS) is presented for an errors-in-variables model with fairly general variance-covariance matrices. In particular, the observations can be heteroscedastic and correlated, but the variance-covariance matrix of the dependent variables needs to have a certain block structure. An algorithm for the computation of the WTLSS is presented and applied to a straight-line fit problem where the data have been observed with different precision, and to a multiple regression problem from recently published climate change research.","10.1007/s00190-007-0190-9","85411","0.1558824"
"243","cold_topic_cluster_no_7","rank_4","681","Error Detection Scheme for the H.264/AVC using the RD Optimized Motion Vector Constraints","The corruption of compressed video bitstreams during their transmission over error-prone communication channels leads to degradation in the visual quality at the decoder. In order to effectively conceal the corrupted video data an exact error detection scheme is of utmost importance. In this paper, we present a new error detection scheme for the H.264/AVC using the RD optimized motion vector constraints to improve the error detection rate. The simulation results show that the detection rate of the proposed algorithm is about 86%, although the proposed scheme brings about a small amount of bitrate increment and causes a slight decrement in the PSNR during the encoding(1).","10.1109/TCE.2012.6311342","77006","0.1591837"
"244","cold_topic_cluster_no_7","rank_4","681","A DISTRIBUTED AND INCREMENTAL SVD ALGORITHM FOR AGGLOMERATIVE DATA ANALYSIS ON LARGE NETWORKS","In this paper it is shown that the SVD of a matrix can be constructed efficiently in a hierarchical approach. The proposed algorithm is proven to recover the singular values and left singular vectors of the input matrix A if its rank is known. Further, the hierarchical algorithm can be used to recover the d largest singular values and left singular vectors with bounded error. It is also shown that the proposed method is stable with respect to round-off errors or corruption of the original matrix entries. Numerical experiments validate the proposed algorithms and parallel cost analysis.","10.1137/16M1058467","61265","0.1918919"
"245","cold_topic_cluster_no_7","rank_4","681","Approximation Limits of Linear Programs (Beyond Hierarchies)","We develop a framework for proving approximation limits of polynomial size linear programs (LPs) from lower bounds on the nonnegative ranks of suitably defined matrices. This framework yields unconditional impossibility results that are applicable to any LP as opposed to only programs generated by hierarchies. Using our framework, we prove that O(n(1/2-is an element of))-approximations for CLIQUE require LPs of size 2(n Omega(is an element of)). This lower bound applies to LPs using a certain encoding of CLIQUE as a linear optimization problem. Moreover, we establish a similar result for approximations of semidefinite programs by LPs. Our main technical ingredient is a quantitative improvement of Razborov's [38] rectangle corruption lemma for the high error regime, which gives strong lower bounds on the nonnegative rank of shifts of the unique disjointness matrix.","10.1287/moor.2014.0694","64630","0.2092308"
"246","cold_topic_cluster_no_7","rank_5","880","The Stackelberg Model in Territorial Planning","We propose a new model for the formation of a public-private partnership mechanism, formulated as a bilevel Boolean programming problem. We show that this task is Sigma P-hard in both optimistic and pessimistic forms. We develop a stochastic iterative algorithm for solving this problem. We also present computational experiments on real information that demonstrate the capabilities of the proposed approach.","10.1134/S0005117919020073","29285","0.2181818"
"247","cold_topic_cluster_no_7","rank_5","880","Non-dominated sorting genetic algorithm with decomposition to solve constrained optimisation problems","Pareto-domination was adopted to handle not only trade-off between objective and constraints but also trade-off between convergence and diversity on solving a constrained optimisation problem (COP) in this paper like many other researchers. But there are some differences. This paper converts a COP into an equivalent dynamic constrained multi-objective optimisation problem (DCMOP) first, then dynamic version of non-dominated sorting genetic algorithm with decomposition (NSGA/D) is designed to solve the equivalent DCMOP, consequently solve the COP. A key issue for the NSGA/D working effectively is that the environmental change should not destroy the feasibility of the population. With a feasible population, the NSGA/D could solve well the DCMOP just as a MOEA usually can solve well an unconstrained MOP. Experimental results show that the NSGA/D outperforms or performs similarly to other state-of-the-art algorithms referred to in this paper, especially in global search.","10.1504/IJBIC.2013.055080","75585","0.2238095"
"248","cold_topic_cluster_no_7","rank_5","880","Theoretical and numerical analysis of an optimal control problem related to wastewater treatment","In this work we deal with the design and management of wastewater treatment systems, mainly the disposal of sea outfalls discharging polluting effluent from a sewerage system. This problem is formulated as a pointwise optimal control problem with state and control constraints. The main difficulties arise from the lack of regularity of the second member in the state system and from the pointwise constraints on the state variables. We develop the theoretical analysis of the problem, we propose an algorithm for its numerical resolution, and finally, we give results for a realistic problem posed in the ria of Vigo, Spain.","10.1137/S0363012998345640","91073","0.2282051"
"249","cold_topic_cluster_no_7","rank_5","880","Sustainability SI: Bikeway Network Design Model for Recreational Bicycling in Scenic Areas","A bikeway network design model was developed for recreational bicycling in scenic areas. The multi-objective 0-1 programming problem is to determine spatial layouts for networks of bikeways and service stations. The model objectives are maximizing bikeway service coverage, maximizing service station service coverage, minimizing cyclist risk and maximizing bikeway suitability. The model constraints are network connectivity, bikeway type, monetary budget, location relationships between bikeways and service stations, and value ranges of decision variables. The programming problem was solved by epsilon-constraint method in a case study of a bikeway network in North Coast & Guanyinshan National Scenic Area in northern Taiwan. Four of the six non-dominated alternatives generated in the case study were substantially superior to existing bikeways, and budget limits sensitively affected cyclist risk and bikeway suitability. The proposed network design model for recreational bikeways can assist planners in efficiently and systematically developing alternatives for further evaluation.","10.1007/s11067-014-9245-7","60477","0.2414634"
"250","cold_topic_cluster_no_7","rank_5","880","Two-Stage Chance-Constrained Fractional Programming for Sustainable Water Quality Management under Uncertainty","In this study, a two-stage chance-constrained fractional programming (TCFP) method is developed for dealing with water quality management problems associated with stochastic inputs. Two-stage chance-constrained fractional programming is a hybrid of stochastic linear fractional programming (SLFP) and two-stage stochastic programming (TSP) methods. It can not only balance objectives of two aspects through converting a bi-objective problem into a ratio one but can also analyze various policy scenarios when the promised production targets are violated. For demonstrating its advantages, the proposed TCFP method is applied to a case study of water quality management where managers have to consider conflicting objectives between economic development and environmental conservation, as well as stochastic features expressed as probability distributions. The obtained solutions under different significance levels can help managers to identify desired policies under various environmental, economic, and constraint-violation conditions.","10.1061/(ASCE)WR.1943-5452.0000470","65804","0.2446154"
"251","cold_topic_cluster_no_8","rank_1","156","How social media influence college students' smoking attitudes and intentions","Building on the influence of presumed influence (IPI) model, this study examines how smoking-related messages on social media influence college students' smoking. We surveyed 366 college students from three U.S. Midwestern universities in 2012 and examined the effects of expression and reception of smoking-related messages on smoking using path analysis. We found that the expression and reception of prosmoking messages not only directly affected smoking but also had indirect effects on smoking through (1) perceived peer expression of prosmoking messages and (2) perceived peer smoking norms. For antismoking messages, only reception had a significant indirect influence on smoking through (1) perceived peer reception of antismoking messages and (2) perceived peer smoking norms. In conclusion, social media function as an effective communication channel for generating, sharing, receiving, and commenting on smoking-related content and are thus influential on college students' smoking. ","10.1016/j.chb.2016.06.061","56728","0.3123288"
"252","cold_topic_cluster_no_8","rank_1","156","Why is such a smart person like you smoking? Using self-affirmation to reduce defensiveness to cigarette warning labels","When researchers communicate the negative health risks of smoking, smokers are likely to minimize such effects. This experiment addressed a way to reduce this defensiveness: allowing smokers to affirm aspects of the self. Smokers (n = 100) and nonsmokers (n = 30) viewed eight health-warning messages about smoking. Smokers were randomly assigned to view (a) warnings without a selfaffirmation manipulation, (b) warnings after a self-affirmation manipulation, or (c) warnings that had a positive self- statement attached to it. Analyses indicated that compared to nonsmokers, no-affirmation smokers rated the warning messages as: (a) communicating less serious consequences, (b) less accurate, and (e) less likely to influence smokers. However, compared to no-affirmation smokers, smokers who affirmed the self were no more likely to rate the messages as serious, accurate, or effective. These data suggest that affirming the self before, or using a self-affirmation within a warning message may not encourage smokers to be more accepting of risk information.","10.1111/j.1751-9861.2005.tb00010.x","88793","0.3338983"
"253","cold_topic_cluster_no_8","rank_1","156","Smokers' beliefs about the relative safety of other tobacco products: Findings from the ITC Collaboration","Most tobacco control efforts in western countries focus on the factory-made, mass-produced (FM) cigarette, whereas other tobacco products receive relatively little attention. Noncombusted tobacco products (i.e., referred to as smokeless tobacco), particularly Swedish-style snus, carry lower disease risks, compared with combusted tobacco products such as cigarettes. In this context, it is important to know what tobacco users believe about the relative harmfulness of various types of tobacco products. Data for this study came from random-digit-dialed telephone surveys of current smokers aged 18 or older in Australia, Canada, the United Kingdom, and the United States. Three waves of data, totaling 13,322 individuals, were assessed. Items assessed use of and beliefs about the relative harms of cigars, pipes, smokeless tobacco, and FM and roll-your-own cigarettes, as well as sociodemographics and smoking behaviors. Cigars (2.8%-12.7%) were the other tobacco products most commonly used by current cigarette smokers, followed by pipes (0.3%-2.1%) and smokeless tobacco (0.0%-2.3%). A significant minority of smokers (12%-21%) used roll-your-own cigarettes at least some of the time. About one-quarter of smokers believed that pipes, cigars, or roll-your-own cigarettes were safer than FM cigarettes, whereas only about 13% responded correctly that smokeless tobacco was less hazardous than cigarettes. Multivariate analyses showed that use of other tobacco products was most strongly related to beliefs about the reduced harm of these other products. Use of other tobacco products was low but may be growing among smokers in the four countries studied. Smokers are confused about the relative harms of tobacco products. Health education efforts are needed to correct smoker misperceptions.","10.1080/14622200701591583","87249","0.3370690"
"254","cold_topic_cluster_no_8","rank_1","156","A Combination of Factors Related to Smoking Behavior, Attractive Product Characteristics, and Socio-Cognitive Factors are Important to Distinguish a Dual User from an Exclusive E-Cigarette User","Although total cessation of nicotine and tobacco products would be most beneficial to improve public health, exclusive e-cigarette use has potential health benefits for smokers compared to cigarette smoking. This study investigated differences between dual users and exclusive e-cigarette users provide information to optimize health communication about smoking and vaping. A cross-sectional survey (n = 116) among 80 current, adult dual users and 36 current, adult-exclusive e-cigarette users was conducted in the Netherlands. The questionnaire assessed four clusters of factors: (1) Past and current smoking and vaping behavior, (2) product characteristics used, (3) attractiveness and reasons related to cigarettes and e-cigarettes, and (4) socio-cognitive factors regarding smoking, vaping, and not smoking or vaping. We used random forest-a machine learning algorithm-to identify distinguishing features between dual users and e-cigarette users. We are able to discern a dual user from an exclusive e-cigarette user with 86.2% accuracy based on seven factors: Social ties with other smokers, quantity of tobacco cigarettes smoked in the past (e-cigarette users) or currently (dual users), self-efficacy to not vape and smoke, unattractiveness of cigarettes, attitude towards e-cigarettes, barriers: accessibility of e-cigarettes, and intention to quit vaping (A). This combination of features provides information on how to improve health communication about smoking and vaping.","10.3390/ijerph16214191","20615","0.3578431"
"255","cold_topic_cluster_no_8","rank_1","156","The impact of cigarette warning labels and smoke-free bylaws on smoking cessation - Evidence from former smokers","Background: To effectively address the health burden of tobacco use, tobacco control programs must find ways of motivating smokers to quit. The present study examined the extent to which former smokers' motivation to quit was influenced by two tobacco control policies recently introduced in the Waterloo Region: a local smoke-free bylaw and graphic cigarette warning labels. Methods: A random digit-dial telephone survey was conducted with 191 former smokers in southwestern Ontario, Canada in October 2001. Former smokers who had quit in the previous three years rated the factors that influenced their decision to quit and helped them to remain abstinent. Results: Thirty-six percent of former smokers cited smoke-free policies as a motivation to quit smoking. Former smokers who quit following the introduction of a total smoke-free bylaw were 3.06 (CI95 = 1.02-9.19) times more likely to cite smoking bylaws as a motivation to quit, compared to former smokers who quit prior to the bylaw. A total of 31% participants also reported that cigarette warning labels had motivated them to quit. Former smokers who quit following the introduction of the new graphic warning labels were 2.78 (CI95 = 1.20-5.94) times more likely to cite the warnings as a quitting influence than former smokers who quit prior to their introduction. Finally, 38% of all former smokers surveyed reported that smoke-free policies helped them remain abstinent and 27% reported that warning labels helped them do so. Conclusion: More stringent smoke-free and labelling policies were associated with a greater impact upon motivations to quit.","10.1007/BF03403649","89204","0.3876106"
"256","cold_topic_cluster_no_8","rank_2","708","Predictors of substance abuse treatment outcomes in Tennessee","In planning and implementing programs to treat substance abuse, it is important to understand which factors influence post-treatment abstinence. This article identifies and analyzes several variables important in predicting the likelihood of abstinence among substance abuse clients. The data used in this study was collected from 1,350 clients treated for alcohol or drug abuse in residential, halfway house, or outpatient facilities in Tennessee. We analyzed 22 variables as possible treatment outcome predictors by using two statistical procedures: stepwise logistic regression analysis and Quick, Unbiased, Efficient, Statistical Tree (QUEST) analysis, a tree-structured classification algorithm analysis. We found one pre-treatment, five in-treatment, and three post-treatment variables to be significant predictors of treatment outcome: previous treatment history, perceived helpfulness of the treatment, simultaneous treatment for mental health, number of days in treatment, completion of treatment, special skills training during treatment, obtaining healthcare services for major physical health problem after treatment, living with someone using alcohol or drugs post treatment, and arrest record since treatment.","10.2190/RD7B-MDED-MEPJ-G7CD","90002","0.2380952"
"257","cold_topic_cluster_no_8","rank_2","708","Avatar-assisted therapy: a proof-of-concept pilot study of a novel technology-based intervention to treat substance use disorders","Background: Avatar-assisted therapy (AAT) is a novel and emerging technology that uses the Internet to enable clinicians and clients in substance abuse treatment to participate in group counseling sessions from separate and remote locations in real time through the use of avatars and virtual environments. Objectives: The current study is a pilot proof-of-concept feasibility study involving individuals in outpatient substance abuse treatment. This report addresses two questions: (1) are individuals who present for substance abuse treatment interested in receiving AAT and (2) what factors are associated with better treatment success. Methods: Individuals who presented at the treatment clinic who met study eligibility criteria, and provided their written informed consent to participate, were included in the current study (N = 59; 78% male). Results: Twenty-eight (47.5%) participants completed 16 weeks of treatment and attended more sessions compared to non-completers (M = 14.3 vs. 7.5 p < .05). Those individuals who completed treatment were less likely to have a positive urine drug screen at baseline (21.5 vs. 78.6%; p < .05). Furthermore, those individuals who successfully completed treatment were less likely to have positive urine drug screens during treatment compared to those who did not complete (29.7% vs. 70.3%, p < .05). There were no arrests during treatment for completers and non-completers. Conclusion: Poor retention in substance use disorder treatment has long been a major problem for public health. AAT is a feasible approach that has the potential to expand treatment to individuals who might have difficulty accessing treatment. Moreover, AAT may be appealing to clients who are concerned about anonymity and confidentiality.","10.1080/00952990.2017.1280816","49966","0.2416667"
"258","cold_topic_cluster_no_8","rank_2","708","Drug-Abuse Nanotechnology: Opportunities and Challenges","Opioid drug abuse and dependence/addiction are complex disorders regulated by a wide range of interacting networks of genes and pathways that control a variety of phenotypes. Although the field has been extensively progressed since the birth of the National Institute on Drug Abuse in 1974, the fundamental knowledge and involved mechanisms that lead to drug dependence/addiction are poorly understood, and thus, there has been limited success in the prevention of drug addiction and development of therapeutics for definitive treatment and cure of addiction disease. The lack of success in both identification of addiction in at-risk populations and the development of efficient drugs has resulted in a serious social and economic burden from opioid drug abuse with global increasing rate of mortality from drug overdoses. This perspective aims to draw the attention of scientists to the potential role of nanotechnologies, which might pave the way for the development of more practical platforms for either drug development or identification and screening of patients who may be vulnerable to addiction after using opioid drugs.","10.1021/acschemneuro.8b00127","34124","0.2500000"
"259","cold_topic_cluster_no_8","rank_2","708","Chemogenomics knowledgebased polypharmacology analyses of drug abuse related G-protein coupled receptors and their ligands","Drug abuse (DA) and addiction is a complex illness, broadly viewed as a neurobiological impairment with genetic and environmental factors that influence its development and manifestation. Abused substances can disrupt the activity of neurons by interacting with many proteins, particularly G-protein coupled receptors (GPCRs). A few medicines that target the central nervous system (CNS) can also modulate DA related proteins, such as GPCRs, which can act in conjunction with the controlled psychoactive substance(s) and increase side effects. To fully explore the molecular interaction networks that underlie DA and to effectively modulate the GPCRs in these networks with small molecules for DA treatment, we built a drug-abuse domain specific chemogenomics knowledgebase (DA-KB) to centralize the reported chemogenomics research information related to DA and CNS disorders in an effort to benefit researchers across a broad range of disciplines. We then focus on the analysis of GPCRs as many of them are closely related with DA. Their distribution in human tissues was also analyzed for the study of side effects caused by abused drugs. We further implement our computational algorithms/tools to explore DA targets, DA mechanisms and pathways involved in polydrug addiction and to explore polypharmacological effects of the GPCR ligands. Finally, the polypharmacology effects of GPCRs-targeted medicines for DA treatment were investigated and such effects can be exploited for the development of drugs with polypharmacophore for DA intervention. The chemogenomics database and the analysis tools will help us better understand the mechanism of drugs abuse and facilitate to design new medications for system pharmacotherapy of DA.","10.3389/fphar.2014.00003","71358","0.2952381"
"260","cold_topic_cluster_no_8","rank_2","708","Computational Systems Pharmacology-Target Mapping for Fentanyl-Laced Cocaine Overdose","The United States of America is fighting against one of its worst-ever drug crises. Over 900 people a week die from opioid- or heroin-related overdoses, while millions more suffer from opioid prescription addiction. Recently, drug overdoses caused by fentanyl-laced cocaine specifically are on the rise. Due to drug synergy and an increase in side effects, polydrug addiction can cause more risk than addiction to a single drug. In the present work, we systematically analyzed the overdose and addiction mechanism of cocaine and fentanyl. First, we applied our established chemogenomics knowledgebase and machine-learning-based methods to map out the potential and known proteins, transporters, and metabolic enzymes and the potential therapeutic target(s) for cocaine and fentanyl. Sequentially, we looked into the detail of (1) the addiction to cocaine and fentanyl by binding to the dopamine transporter and the mu opioid receptor (DAT and mu OR, respectively), (2) the potential drug-drug interaction of cocaine and fentanyl via p-glycoprotein (P-gp) efflux, (3) the metabolism of cocaine and fentanyl in CYP3A4, and (4) the physiologically based pharmacokinetic (PBPK) model for two drugs and their drug-drug interaction at the absorption, distribution, metabolism, and excretion (ADME) level. Finally, we looked into the detail of JWH133, an agonist of cannabinoid 2-receptor (CB2) with potential as a therapy for cocaine and fentanyl overdose. All these results provide a better understanding of fentanyl and cocaine polydrug addiction and future drug abuse prevention.","10.1021/acschemneuro.9b00109","23725","0.3281553"
"261","cold_topic_cluster_no_8","rank_3","337","Morphometry of mass-transport deposits as a predictive tool","Mass-transport deposits (MTDs) are gravity-induced units that represent an important component of modern and ancient deep-water stratigraphic successions. MTDs have been widely documented in the literature, but a comprehensive compilation of quantitative morphometric parameters associated with their external architecture is still lacking. This work presents a morphometric database that contains 332 data points that document the length, area, volume, and thickness of MTDs from different geologic ages and a variety of continental margins around the world. The compilation contains data collected from interpretations done by the authors in eastern offshore Trinidad and the Gulf of Mexico as well as from data mining from the peer-reviewed literature. Preliminary results indicate that there is a good correlation between a series of parameters that include the area, length, and volume of MTDs. On the other hand, the correlation between thickness and volume seems to be harder to document mainly due to lateral variations in thickness that are typical within MTDs. Data analysis suggests that previous qualitative classification of attached and detached MTDs can be validated by using a quantitative approach. This validation suggests that morphometric parameters associated with the architecture of MTDs can be used as a hint to link geologic setting, deposit geometry, and potential causal mechanisms. In addition, the defined morphometric relationships that were encountered between the different morphometric parameters (e.g., length and area) are useful to predict MTD dimensions in areas of the subsurface where data are limited and/or data quality is low. Likewise, these morphometric relationships can be used in outcrop studies where exposure of the MTD units is also limited.","10.1130/B31221.1","62021","0.1559322"
"262","cold_topic_cluster_no_8","rank_3","337","Geometric arrangement and operation mode adjustment in low-enthalpy geothermal borehole fields for heating","The efficient operation of ground source heat pump (GSHP) systems with multiple borehole heat exchangers (BHEs) over a lifetime of decades implies an optimized performance of the BHEs and a mitigation of the environmental impact of the system. This paper introduces a new combined optimization approach, which adjusts the BHE positions as well as the individually regulated energy extraction for each single BHE within a given borehole field in conduction dominated media for a given seasonal changing load profile. The optimization of only the BHE positions without optimizing the individual BHE loads nearly produces the same improvement of the underground temperature change of approximately 12% as an optimization of the BHE loads without optimized positioning. The combination of both optimization approaches results in only slightly better results compared to a result achieved by only one of the optimization approaches. Thus for homogeneous fields without groundwater flow, an optimal load assignment can be substituted by an optimal BHE placement, which leads to a considerably reduced complexity of the borehole field. ","10.1016/j.energy.2012.10.060","75725","0.1698795"
"263","cold_topic_cluster_no_8","rank_3","337","Numerical modeling of foreland basin formation: A program relating thrusting, flexure, sediment geometry and lithosphere rheology","An algorithm has been developed which allows the flexural deflection of the lithosphere to be calculated under thrust loading and the geometry of the sedimentary infill in the adjacent foreland basin. To that purpose we have considered arbitrarily shaped thrust-load systems moving towards the foreland, and surface processes denudating the orogen and filling the basin. The regional compensation of topographic loads is based on the assumption that the lithosphere behaves as a thin prate with either homogeneous (elastic, viscoelastic) behavior or more realistic depth-dependent elastic-plastic rheology. When using heterogeneous rheology, the program calculates the flexural behavior as a function of crustal geometry and thermal regime of the lithosphere, thus relating the basin infill geometry with the deep lithosphere properties. We show some examples where the geometry of the basin and sedimentary infill (e.g. onlap/toplap patterns) are generated assuming different lithosphere rheologies and synthetic tectonic contexts which support the applicability of the model to study the formation and evolution of foreland basins. ","10.1016/S0098-3004(97)00057-5","91878","0.1769231"
"264","cold_topic_cluster_no_8","rank_3","337","Subaerial and Subaqueous Investigations of Volcanic Debris Avalanche and Lahar Deposits on the Northern Coast of Ulleung Island, Korea","Volcanic debris avalanche and lahar deposits associated with small-scale lava dome collapse have been poorly documented in volcanic islands. The Chusan Formation was emplaced on the collapsed northern flank of Ulleung Island, Korea. Subaerial and subaqueous investigations of the collapsed northern part of Ulleung Island have been performed based on shaded relief images generated from a digital elevation model (DEM), a GeoEye satellite image, outcrop observations, and multi-beam echosounder and high-resolution Chirp sub-bottom profiling systems. The Chusan Formation is an elongated (length, 800 m; width, 250 m) system of valley-confined deposits connected to the Albong lava dome within the Nari caldera depression. A 40-m-thick outcrop of the Chusan Formation consists of three aggradational units that were emplaced by lahars, volcanic debris avalanches, and mixed deposition due to these processes. The peat layer between the overlying Chusan Formation and underlying reworked sediments was dated at 3,070-3,275 cal B.P., matching the emplacement of the Chusan Formation. Equivalent subaqueous deposits of the Chusan Formation were not detected in the northern shelf; thus, large-scale caldera collapse deposits covered the marine terrace before emplacement of the Chusan Formation. The valley confined Chusan Formation is the result of an aggradational succession of lahar and volcanic debris avalanche deposits associated with the Albong lava dome collapse, corresponding to the most recent volcanic activity of Ulleung Island.","10.2112/SI90-047.1","25507","0.1771930"
"265","cold_topic_cluster_no_8","rank_3","337","Luminescence dating of ice-marginal deposits in northern Germany: evidence for repeated glaciations during the Middle Pleistocene (MIS 12 to MIS 6)","The exact number, extent and chronology of the Middle Pleistocene Elsterian and Saalian glaciations in northern Central Europe are still controversial. This study presents new luminescence data from Middle Pleistocene ice-marginal deposits in northern Germany, giving evidence for repeated glaciations during the Middle Pleistocene (MIS 12 to MIS 6). The study area is located in the Leine valley south of the North German Lowlands. The data set includes digital elevation models, high-resolution shear wave seismic profiles, outcrop and borehole data integrated into a 3D subsurface model to reconstruct the bedrock relief surface. For numerical age determination, we performed luminescence dating on 12 ice-marginal and two fluvial samples. Luminescence ages of ice-marginal deposits point to at least two ice advances during MIS 12 and MIS 10 with ages ranging from 461 +/- 34 to 421 +/- 25ka and from 376 +/- 27 to 337 +/- 21ka. The bedrock relief model and different generations of striations indicate that the older ice advance came from the north and the younger one from the northeast. During rapid ice-margin retreat, subglacial overdeepenings were filled with glaciolacustrine deposits, partly rich in re-worked Tertiary lignite and amber. During MIS 8 and MIS 6, the study area may have been affected by two ice advances. Luminescence ages of glaciolacustrine delta deposits point to a deposition during MIS 8 or early MIS 6, and late MIS 6 (250 +/- 20 to 161 +/- 10ka). The maximum extent of both the Elsterian (MIS 12 and MIS 10) and Saalian glaciations (MIS 8? and MIS 6) approximately reached the same position in the Leine valley and was probably controlled by the formation of deep proglacial lakes in front of the ice sheets, preventing a further southward advance.","10.1111/bor.12083","67741","0.2145038"
"266","cold_topic_cluster_no_8","rank_4","166","KEGG orthology-based annotation of the predicted proteome of Acropora digitifera: ZoophyteBase - an open access and searchable database of a coral genome","Background: Contemporary coral reef research has firmly established that a genomic approach is urgently needed to better understand the effects of anthropogenic environmental stress and global climate change on coral holobiont interactions. Here we present KEGG orthology-based annotation of the complete genome sequence of the scleractinian coral Acropora digitifera and provide the first comprehensive view of the genome of a reef-building coral by applying advanced bioinformatics. Description: Sequences from the KEGG database of protein function were used to construct hidden Markov models. These models were used to search the predicted proteome of A. digitifera to establish complete genomic annotation. The annotated dataset is published in ZoophyteBase, an open access format with different options for searching the data. A particularly useful feature is the ability to use a Google-like search engine that links query words to protein attributes. We present features of the annotation that underpin the molecular structure of key processes of coral physiology that include (1) regulatory proteins of symbiosis, (2) planula and early developmental proteins, (3) neural messengers, receptors and sensory proteins, (4) calcification and Ca2+-signalling proteins, (5) plant-derived proteins, (6) proteins of nitrogen metabolism, (7) DNA repair proteins, (8) stress response proteins, (9) antioxidant and redox-protective proteins, (10) proteins of cellular apoptosis, (11) microbial symbioses and pathogenicity proteins, (12) proteins of viral pathogenicity, (13) toxins and venom, (14) proteins of the chemical defensome and (15) coral epigenetics. Conclusions: We advocate that providing annotation in an open-access searchable database available to the public domain will give an unprecedented foundation to interrogate the fundamental molecular structure and interactions of coral symbiosis and allow critical questions to be addressed at the genomic level based on combined aspects of evolutionary, developmental, metabolic, and environmental perspectives.","10.1186/1471-2164-14-509","73974","0.1895105"
"267","cold_topic_cluster_no_8","rank_4","166","Prediction of protein corona on nanomaterials by machine learning using novel descriptors","Effective in silico methods to predict protein corona compositions on engineered nanomaterials (ENMs) could help elucidate the biological outcomes of ENMs in biosystems without the need for conducting lengthy experiments for corona characterization. However, the physicochemical properties of ENMs, used as the descriptors in current modeling methods, are insufficient to represent the complex interactions between ENMs and proteins. Herein, we utilized the fluorescence change (FC) from fluorescamine labeling on a protein, with or without the presence of the ENM, as a novel descriptor of the ENM to build machine learning models for corona formation. FCs were significantly correlated with the abundance of the corresponding proteins in the corona on diverse classes of ENMs, including metal and metal oxides, nanocellulose, and 2D ENMs. Prediction models established by the random forest algorithm using FCs as the ENM descriptors showed better performance than the conventional descriptors, such as ENM size and surface charge, in the prediction of corona formation. Moreover, they were able to predict protein corona formation on ENMs with very heterogeneous properties. We believe this novel descriptor can improve in silico studies of corona formation, leading to a better understanding on the protein adsorption behaviors of diverse ENMs in different biological matrices. Such information is essential for gaining a comprehensive view of how ENMs interact with biological systems in ENM safety and sustainability assessments.","10.1016/j.impact.2020.100207","18036","0.1901961"
"268","cold_topic_cluster_no_8","rank_4","166","Comparative proteomic profiling of murine skin","Mammalian skin is regularly exposed to different environmental stresses, each of which results in specific compensatory changes in protein expression that can be assessed by proteomic analysis. We have established a reference proteome map of BALB/c murine skin allowing the resolution of greater than 500 protein spots in a single two-dimensional polyacrylamide gel. Forty-four protein spots, corresponding to 28 different cutaneous proteins, were identified using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry and the Mascot online database searching algorithm. Twenty-five proteins were expressed at higher levels in the epidermis, whereas only nine were found predominantly in the subepidermal tissues. A subset of protein spots exhibited strain-specific expression. Proteins of diverse function were identified, including those involved in stress response, apoptosis, growth inhibition, the maintenance of structural integrity, translational control, energy metabolism, calcium binding, cholesterol transport, and the scavenging of free radicals. Prohibitin expression was detected cutaneously, with more abundant protein and mRNA levels in the epidermis. Five molecular chaperones including protein di-sulfide isomerase, 78 kDa glucose-regulated protein precursor, heat shock protein 60 (HSP60), HSP70, and HSP27 were also identified. Of these, HSP27 expression was confined mainly to the epidermis, and expression of protein disulfide isomerase was found primarily in the subepidermal tissues. Proteomic analysis of skin following heat or cold shock resulted in increased levels of HSP27, HSP60, and HSP70 suggesting involvement of these chaperones in the cutaneous response mechanism to temperature stress. These data establish numerous reference markers within the proteome map of murine skin and provide an important framework for future efforts aimed at characterization of the epidermal and subepidermal responses to environmental changes.","10.1046/j.1523-1747.2003.12327.x","89707","0.2083916"
"269","cold_topic_cluster_no_8","rank_4","166","A membrane protease is targeted to the relict plastid of Toxoplasma via an internal signal sequence","The apicoplast is a secondary plastid found in Toxoplasma gondii, Plasmodium species and many other apicomplexan parasites. Although the apicoplast is essential to parasite survival, little is known about the protein constituents of the four membranes surrounding the organelle. Luminal proteins are directed to the endoplasmic reticulum (ER) by an N-terminal signal sequence and from there to the apicoplast by a transit peptide domain. We have identified a membrane-associated AAA protease in T. gondii, FtsH1. Although the protein lacks a canonical bipartite-targeting sequence, epitope-tagged FtsH1 colocalizes with the recently identified apicoplast membrane marker APT1 and immunoelectron microscopy confirms the residence of FtsH1 on plastid membranes. Trafficking appears to occur via the ER because deletion mutants lacking the peptidase domain are retained in the ER. When extended to include the peptidase domain, the protein trafficks properly. The transmembrane domain is required for localization of the full-length protein to the apicoplast and a truncation mutant to the ER. Thus, at least two distinct regions of FtsH1 are required for proper trafficking, but they differ from those of luminal proteins and would not be detected by the algorithms currently used to identify apicoplast proteins.","10.1111/j.1600-0854.2007.00637.x","86368","0.2235294"
"270","cold_topic_cluster_no_8","rank_4","166","Resolubilization of Protein from Water-Insoluble Phlorotannin-Protein Complexes upon Acidification","Marine phlorotannins (PhT) from Laminaria digitata might protect feed proteins from ruminal digestion by formation of insoluble non-covalent tannin-protein complexes at rumen pH (6-7). Formation and disintegration of PhT protein complexes was studied with,beta-casein (random coil) and bovine serum albumin (BSA, globular) at various pH-PhT had similar binding affinity for beta-casein and BSA as pentagalloyl glucose, as studied by fluorescence quenching. The affinity of PhT for both proteins was independent of pH (3.0, 6.0, and 8.0). In the presence of PhT, the pH range for precipitation of tannin protein complexes widened to 0.5-1.5 pH units around the isoelectric point (pI) of the protein. Complete protein resolubilization from insoluble PhT-protein complexes was achieved at pH 7 and 2 for beta-casein and BSA, respectively. It was demonstrated that PhT modulate the solubility of proteins at neutral pH and that resolubilization of PhT-protein complexes at pH deviating from pI is mainly governed by the charge state of the protein.","10.1021/acs.jafc.7b03779","43415","0.3112676"
"271","cold_topic_cluster_no_8","rank_5","284","Neural Correlates of Gender Face Perception in Transgender People","To date, MRI studies focused on brain sexual dimorphism have not explored the presence of specific neural patterns in gender dysphoria (GD) using gender discrimination tasks. Considering the central role of body image in GD, the present study aims to evaluate brain activation patterns with 3T-scanner functional MRI (fMRI) during gender face discrimination task in a sample of 20 hormone-naive transgender and 20 cisgender individuals. Additionally, participants were asked to complete psychometric measures. The between-group analysis of average blood oxygenation level dependent (BOLD) activations of female vs. male face contrast showed a significant positive cluster in the bilateral precuneus in transmen when compared to the ciswomen. In addition, the transwomen group compared to the cismen showed higher activations also in the precuneus, as well as in the posterior cingulate gyrus, the angular gyrus and the lateral occipital cortices. Moreover, the activation of precuneus, angular gyrus, lateral occipital cortices and posterior cingulate gyrus was significantly associated with higher levels of body uneasiness. These results show for the first time the existence of a possible specific GD-neural pattern. However, it remains unclear if the differences in brain phenotype of transgender people may be the result of a sex-atypical neural development or of a lifelong experience of gender non-conformity.","10.3390/jcm9061731"," 9837","0.2357895"
"272","cold_topic_cluster_no_8","rank_5","284","The Occupational Brain Plasticity Study Using Dynamic Functional Connectivity Between Multi-Networks: Take Seafarers for Example","The exploration of brain plasticity through functional magnetic resonance imaging (fMRI) technology is a hot topic in the field of brain science research. In order to explore the specificity of the brain functional networks of seafarers and the influence of marine environment on seafarer brain functional networks. In this paper, nine resting-state brain functional networks of seafarers were studied by using group independent component analysis with intrinsic reference method based on the full use of fMRI priori information, and the static and dynamic functional connections between these brain networks were statistically analyzed. The results showed that there was a significant difference between seafarers and non-seafarers in the dynamic functional connectivity of individual subjects. Furthermore, the dynamic functional connection patterns between the two groups of subjects corresponding to the nine brain networks were extracted by using the sliding time window and clustering methods. It was found through analysis that the brain functional networks underwent specific functional recombination and transformation during the process of brain activity, and showed dynamic functional connectivity states which were significantly different from those of non-seafarers. The research results have important reference value for revealing the specificity of the brain function neural activity of seafarer population and the brain plasticity of seafarer occupation.","10.1109/ACCESS.2019.2946322","30266","0.2358696"
"273","cold_topic_cluster_no_8","rank_5","284","Prostate biopsy in the diagnosis of prostate cancer: Current trends and techniques","The advent of prostate specific antigen (PSA) screening and transrectal ultrasonography (TRUS) has had a significant impact on the detection of prostate cancer over the last 15 years. The mean age at diagnosis has decreased and the most common stage at diagnosis is now localized disease. TRUS guidance, spring-loaded biopsy needles, utilization of oral antibiotic prophylaxis, developments in local anesthesia, increases in the number of cores sampled and the use of site-specific containers have all made the prostate biopsy easier to perform and more accurate. The indications for an initial prostate biopsy have been strongly influenced by digital rectal examinations (DRIES), PSA levels and the PSA-related parameters of velocity, density, and percent free. These parameters, along with abnormal histology, also dictate the need for a repeat biopsy. With the better, earlier, and more patient-friendly usage of the prostate biopsy, there has been a decrease in the mortality rate of prostate cancer. ","10.1358/dot.2005.41.3.892523","88647","0.2361111"
"274","cold_topic_cluster_no_8","rank_5","284","Violence and Latin-American preadolescents: A study of social brain function and cortisol levels","The present study investigated exposure to violence and its association with brain function and hair cortisol concentrations in Latin-American preadolescents. Self-reported victimization scores (JVQ-R2), brain imaging (fMRI) indices for a social cognition task (the 'eyes test'), and hair cortisol concentrations were investigated, for the first time, in this population. The eyes test is based on two conditions: attributing mental state or sex to pictures of pairs of eyes (Baron-Cohen, Wheelwright, Hill, Raste, & Plumb, 2001). The results showed an association among higher victimization scores and (a) less activation of posterior temporoparietal right-hemisphere areas, in the mental state condition only (including right temporal sulcus and fusiform gyrus); (b) higher functional connectivity indices for the Amygdala and Right Fusiform Gyrus (RFFG) pair of brain regions, also in the mental state condition only; (c) higher hair cortisol concentrations. The results suggest more exposure to violence is associated with significant differences in brain function and connectivity. A putative mechanism of less activation in posterior right-hemisphere regions and of synchronized Amygdala: RFFG time series was identified in the mental state condition only. The results also suggest measurable effects of exposure to violence in hair cortisol concentrations, which contribute to the reliability of self-reported scores by young adolescents. The findings are discussed in light of the effects of exposure to violence on brain function and on social-cognitive development in the adolescent brain. A video abstract of this article can be viewed at","10.1111/desc.12799","22925","0.2508929"
"275","cold_topic_cluster_no_8","rank_5","284","Health risk behaviors and prostate specific antigen awareness among men in California","Purpose: Differences in prostate specific antigen awareness may contribute to differences in the frequency of prostate specific antigen testing. We investigated the association of health risk behaviors, including smoking, physical inactivity, obesity and excessive alcohol consumption, with awareness of the prostate specific antigen test in men in California at risk for prostate cancer. Materials and Methods: Using 2003 data from the California Health Interview Survey, a population based, random digit dial telephone survey, the records of 7,297 men 50 years or older without a history of prostate cancer were identified. The outcome was self-reported awareness of the prostate specific antigen test. The main independent variables were smoking status, physical activity level, body mass index and alcohol consumption. The prevalence, OR and 95% CI for prostate specific antigen awareness were calculated using SUDAAN (R) to account for the complex sampling design. Results: The overall prevalence of prostate specific antigen awareness was 73.0%. After controlling for potential confounders the odds of being aware of the prostate specific antigen test was lower in current smokers (vs never smoked OR 0.53, 95% CI 0.41-0.68), physically inactive men (vs physically active OR 0.77, 95% CI 0.63-0.93) and obese men (vs normal weight OR 0.77, 95% CI 0.62-0.95). Conclusions: Health risk behaviors are associated with lower prostate specific antigen awareness. Our findings suggest opportunities for focused health education interventions and quality improvement programs tailored to men who,engage in unhealthy behaviors to improve their prostate specific antigen test awareness.","10.1016/j.juro.2008.04.007","85302","0.2638655"
"276","cold_topic_cluster_no_9","rank_1","589","Towards the retrieval of tropospheric ozone with the Ozone Monitoring Instrument (OMI)","We have assessed the sensitivity of the operational Ozone Monitoring Instrument (OMI) ozone profile retrieval algorithm to a number of a priori and radiative transfer assumptions. We studied the effect of stray light correction, surface albedo assumptions and a priori ozone profiles on the retrieved ozone profile. Then, we studied how to modify the algorithm to improve the retrieval of tropospheric ozone. We found that stray light corrections have a significant effect on the retrieved ozone profile but mainly at high altitudes. Surface albedo assumptions, on the other hand, have the largest impact at the lowest layers. Choice of an ozone profile climatology which is used as a priori information has small effects on the retrievals at all altitudes. However, the usage of climatological a priori covariance matrix has a significant effect. Based on these sensitivity tests, we made several modifications to the retrieval algorithm: the a priori ozone climatology was replaced with a new tropopause-dependent climatology, the a priori covariance matrix was calculated from the climatological ozone variability values, and the surface albedo was assumed to be linearly dependent on wavelength in the 311.5-330 nm channel. As expected, we found that the a priori covariance matrix basically defines the vertical distribution of degrees of freedom for a retrieval. Moreover, our case study over Europe showed that the modified version produced over 10% smaller ozone abundances in the troposphere which reduced the systematic overestimation of ozone in the retrieval algorithm and improved correspondence with Infrared Atmospheric Sounding Instrument (IASI) retrievals. The comparison with ozonesonde measurements over North America showed that the operational retrieval performed better in the upper troposphere/lower stratosphere (UTLS), whereas the modified version improved the retrievals in the lower troposphere and upper stratosphere. These comparisons showed that the systematic biases in the OMI ozone profile retrievals are not caused by the a priori information but by some still unidentified problem in the radiative transfer modelling. Instead, the a priori information pushes the systematically wrong ozone profiles towards the true values. The smaller weight of the a priori information in the modified retrieval leads to better visibility of tropospheric ozone structures, because it has a smaller tendency to damp the variability of the retrievals in the troposphere. In summary, the modified retrieval unmasks systematic problems in the radiative transfer/instrument model and is more sensitive to tropospheric ozone variation; that is, it is able to capture the tropospheric ozone morphology better.","10.5194/amt-8-671-2015","67588","0.3034091"
"277","cold_topic_cluster_no_9","rank_1","589","A novel tropopause-related climatology of ozone profiles","A new ozone climatology, based on ozonesonde and satellite measurements, spanning the altitude region between the earth's surface and similar to 60 km is presented (TpO(3) climatology). This climatology is novel in that the ozone profiles are categorized according to calendar month, latitude and local tropopause heights. Compared to the standard latitude-month categorization, this presentation improves the representativeness of the ozone climatology in the upper troposphere and the lower stratosphere (UTLS). The probability distribution of tropopause heights in each latitude-month bin provides additional climatological information and allows transforming/comparing the TpO(3) climatology to a standard climatology of zonal mean ozone profiles. The TpO(3) climatology is based on high-vertical-resolution measurements of ozone from the satellite-based Stratospheric Aerosol and Gas Experiment II (in 1984 to 2005) and from balloon-borne ozonesondes from 1980 to 2006. The main benefits of the TpO(3) climatology are reduced standard deviations on climatological ozone profiles in the UTLS, partial characterization of longitudinal variability, and characterization of ozone profiles in the presence of double tropopauses. The first successful application of the TpO(3) climatology as a priori in ozone profile retrievals from Ozone Monitoring Instrument on board the Earth Observing System (EOS) Aura satellite shows an improvement of ozone precision in UTLS of up to 10 % compared with the use of conventional climatologies. In addition to being advantageous for use as a priori in satellite retrieval algorithms, the TpO(3) climatology might be also useful for validating the representation of ozone in climate model simulations.","10.5194/acp-14-283-2014","72243","0.3352381"
"278","cold_topic_cluster_no_9","rank_1","589","On the accuracy of Total Ozone Mapping Spectrometer retrievals over tropical cloudy regions","Motivated by the desire to accurately derive tropospheric ozone from Total Ozone Mapping Spectrometer (TOMS) measurements, we investigate several aspects of these observations in the presence of highly reflecting clouds. Using the collocated Temperature Humidity InfraRed (THIR) measurements of cloud-top pressures, we identify three TOMS algorithm errors resulting from the inaccurate assignment of cloud-top pressure. The most significant error results from the inappropriate tropospheric ozone amount added below cloudy scenes to complete the total ozone column. After accounting for the cloud-height errors, we find significant total ozone column excesses of 10-15 Dobson units (DU) (1 DU 2.6867 x 10(16) molecules cm(-2)) over high-altitude, highly reflecting clouds compared with clear area observations. After accounting for additional algorithm errors involving the tropospheric ozone climatology and considering potential dynamical, photochemical, and NIMBUS-7/Earth Probe calibration errors, approximately 4-9 DU excesses over cloudy scenes remain. We speculate that the TOMS algorithm approximation of clouds as opaque Lambertian reflecting surfaces may account for a significant portion of these unexplained excesses. The excess ozone due to calibration error and unknown sources will affect the tropospheric ozone derived from TOMS measurements using clear/cloudy difference techniques.","10.1029/2000JD000151","90509","0.3482456"
"279","cold_topic_cluster_no_9","rank_1","589","Climatology 2011: An MLS and sonde derived ozone climatology for satellite retrieval algorithms","The ozone climatology used as the a priori for the version 8 Solar Backscatter Ultraviolet (SBUV) retrieval algorithms has been updated. The climatology was formed by combining data from Aura MLS (2004-2010) with data from balloon sondes (1988-2010). The Microwave Limb Sounder (MLS) instrument on Aura has excellent latitude coverage and measures ozone daily from the upper troposphere to the lower mesosphere. The new climatology consists of monthly average ozone profiles for ten degree latitude zones covering pressure altitudes from 0 to 65 km. Ozone below 8 km (below 12 km at high latitudes) is based on balloons sondes, while ozone above 16 km (21 km at high latitudes) is based on MLS measurements. Sonde and MLS data are blended in the transition region. Ozone accuracy in the upper troposphere is greatly improved because of the near uniform coverage by Aura MLS, while the addition of a large number of balloon sonde measurements improves the accuracy in the lower troposphere, in the tropics and southern hemisphere in particular. The addition of MLS data also improves the accuracy of the climatology in the upper stratosphere and lower mesosphere. The revised climatology has been used for the latest reprocessing of SBUV and TOMS satellite ozone data.","10.1029/2011JD017006","77535","0.4511364"
"280","cold_topic_cluster_no_9","rank_1","589","Ozone climatological profiles for satellite retrieval algorithms","[1] A new altitude-dependent ozone climatology has been produced for use with the version 8 Total Ozone Mapping Spectrometer (TOMS) and Solar Backscatter Ultraviolet retrieval algorithms. The climatology consists of monthly average ozone profiles for 10 degrees latitude zones covering altitudes from 0 to 60 km (in Z* pressure altitude coordinates). The climatology was formed by combining data from Stratospheric Aerosol and Gas Experiment II (SAGE II; 1988-2001) or Microwave Limb Sounder (MLS; 1991-1999) with data from balloon sondes (1988-2002). Ozone below 10 km is based on balloon sondes, whereas ozone at 19 km and above is based on SAGE II measurements. When SAGE data are not available (at high latitudes), MLS data are used. The ozone climatology in the southern hemisphere and tropics has been greatly improved in recent years by the addition of a large number of balloon sonde measurements made under the Southern Hemisphere Additional Ozonesondes program. The new climatology better represents the seasonal behavior of ozone in the troposphere, including the known hemispheric asymmetry, and in the upper stratosphere. A modification of this climatology was used for the TOMS version 8 retrieval that includes total ozone dependence, which is important in the lower stratosphere. Comparisons of TOMS ozone with ground stations show improved accuracy over previous TOMS retrievals due in part to the new climatology.","10.1029/2005JD006823","86936","0.4829787"
"281","cold_topic_cluster_no_9","rank_2","717","TRMM-Based Optical and Microphysical Features of Precipitating Clouds in Summer Over the Yangtze-Huaihe River Valley, China","The optical and microphysical features of precipitating clouds are key information for studying the satellite-based precipitation estimation, cloud radiative effects, aerosol-cloud-precipitation interactions, cloud and precipitation parameterization in weather and climate models. In this study, 15-year synchronous spectral and radar observations from the TRMM satellite were used to statistically explore the optical and microphysical features of precipitating clouds (PCs), including cloud effective radius (CER), cloud optical thickness (COT), cloud water path (CWP), thermal infrared brightness temperature at channel 4 (TB4) of cloud top, and storm top height (STH) and their relationships with surface rain rates in summer over Yangtze-Huaihe River Valley (YHRV). Results show that the optical and microphysical features of PCs/stratiform PCs/convective PCs vary with geographical locations in summer over YHRV, due to the different ambient meteorological and topographical conditions. Higher CER/COT/CWP/STH and lower TB4 mainly locate at areas of bigger rain rates. For PCs, their spatial distribution of CER is mainly dominated by stratiform PCs, while their spatial distribution of COT/CWP is mainly dominated by convective PCs. Moreover, stratiform precipitation is the dominant form in summer over YHRV and, thus, most PCs present vertical structures of optical and microphysical features as stratiform PCs. Stratiform PCs are usually thicker and contain more water vapor with bigger cloud particles than convective PCs (including deep and shallow convective PCs). In addition, existing shallow convective PCs are associated with lower storm heights and warmer cloud tops. Finally, surface rain rates of PCs (convective/stratiform PCs) increase gradually with the increment of CER/COT/CWP/STH, especially under 5 (15/5) mm/h. Similar relationship between surface rain rates and COT/CWP for shallow convective PCs is also found under 0.75mm/h. Surface rain rate of PCs (convective/stratiform PCs) with cold cloud tops (TB4<247K) obviously increases as TB4 decreases. Differently, for shallow convective PCs with warmer cloud tops (TB4>264K), surface rain rate usually increases as CER decreases, which suggests that aerosol indirect effects are dominant in lower PCs, because over pollution regions abundant aerosols enterinto lower clouds more easily and then suppress the development of shallow convective PCs.","10.1007/s00024-018-1940-8","31746","0.3043062"
"282","cold_topic_cluster_no_9","rank_2","717","Performance of the GCOM-C/SGLI satellite prelaunch phase cloud properties' algorithm","The performance of the cloud properties algorithm of the future Global Change Observation Mission-Climate/Second-Generation Global Imager (GCOM-C/SGLI) satellite is compared with that of a spectrally compatible sensor, the moderate resolution image spectroradiometer (MODIS). The results obtained are evaluated against the target accuracy of the GCOM-C/SGLI satellite mission. Three direct cloud parameters: the cloud optical thickness (COT), the cloud particle effective radius (CLER), and the cloud top temperature (CTT), and an indirect parameter: the cloud liquid water path (CLWP), are the cloud properties that are evaluated. The satellite-satellite comparison shows a good alignment between the retrievals of the GCOM-C/SGLI algorithm and those of MODIS in most of the areas and agreement with the accuracy targets of the new satellite mission. However, the COT comparison shows an increasing dispersion with the increase of the cloud thickness along the GCOM-C/SGLI-MODIS 1 : 1 line. The CTT is systematically overestimated by the GCOM-C/SGLI (against MODIS), particularly in mid-thermal clouds. This is found to be due to an insufficient cloud emissivity correction of the thermal radiances by the GCOM-C/SGLI algorithm. The lowest COT, CLER, and CLWP accuracies, noticed in forest areas, are found to be related to the cloud detection uncertainty and the nonabsorption channel sensitivity differences. ","10.1117/1.JRS.7.073693","73427","0.3169811"
"283","cold_topic_cluster_no_9","rank_2","717","FACTORS CONTROLLING ERBE LONGWAVE CLEAR-SKY AND CLOUD FORCING FLUXES","The factors controlling the Earth Radiation Budget satellite (ERBS) longwave clear sky and cloud-forcing fluxes are investigated using statistical analyses of the ERBS fluxes with International Satellite Cloud Climatology Project (ISCCP) cloud and ECMWF thermodynamic variables. For both land and ocean between 60 degrees S and 60 degrees N statistically significant models exist relating interannual variations of clear sky fluxes and surface temperature, precipitable water, tropospheric temperature, total cloud amount, and cloud-top pressure. An analysis of mean clear sky fluxes suggests that mean ERBS longwave clear sky fluxes are overestimates of the ''true'' values by between +2 and +10 W m(-2) over the area investigated. The biases appear to be most related to errors in the detection or exclusion of low clouds in the Earth Radiation Budget Experiment clear sky determination algorithm. Other statistical models show that variations in total cloud amount and cloud-top pressure control cloud forcing over oceans, but that total cloud amount and clear sky flux are most important over land. Thus over land, especially at higher latitudes, care must be taken not to interpret changes in cloud forcing solely in terms of variations in cloud parameters.","10.1175/1520-0442(1995)008<1889:FCELCS>2.0.CO;2","92387","0.3200000"
"284","cold_topic_cluster_no_9","rank_2","717","A 10-year climatology of vertical properties of most active convective clouds over the Indian regions using TRMM PR","Vertical distribution of hydrometeors in the most intense convective clouds over the Indian region during the summer monsoon season (JJAS) is described for ten climatologically important areas. Tropical Rainfall Measuring Mission Precipitation Radar (TRMM PR) 3D radar reflectivity data is used in the present study for 10 years (2001-2010). The study constructs a convective cloud cell based on reflectivity thresholds, known as most intense convective cloud. The cloud cells are formed by taking the maximum reflectivity (Ze) at each altitude in the convective area with at least one radar pixel containing reflectivity of 40 dBZ or more. TRMM 2A23 data was used to eliminate the stratiform clouds from our analyses. The Vertical structure of convective clouds were studied over the east and west coast of India, and observation shows that the east coast consists of a higher frequency of convective clouds with high reflectivity values in average vertical profiles. It is observed that over the northeastern parts of the Indian subcontinent, similar to 30 % of convective cells extend beyond 15-km height whereas it is only similar to 4 % over the central Bay of Bengal. Over the Western Ghats, similar to 13 % of the cells have their tops below the freezing level, i.e. warm clouds do give heavy rain here. The regional differences in the vertical profile are high between the 5- and 12-km altitude. Most intense convective cells (MICCs) with a cloud top height more than 10 and 15 km show different characteristics, and the Western Ghats shows the most intense average vertical profile. Above 12 km, the western coast shows increased reflectivity value. Convective intensity is higher over the land-dominated areas for the cloud cells and decreases when we restrict the cloud cells to a certain altitude.","10.1007/s00704-015-1641-5","55582","0.3273504"
"285","cold_topic_cluster_no_9","rank_2","717","Application of cloud vertical structure from CloudSat to investigate MODIS-derived cloud properties of cirriform, anvil, and deep convective clouds","CloudSat cloud vertical structure is combined with the CALIPSO Lidar and Collection-5 Level 2 cloud data from Aqua's Moderate Resolution Imaging Spectroradiometer (MODIS) to investigate the mean properties of high/cirriform, anvil, and deep convective (DC) clouds. Cloud properties are sampled over 30 degrees S-30 degrees N for 1year and compared to existing results of Collection-4 Aqua MODIS high-level cloud observations where cloud types were categorized using the International Satellite Cloud Climatology Project (ISCCP) cloud classification scheme. Results show high/cirriform sampled in this study have high biases in cloud top pressure and temperature due to CloudSat's sensitivity to thin high clouds. Mean cloud properties of DC show reasonable agreement with existing DC results notwithstanding mean cloud optical thickness which is similar to 23% higher due to the exclusion of thick cirrus and anvil clouds. Anvil cloud properties are a mix between high/cirriform and DC according to ISCCP cloud optical thickness thresholds whereby similar to 80% are associated with high/cirriform and the other 20% are associated with DC. The variability of cloud effective particle radii was also evaluated using DC with 5dBZ echoes at and above 10km. No evidence of larger cloud effective particle radii are given despite considering higher reaching echoes. Using ISCCP cloud optical thickness thresholds, similar to 25% of DC would be classified as cirrostratus clouds. These results provide a basis to evaluate the uncertainty of the ISCCP cloud classification scheme and MODIS-derived cloud properties using active satellite observations.","10.1002/jgrd.50306","74499","0.3465517"
"286","cold_topic_cluster_no_9","rank_3","715","Comparison of in situ and satellite ocean color determinations of particulate organic carbon concentration in the global ocean","Ocean color satellite missions have provided more than 16-years of consistent, synoptic observations of global ocean ecosystems. Surface chlorophyll concentrations (Chl) derived from satellites have been traditionally used as a metric for phytoplankton biomass. In recent years interpretation of ocean-color satellite data has progressed beyond the estimation of Chl. One of the newer ocean color products is particulate organic carbon (POC) concentration. In this paper we carry out comparisons of simultaneous satellite and in situ POC determinations. Our results indicate that the performance of the standard NASA POC algorithm (Stramski et al., 2008) is comparable to the standard empirical band ratio algorithms for Chl. ","10.1016/j.oceano.2014.09.002","67599","0.3300000"
"287","cold_topic_cluster_no_9","rank_3","715","Bio-optical characteristics of phytoplankton populations in the upwelling system off the coast of Chile","Phytoplankton samples collected from two cruises off the coast of Chile were analysed for pigment composition and absorption characteristics. High pigment concentrations (up to 20 mg chl-a m(-3)) were found in the upwelled waters over the shelf break off the coast of Concepcion during spring (October 1998), but relatively oligotrophic conditions were found further offshore. Similarly, stations further north (between Coquimbo and Iquique), sampled during the austral summer (February 1999), also showed low pigment concentrations, characterised by the presence of prymnesiophytes, and cyanobacteria including Prochlorococcus sp. The specific absorption coefficient of phytoplankton at 443 nm (a*(ph) (443)) was much higher for the offshore population than the inshore population, which was dominated by large diatoms. These differences are attributed to changes in pigment packaging and pigment composition. The relative proportion of non-photosynthetic carotenoids to chl-a, together with the ratio of the peak height of the Gaussian bands in the blue and red regions of the spectrum, p(435)/p(676), (an indicator of the importance of the packaging effect) could account for up to 92 % of the total variation in a*(ph) (443). Blue/green absorption ratios were strongly related to the relative concentration of 19'-hexanoyloxyfucoxanthin and fucoxanthin. A reasonable agreement was found between in situ and satellite estimates of chl-a (SeaWiFS data) despite the large variability in phytoplankton specific absorption coefficients, suggesting that the 'global' absorption-to-chlorophyll relationships encompass the regional variations observed off the coast of Chile. Satellite chl-a was overestimated in oligotrophic water when compared to HPLC chl-a measurements, apparently because of the high specific absorption coefficients of phytoplankton in the offshore waters. On the other hand, ship and satellite data were in closer agreement when in situ fluorometric chl-a data was used. It is likely that the correlation between in situ and satellite chl-a could be improved by using regional algorithms.","","89330","0.3305970"
"288","cold_topic_cluster_no_9","rank_3","715","Relating chlorophyll from cyanobacteria-dominated inland waters to a MERIS bloom index","The cyanobacteria index (Cl) has been applied to Medium Resolution Imaging Spectrometer (MERIS) imagery to characterize cyanobacterial biomass in diverse regions. While a consistent qualitative estimate of cyanobacterial biomass is useful, establishing universal relationships to chlorophyll a (chl) concentration, the dominant photosynthetic pigment in phytoplankton, would quantify blooms for regional comparisons. Relationships between chl concentrations were determined from water reflectance measurements and chl concentration from water samples taken from cyanobacterial blooms in eutrophic lakes in Florida, where chl ranged from 16 to 115 mu g L-1. When the chl relationship was applied to simultaneous satellite and field data, the chl concentration determined from satellite Cl showed negligible bias and root mean square error of 27%. The generic Cl-based chl algorithm presented here seems suitable for consistent quantification of chl concentration within cyanobacterial blooms.","10.1080/2150704X.2015.1117155","60909","0.3353846"
"289","cold_topic_cluster_no_9","rank_3","715","Regional patterns of particulate spectral absorption in the Pearl River estuary","Spectral absorption coefficients of the total particulate, a(p)(lambda), nonalgal particulate, a(d)(lambda), and phytoplankton pigment, a(ph)(lambda), in the Pearl River estuary and its vicinity waters were determined using the quantitative filter technique. The particulate absorption ap(443) ranged from 0.04 to 1.82 m(-1), with the corresponding a(ph)(443) ranging from 0.016 to 0.484 m(-1). Two typical spectral patterns are found for the total particulate absorption. For the first typical spectral pattern, the total particulate absorption spectra are similar to that of nonalgal particulate, with values of absorption coefficient decreasing with wavelength. In contrast, for the second spectral pattern the spectral absorptions by total particulate are very similar to that of phytoplankton pigment. The spectral dependency of absorption by nonalgal particulate follows an exponential increase toward short wavelengths, with an average slope of 0.012 +/- 0.002 nm(-1). The nonalgal absorption and the fraction of the nonalgal particulate absorption to the total particulate absorption exhibit a distinct trend of decreasing with salinity of the surface water. Phytoplankton pigment absorption exhibits a clear trend of increasing nonlinearly with chlorophyll a concentration. The relationships between the phytoplankton pigment absorption and chlorophyll a concentration can be described by power law, with the determination coefficient r(2) of 0.82. But only weak relationships between a(p)(lambda) and chlorophyll a concentration are observed, with the determination coefficient r(2) of 0.42. The relatively large scatter around ap(443) versus chl-a relationship would be attributed to the effects of loading of the nonalgal particulate absorption. Our analysis indicated that such relationships similar to that for Case I waters can be applicable to optically complex Case II waters if the effects caused by nonalgal are corrected. The chlorophyll-specific absorption coefficients of phytoplankton pigment are not constant, it increases with decreasing chlorophyll a level. To improve the accuracy of bio-optical algorithms for remote sensing in coastal waters, further investigations on the variations of specific absorption of chlorophyll pigment must be made.","10.1360/03wd0151","89538","0.3396947"
"290","cold_topic_cluster_no_9","rank_3","715","Light Absorption by Phytoplankton in the Upper Mixed Layer of the Black Sea: Seasonality and Parametrization","Standard NASA ocean color algorithm OC4 was developed on the basis of ocean optical data and while appropriate for Case 1 oceanic waters could not be adequately applied for the Black Sea waters due to its different bio-optical properties. OC4 algorithm is shown to overestimate chlorophyll concentration (Chl-a) in summer and underestimate Chl-a during early spring phytoplankton blooms in the Black Sea. For correct conversion of satellite data to Chl-a, primary production and other indicators regional algorithms should be developed taking into account bio-optical properties of the Black Sea waters. Light absorption by phytoplankton pigments-a(ph)(lambda) have been measured in open sea and shelf Black Sea waters in different seasons since 1998. It was shown that the first optical depth was located within the upper mixed layer (UML) for most of the year with the exception of the spring when seasonal stratification was developing. As a result spectral features of water leaving radiance were determined by optical properties of the UML. Significant seasonal differences in Chl-a specific light absorption coefficients of phytoplankton within UML have been revealed. These differences were caused by adaptive changes of composition and intracellular pigment concentration due to variable environment conditions-mainly light intensity. Empirical relationships between a(ph)(lambda) and Chl-a were derived by least squares fitting to power functions for different seasons. Incorporation of these results will refine the regional ocean color models and provide improved and seasonally adjusted estimates of chlorophyll a concentration, downwelling radiance and primary production in the Black Sea based on satellite data.","10.3389/fmars.2017.00090","55676","0.3958333"
"291","cold_topic_cluster_no_9","rank_4","425","Satellite and CALJET aircraft observations of atmospheric rivers over the eastern north pacific ocean during the winter of 1997/98","This study uses a unique combination of airborne and satellite observations to characterize narrow regions of strong horizontal water vapor flux associated with polar cold fronts that occurred over the eastern North Pacific Ocean during the winter of 1997/98. Observations of these ""atmospheric rivers'' are compared with past numerical modeling studies to confirm that such narrow features account for most of the instantaneous meridional water vapor transport at midlatitudes. Wind and water vapor profiles observed by dropsondes deployed on 25-26 January 1998 during the California Land-falling Jets Experiment (CALJET) were used to document the structure of a modest frontal system. The horizontal water vapor flux was focused at low altitudes in a narrow region ahead of the cold front where the combination of strong winds and large water vapor content were found as part of a low-level jet. A close correlation was found between these fluxes and the integrated water vapor (IWV) content. In this case, 75% of the observed flux through a 1000-km cross-front baseline was within a 565-km-wide zone roughly 4 km deep. This zone contained 1.5 x 10(8) kg s(-1) of meridional water vapor flux, the equivalent of similar to20% of the global average at 35degreesN. By compositing polar-orbiting satellite Special Sensor Microwave Imager (SSM/I) data from 46 dates containing long, narrow zones of large IWV, it was determined that the single detailed case was representative of the composite in terms of both the IWV amplitude (3.09 cm vs 2.81 cm) and the width of the area where IWV greater than or equal to 2 cm (424 km vs 388 km). The SSM/I composites also showed that the width scales (defined by the 75% cumulative fraction along a 1500-km cross-plume baseline) for cloud liquid water and rain rate were 176 and 141 km, respectively, which are narrower than the 417 km for IWV. Examination of coincident Geostationary Operational Environmental Satellite (GOES) and SSM/I satellite data revealed that GOES cloud-top temperatures were coldest and cloud-top pressures were lowest in the core of the IWV plumes, and that the core cloud tops became substantially colder and deeper for larger IWV. A strong latitudinal dependence of the satellite-derived cross-river characteristics was also found. Atmospheric rivers form a critical link between weather and climate scales. They strongly influence both short-term weather and flood prediction, as well as seasonal climate anomalies and the global water cycle, through their cumulative effects. However, the rivers remain poorly observed by the existing global atmospheric observing system in terms of their horizontal water vapor fluxes.","10.1175/1520-0493(2004)132<1721:SACAOO>2.0.CO;2","89132","0.1653659"
"292","cold_topic_cluster_no_9","rank_4","425","Moisture Origin and Meridional Transport in Atmospheric Rivers and Their Association with Multiple Cyclones","During December 2006 many cyclones traveled across the North Atlantic, causing temperature and precipitation in Norway to be well above average. Large excursions of high vertically integrated water vapor, often referred to as atmospheric rivers, reached from the subtropics to high latitudes, inducing precipitation over western Scandinavia. The sources and transport of atmospheric water vapor in the North Atlantic storm track during that month are examined by means of a mesoscale model fitted with water vapor tracers. Decomposition of the modeled total water vapor field into numerical water vapor tracers tagged by evaporation latitude shows that when an atmospheric river was present, a higher fraction of water vapor from remote, southerly source regions caused more intense precipitation. The tracer transport analysis revealed that the atmospheric rivers were composed of a sequence of meridional excursions of water vapor, in close correspondence with the upper-level flow configuration. In cyclone cores, fast turnover of water vapor by evaporation and condensation were identified, leading to a rapid assimilation of water from the underlying ocean surface. In the regions of long-range transport, water vapor tracers from the southern midlatitudes and subtropics dominated over local contributions. By advection of water vapor along their trailing cold fronts cyclones were reinforcing the atmospheric rivers. At the same time the warm conveyor belt circulation was feeding off the atmospheric rivers by large-scale ascent and precipitation. Pronounced atmospheric rivers could persist in the domain throughout more than one cyclone's life cycle. These findings emphasize the interrelation between midlatitude cyclones and atmospheric rivers but also their distinction from the warm conveyor belt airstream.","10.1175/MWR-D-12-00256.1","73946","0.1692308"
"293","cold_topic_cluster_no_9","rank_4","425","Deep Convection and Column Water Vapor over Tropical Land versus Tropical Ocean: A Comparison between the Amazon and the Tropical Western Pacific","The relationships between the onset of tropical deep convection, column water vapor (CWV), and other measures of conditional instability are analyzed with 2 yr of data from the DOE Atmospheric Radiation Measurement (ARM) Mobile Facility in Manacapuru, Brazil, as part of the Green Ocean Amazon (GOAmazon) campaign, and with 3.5 yr of CWV derived from global positioning system meteorology at a nearby site in Manaus, Brazil. Important features seen previously in observations over tropical oceans-precipitation conditionally averaged by CWV exhibiting a sharp pickup at high CWV, and the overall shape of the CWV distribution for both precipitating and nonprecipitating points-are also found for this tropical continental region. The relationship between rainfall and CWV reflects the impact of lower-free-tropospheric moisture variability on convection. Specifically, CWV over land, as over ocean, is a proxy for the effect of free-tropospheric moisture on conditional instability as indicated by entraining plume calculations from GOAmazon data. Given sufficient mixing in the lower troposphere, higher CWV generally results in greater plume buoyancies through a deep convective layer. Although sensitivity of buoyancy to other controls in the Amazon is suggested, such as boundary layer and microphysical processes, the CWV dependence is consistent with the observed precipitation onset. Overall, leading aspects of the relationship between CWV and the transition to deep convection in the Amazon have close parallels over tropical oceans. The relationship is robust to averaging on time and space scales appropriate for convective physics but is strongly smoothed for averages greater than 3 h or 2.58.","10.1175/JAS-D-16-0119.1","57266","0.1992537"
"294","cold_topic_cluster_no_9","rank_4","425","A Comparative Study of Atmospheric Moisture Recycling Rate between Observations and Models","Precipitation and column water vapor data from 13 CMIP5 models and observational datasets are used to analyze atmospheric moisture recycling rate from 1988 to 2008. The comparisons between observations and model simulations suggest that most CMIP5 models capture two main characteristics of the recycling rate: 1) long-term decreasing trend of the global-average maritime recycling rate (atmospheric recycling rate over ocean within 608S-608N) and 2) dominant spatial patterns of the temporal variations of the recycling rate (i.e., increasing in the intertropical convergence zone and decreasing in subtropical regions). All models, except one, successfully simulate not only the long-term trend but also the interannual variability of column water vapor. The simulations of precipitation are relatively poor, especially over the relatively short time scales, which lead to the discrepancy of the recycling rate between observations and the CMIP5 models. Comparisons of spatial patterns also suggest that the CMIP5 models simulate column water vapor better than precipitation. The comparative studies indicate the scope of improvement in the simulations of precipitation, especially for the relatively short-time-scale variations, to better simulate the recycling rate of atmospheric moisture, an important indicator of climate change.","10.1175/JCLI-D-17-0421.1","39385","0.2035714"
"295","cold_topic_cluster_no_9","rank_4","425","Tropopause level Rossby wave breaking in the Northern Hemisphere: a feature-based validation of the ECHAM5-HAM climate model","Breaking synoptic-scale Rossby waves (RWB) at the tropopause level are central to the daily weather evolution in the extratropics and the subtropics. RWB leads to pronounced meridional transport of heat, moisture, momentum, and chemical constituents. RWB events are manifest as elongated and narrow structures in the tropopause-level potential vorticity (PV) field. A feature-based validation approach is used to assess the representation of Northern Hemisphere RWB in present-day climate simulations carried out with the ECHAM5-HAM climate model at three different resolutions (T42L19, T63L31, and T106L31) against the ERA-40 reanalysis data set. An objective identification algorithm extracts RWB events from the isentropic PV field and allows quantifying the frequency of occurrence of RWB. The biases in the frequency of RWB are then compared to biases in the time mean tropopause-level jet wind speeds. The ECHAM5-HAM model captures the location of the RWB frequency maxima in the Northern Hemisphere at all three resolutions. However, at coarse resolution (T42L19) the overall frequency of RWB, i.e. the frequency averaged over all seasons and the entire hemisphere, is underestimated by 28%.The higher-resolution simulations capture the overall frequency of RWB much better, with a minor difference between T63L31 and T106L31 (frequency errors of -3.5 and 6%, respectively). The number of large-size RWB events is significantly underestimated by the T42L19 experiment and well represented in the T106L31 simulation. On the local scale, however, significant differences to ERA-40 are found in the higher-resolution simulations. These differences are regionally confined and vary with the season. The most striking difference between T106L31 and ERA-40 is that ECHAM5-HAM overestimates the frequency of RWB in the subtropical Atlantic in all seasons except for spring. This bias maximum is accompanied by an equatorward extension of the subtropical westerlies.","10.1002/joc.3631","72857","0.2131387"
"296","cold_topic_cluster_no_9","rank_5","860","Stochastic generator of monthly precipitation and monthly average temperature","A method is proposed for simulating the monthly precipitation and monthly average temperature of air near the earth's surface. The proposed algorithm is based on the method of inverse functions. The climatic parameters used in simulations were derived from the UEA/CRU database. The method was applied to simulate time series of monthly precipitation and monthly average temperatures of air over a dry land territory with a spatial resolution of 0.5 x 0.5degrees in latitude and longitude. The time series of the climatic parameters obtained with the proposed stochastic generator appear statistically adequate to actual data series for any time interval in months and years.","","90461","0.2347826"
"297","cold_topic_cluster_no_9","rank_5","860","A semi-parametric regression approach to climatological quantile estimation for generating percentile-based temperature extremes indices","A semi-parametric regression approach to quantile estimation for daily temperature data is proposed, in which both the biases and inhomogeneity are negligible, and is applied to the calculation of the six percentile-based Expert Team on Climate Change Detection and Indices (ETCCDI) temperature extremes indices. Comparisons of the results with those from the CLIMDEX datasets show that the three warmth indices in the latter are probably biased such that their linear trends under the RCP4.5 scenario seem to be overestimated. In order to avoid drawing misleading conclusions, it is necessary to re-examine currently adopted algorithms and available datasets, and to develop new methods for generating the percentile-based ETCCDI indices.","10.1002/asl.724","48425","0.2428571"
"298","cold_topic_cluster_no_9","rank_5","860","A Gauge-based analysis of daily precipitation over East Asia","A new gauge-based analysis of daily precipitation has been constructed on a 0.5 degrees latitude-longitude grid over East Asia (5 degrees-60 degrees N, 65 degrees-155 degrees E) for a 26-yr period from 1978 to 2003 using gauge observations at over 2200 stations collected from several individual sources. First, analyzed fields of daily climatology are computed by interpolating station climatology defined as the summation of the first six harmonics of the 365-calendar-day time series of the mean daily values averaged over a 20-yr period from 1978 to 1997. These fields of daily climatology are then adjusted by the Parameter-Elevation Regressions on Independent Slopes Model (PRISM) monthly precipitation climatology to correct the bias caused by orographic effects. Gridded fields of the ratio of daily precipitation to the daily climatology are created by interpolating the corresponding station values using the optimal interpolation method. Analyses of total daily precipitation are finally calculated by multiplying the daily climatology by the daily ratio.","10.1175/JHM583.1","86763","0.2957143"
"299","cold_topic_cluster_no_9","rank_5","860","Stochastic simulation of daily air temperature and precipitation from monthly normals in North America north of Mexico","A simple, stochastic daily temperature and precipitation generator (TEMPGEN) was developed to generate inputs for the study of the effects of climate change on models driven by daily weather information when climate data are available as monthly summaries. The model uses as input only 11 sets of monthly normal statistics from individual weather stations. It needs no calibration, and was parameterized and validated for use in Canada and the continental United States. Monthly normals needed are: mean and standard deviation of daily minimum and maximum temperature, first and second order autoregressive terms for daily deviations of minimum and maximum temperatures from their daily means, correlation of deviations of daily minimum and maximum temperatures, total precipitation, and the interannual variance of total precipitation. The statistical properties and distributions of daily temperature and precipitation data produced by this generator compared quite favorably with observations from 708 stations throughout North America (north of Mexico). The algorithm generates realistic seasonal patterns, variability and extremes of temperature, precipitation, frost-free periods and hot spells. However, it predicts less accurately the daily probability of precipitation, extreme precipitation events and the duration of extreme droughts.","10.1007/s00484-006-0078-z","86823","0.3361702"
"300","cold_topic_cluster_no_9","rank_5","860","What are daily maximum and minimum temperatures in observed climatology?","Instrumental daily maximum and minimum temperatures are reported and archived from various surface thermometers along with different average algorithms in historical and current U.S. surface climate networks. An instrumental bias in daily maximum and minimum temperatures caused by surface temperature sensors due to the different sampling rates, average algorithms, and sensor's time constants was examined using a Gaussian-distributed function of surface air temperature fluctuations in simulation. In this study, the field observations were also included to examine the effects of average algorithms used in reporting daily maximum and minimum temperatures. Compared to the longest-recorded and standard liquid-in-glass maximum and minimum thermometers, some surface climate networks produced a systematic warming (cooling) bias in daily maximum (minimum) temperature observations, thus, resulting biases made the diurnal temperature range (DTR) more biased in extreme climate studies. Our study clarified the ambiguous concepts on daily maximum and minimum temperature observations defined by the World Meteorological Organization (WMO) in terms of sensor's time constants and average lengths and an accurate description of daily maximum and minimum temperatures is recommended to avoid the uncertainties occurred in the observed climatology. ","10.1002/joc.1536","85778","0.3913462"
"301","hot_topic_cluster_no_1","rank_1","121","A Large Epidemic of Hepatitis B in Serbia: An Integrated Model for Outbreak Investigations in Healthcare Settings","We report a comprehensive approach for outbreak investigations, including cluster analysis (Bernoulli model), an algorithm to build inferential models, and molecular techniques to confirm cases. Our approach may be an interesting tool to best exploit the large amount of unsystematically collected information available during outbreak investigations in healthcare settings.","10.1086/676432","70421","0.2333333"
"302","hot_topic_cluster_no_1","rank_1","121","Information Technology and the pandemic: a preliminary multinational analysis of the impact of mobile tracking technology on the COVID-19 contagion control","This paper explores the benefits and drawbacks of government surveillance within a public health crisis, specifically the COVID-19 outbreak of 2020. We review the current state of COVID-19 infection tracking by public health authorities, and then we examine the effectiveness of voluntary and mandatory mobile contact-tracing apps by COVID-19-positive or suspected positive individuals in China, Germany, Italy, Singapore, South Korea, and the United States. Through a Difference-In-Differences test, the apps were found to be highly significantly correlated with a reduction in the spread of COVID-19 in their countries. Robustness tests were run with four alternative models and the results are kept and presented within. In light of the success of these apps, ethical implications for their use during and beyond this public health crisis are discussed, including data governance and individual privacy issues.","10.1080/0960085X.2020.1802358"," 6838","0.2428571"
"303","hot_topic_cluster_no_1","rank_1","121","THE COVID-19 DISEASE SITUATION PRIOR TO OCTOBER 2020 - IN SELECTED COUNTRIES: THE POTENTIALLY SEVERE CONSEQUENCES OF COVID-19 ON THE NERVOUS SYSTEM AND SEVERAL KEY SOLUTIONS TO COVID-19 IN RELATION TO DIGITIZATION AND ARTIFICIAL INTELLIGENCE","Background: The aim of this brief analysis is to discuss the COVID-19 situation in selected countries prior to October 2020, using relevant statistics. In relation to the COVID-19 pandemic and the current situation in African countries, we will also touch upon certain specificities of missionary and charity work. With regards to the demonstrated consequences of the Spanish flu, we would like to point out certain severe consequences which COVID-19 could have on the nervous system and then discuss the co-morbidities related to COVID-19. Conclusion: This analysis, including areas mentioned in the final section, also opens space for scientific and expert discussions focused on the specificities of data analysis related to the COVID-19 pandemic and its connection to artificial intelligence, plus the need to carry out further interesting research on the issue of COVID-19 and artificial intelligence. These studies can be of significant help for similar diseases or pandemics.","","16276","0.2500000"
"304","hot_topic_cluster_no_1","rank_1","121","COVID-19: Ocular Manifestations and the APAO Prevention Guidelines for Ophthalmic Practices","The World Health Organization declared the Coronavirus Disease 2019 (COVID-19) caused by Severe Acute Respiratory Syndrome Coronavirus 2 a ""Pandemic"" on March 11, 2020. As of June 1, 2020, Severe Acute Respiratory Syndrome Coronavirus 2 has infected >6.2 million people and caused >372,000 deaths, including many health care personnel. It is highly infectious and ophthalmologists are at a higher risk of the infection due to a number of reasons including the proximity between doctors and patients during ocular examinations, microaerosols generated by the noncontact tonometer, tears as a potential source of infection, and some COVID-19 cases present with conjunctivitis. This article describes the ocular manifestations of COVID-19 and the APAO guidelines in mitigating the risks of contracting and/or spreading COVID-19 in ophthalmic practices.","10.1097/APO.0000000000000.308"," 8507","0.2583333"
"305","hot_topic_cluster_no_1","rank_1","121","Awareness, Perceptions, and Attitude Regarding Coronavirus Disease 2019 (COVID-19) Among Ophthalmologists in Jordan: Cross-Sectional Online Survey","Purpose: COVID-19 pandemic is a serious public health concern, and the role of health care workers is essential in preventing spread. The study objective is to investigate awareness, perception, and attitude towards COVID-19 and infection control measures among ophthalmologists in Jordan. Methods: The design of the study was a cross-sectional survey among Jordanian ophthalmologists. An online survey was sent to 197 ophthalmologists on March 23, 2020. Information on participants' socio-demographic characteristics, knowledge of clinical features of COVID-19, risk assessment and infection control measures for preventing disease transmission, and attitude towards treating patients with COVID-19 were collected. Results: A total of 132 ophthalmologists (mean age 40.5 years) responded, 23 (17.6%) had received training on infection control in ophthalmology. Most ophthalmologists were aware of COVID-19 symptoms and ways to identify patients at risk of having the disease, correctly reported modes of transmission and were aware of measures for preventing COVID-19 transmission in the ophthalmic setup. Social media was the commonest source of information on COVID-19 (75%), and 38.2% were not provided with instructions on infection control plan to reduce transmission of COVID-19. Most ophthalmologists (79.5%) thought that the virus could be detected in tears, and 72.2% thought that red eye as a symptom of COVID-19. Conclusion: Ophthalmologists in Jordan were aware of the epidemiology of COVID-19 and related infection preventive measures. Knowledge was lacking regarding ocular aspects of the disease. Training on infection prevention needs to be improved. Access to guidelines from international ophthalmological organizations should be promoted and seeking updated literature from peer-reviewed journals needs to be encouraged.","10.2147/OPTH.S260460","17162","0.2829268"
"306","hot_topic_cluster_no_1","rank_2","212","Reach of Messages in a Dental Twitter Network: Cohort Study Examining User Popularity, Communication Pattern, and Network Structure","Background: Increasing the reach of messages disseminated through Twitter promotes the success of Twitter-based health education campaigns. Objective: This study aimed to identify factors associated with reach in a dental Twitter network (1) initially and (2) sustainably at individual and network levels. Methods: We used instructors' and students' Twitter usernames from a Saudi dental school in 2016-2017 and applied Gephi (a social network analysis tool) and social media analytics to calculate user and network metrics. Content analysis was performed to identify users disseminating oral health information. The study outcomes were reach at baseline and sustainably over 1.5 years. The explanatory variables were indicators of popularity (number of followers, likes, tweets retweeted by others), communication pattern (number of tweets, retweets, replies, tweeting/ retweeting oral health information or not). Multiple logistic regression models were used to investigate associations. Results: Among dental users, 31.8% had reach at baseline and 62.9% at the end of the study, reaching a total of 749,923 and dropping to 37,169 users at the end. At an individual level, reach was associated with the number of followers (baseline: odds ratio, OR=1.003, 95% CI=1.001-1.005 and sustainability: OR=1.002, 95% CI=1.0001-1.003), likes (baseline: OR=1.001, 95% CI=1.0001-1.002 and sustainability: OR=1.0031, 95% CI=1.0003-1.002), and replies (baseline: OR=1.02, 95% CI=1.005-1.04 and sustainability: OR=1.02, 95% CI=1.004-1.03). At the network level, users with the least followers, tweets, retweets, and replies had the greatest reach. Conclusions: Reach was reduced by time. Factors increasing reach at the user level had different impact at the network level. More than one strategy is needed to maximize reach.","10.2196/10781","35317","0.2863636"
"307","hot_topic_cluster_no_1","rank_2","212","Twitter Campaigns Around the Fifth IPCC Report: Campaign Spreading, Shared Hashtags, and Separate Communities","In this article, we analyzed campaigning on Twitter around the publication of the fifth Intergovernmental Panel for Climate Change (IPCC) Working Group 1 report in September, 2013. In particular, we analyzed how participation in a specific campaign and use of hashtags connected to the campaign developed over time and what kind of sub-flows of tweets or spinoff conversations emerged. The campaign hashtag that we observed later appeared in connection to sharing of an article that was not directly connected to the original campaign. Although both the original campaign and this sub-flow of it were connected to the broader context of climate change, the sub-flow formed a separate community of tweeters that did not overlap with tweeters participating in the original campaign. Twitter campaigns have flexible boundaries both around the shared issues and around the communities of tweeters. Our results show that using information spreading approach does not account for the evolution of campaign spreading on Twitter, as other factors, such as celebrity endorsement, may heavily influence the spread of information and content on Twitter. Thus, our results suggest that although different tweeters participated in the two separate campaigns using shared hashtags, hashtags per se do not always indicate shared communities of tweeters nor can they always be considered as indicators of completely shared issues online.","10.1177/2158244016659117","57535","0.2974684"
"308","hot_topic_cluster_no_1","rank_2","212","News Media Trust and News Consumption: Factors Related to Trust in News in 35 Countries","The changes in how people consume news and the emergence of digital and distributed news sources call for a reexamination of the relationship between news use and trust in news. Previous research had suggested that alternative news use is correlated with lower levels of trust in news, whereas mainstream news use is correlated with higher levels of trust in news. Our research, based on a survey of news users in 35 countries, shows that using either mainstream or alternative news sources is associated with higher levels of trust in news. However, we find that using social media as a main source of news is correlated with lower levels of trust in news. When looking at country effects, we find that systemic factors such as the levels of press freedom or the audience share of the public service broadcaster in a country are not significantly correlated with trust in news.","","30801","0.3382353"
"309","hot_topic_cluster_no_1","rank_2","212","Dynamics of Health Agency Response and Public Engagement in Public Health Emergency: A Case Study of CDC Tweeting Patterns During the 2016 Zika Epidemic","Background: Social media have been increasingly adopted by health agencies to disseminate information, interact with the public, and understand public opinion. Among them, the Centers for Disease Control and Prevention (CDC) is one of the first US government health agencies to adopt social media during health emergencies and crisis. It had been active on Twitter during the 2016 Zika epidemic that caused 5168 domestic noncongenital cases in the United States. Objective: The aim of this study was to quantify the temporal variabilities in CDC's tweeting activities throughout the Zika epidemic, public engagement defined as retweeting and replying, and Zika case counts. It then compares the patterns of these 3 datasets to identify possible discrepancy among domestic Zika case counts, CDC's response on Twitter, and public engagement in this topic. Methods: All of the CDC-initiated tweets published in 2016 with corresponding retweets and replies were collected from 67 CDC-associated Twitter accounts. Both univariate and multivariate time series analyses were performed in each quarter of 2016 for domestic Zika case counts, CDC tweeting activities, and public engagement in the CDC-initiated tweets. Results: CDC sent out >84.0% (5130/6104) of its Zika tweets in the first quarter of 2016 when Zika case counts were low in the 50 US states and territories (only 560/5168, 10.8% cases and 662/38,885, 1.70% cases, respectively). While Zika case counts increased dramatically in the second and third quarters, CDC efforts on Twitter substantially decreased. The time series of public engagement in the CDC-initiated tweets generally differed among quarters and from that of original CDC tweets based on autoregressive integrated moving average model results. Both original CDC tweets and public engagement had the highest mutual information with Zika case counts in the second quarter. Furthermore, public engagement in the original CDC tweets was substantially correlated with and preceded actual Zika case counts. Conclusions: Considerable discrepancies existed among CDC's original tweets regarding Zika, public engagement in these tweets, and actual Zika epidemic. The patterns of these discrepancies also varied between different quarters in 2016. CDC was much more active in the early warning of Zika, especially in the first quarter of 2016. Public engagement in CDC's original tweets served as a more prominent predictor of actual Zika epidemic than the number of CDC's original tweets later in the year.","10.2196/10827","34579","0.3419162"
"310","hot_topic_cluster_no_1","rank_2","212","Utilizing Bots for Sustainable News Business: Understanding Users' Perspectives of News Bots in the Age of Social Media","The move of news audiences to social media has presented a major challenge for news organizations. How to adapt and adjust to this social media environment is an important issue for sustainable news business. News bots are one of the key technologies offered in the current media environment and are widely applied in news production, dissemination, and interaction with audiences. While benefits and concerns coexist about the application of bots in news organizations, the current study aimed to examine how social media users perceive news bots, the factors that affect their acceptance of bots in news organizations, and how this is related to their evaluation of social media news in general. An analysis of the US national survey dataset showed that self-efficacy (confidence in identifying content from a bot) was a successful predictor of news bot acceptance, which in turn resulted in a positive evaluation of social media news in general. In addition, an individual's perceived prevalence of social media news from bots had an indirect effect on acceptance by increasing self-efficacy. The results are discussed with the aim of providing a better understanding of news audiences in the social media environment, and practical implications for the sustainable news business are suggested.","10.3390/su12166515"," 7072","0.3811765"
"311","hot_topic_cluster_no_1","rank_3","60","Embedding Young Children's Participation Rights into Research: How the Interactive Narrative Approach Enhances Meaningful Participation","This research explored how young children's research participation can be enhanced when an interactive narrative approach is embedded within research to enhance children's consent to participation and their understanding of the research process. The context for this research was a 1-h, science outreach programme delivered into Australian playgroups. An interactive digital story was devised to inform nine children (aged 3-4 years) about the purposes of the research. Across a period of 11-16 weeks and across three occasions, an informing story about the research was used with the children to elicit their descriptions and explanations about their research role. Data gathered from semi-structured interviews with parents and audio- and video-recorded conversations with children also aided in the analyses to understand how children perceived their role as a research participant. Through cross-case analysis, six aspects of meaningful research participation were identified from children's and parents' responses: engagement, journeying, authenticity, consequence, ownership, and identity. The research extends current knowledge about young children's rights to understand their participatory role in research and about the considerations that can be put in place to ensure participation is personally meaningful to children. ResumeCette recherche explore comment la participation de jeunes enfants a la recherche peut etre amelioree quand une approche narrative interactive est integree a la recherche pour ameliorer le consentement des enfants a la participation et leur comprehension des processus de recherche. Le contexte de cette recherche est un programme d'une heure de sensibilisation a la science dispense dans des groupes de jeu australiens. Une histoire numerique interactive a ete concue afin d'informer neuf enfants (ages de 3 a 4 ans) des objectifs de la recherche. Sur une periode de 11 a 16 semaines et a trois reprises, une histoire informative sur la recherche a ete utilisee avec les enfants pour obtenir leurs descriptions et explications sur leur role dans la recherche. Les donnees recueillies a partir d'entrevues semi-structurees avec les parents et d'enregistrements audio et video de conversations avec les enfants ont egalement aide aux analyses pour comprendre comment les enfants percevaient leurs roles de participants a la recherche. Au moyen d'une analyse de cas transversale, six aspects d'une participation significative a la recherche ont ete identifies dans les reponses des enfants et des parents: engagement, cheminement, authenticite, consequence, appropriation et identite. La recherche elargit les connaissances actuelles sur les droits des jeunes enfants a comprendre leur role participatif dans la recherche et sur les mesures qui peuvent etre mises en place pour assurer que la participation soit personnellement significative pour les enfants. ResumenEl presente estudio investigo la forma en que la participacion de ninos pequenos en investigaciones puede mejorarse al incorporar un Metodo Narrativo Interactivo en la investigacion para aumentar la participacion voluntaria de los ninos y su comprension sobre los procesos de investigacion. Se utilizo un programa de ciencia de una hora de largo alcance comunitario suministrado a grupos de actividades ludicas en Australia. Se diseno un cuento digital interactivo para comunicar el proposito de esta investigacion a nueve ninos entre 3 y 4 anos de edad. Durante un periodo de 11 a 16 semanas y en tres ocasiones se utilizo un cuento que incluia informacion sobre la investigacion para recolectar las descripciones y explicaciones de los ninos sobre su participacion en la investigacion. Entrevistas semi-estructuradas con los padres y conversaciones grabadas en video y audio con los ninos tambien se utilizaron para analizar la informacion y asi comprender la forma en que los ninos percibian su papel como participantes en dicha investigacion. Las respuestas de padres y ninos permitieron identificar seis aspectos de participacion significativa en la investigacion mediante analisis cruzado: participacion, experiencia, autenticidad, consecuencia, posesion e identidad. Esta investigacion contribuye al conocimiento actual acerca del derecho de ninos pequenos a comprender su papel participativo en investigaciones, y tambien acerca de las consideraciones que pueden ponerse en practica para garantizar que la participacion sea significativa para los ninos a nivel personal.","10.1007/s13158-019-00255-2","20342","0.2878924"
"312","hot_topic_cluster_no_1","rank_3","60","Are Canadian professors teaching the skills and knowledge students need to prevent plagiarism?","Max 150 words. If possible, please submit your abstract in both English and French. When writing an assignment, most students start by searching for information online, which they integrate in their writing and conclude by producing a bibliography for the sources used. They use their informational, writing and referencing skills to do this as well as refer to their plagiarism knowledge to make sure their text is exempt from plagiarism. In this paper, we examined which skills and knowledge students feel the need to further develop in university to prevent plagiarism in their assignments. Professors were also questioned as to their perceptions of their students' skills development during their pre-university studies. Questionnaires were administered in six Quebec Universities to students (n = 1170) and professors (n = 279). Results show that students feel the need for more training while professors expect students to have already mastered the skills and knowledge to prevent plagiarism. Recommendations are made on how to implement better training for students through a program approach. Lors de la redaction d'un devoir, la plupart des etudiants universitaires commencent par chercher des informations en ligne, qu'ils integrent dans leur redaction et terminent en produisant une bibliographie des sources utilisees. Ils utilisent leurs competences informationnelles, redactionnelles, et de referencement documentaire et se referent a leurs connaissances en matiere de plagiat pour s'assurer que leur texte en soit exempt. Dans cet article, nous avons examine les competences et les connaissances que les etudiants ressentent le besoin de developper davantage a l'universite pour prevenir le plagiat dans leurs travaux. Les professeurs ont egalement ete interroges sur leur perception du developpement des competences de leurs etudiants durant leurs etudes pre-universitaires. Des questionnaires ont ete administres dans six universites quebecoises a des etudiants (n = 1170) et a des professeurs (n = 279). Les resultats montrent que les etudiants ressentent le besoin d'une formation plus poussee alors que les professeurs s'attendent a ce que les etudiants maitrisent deja les competences et les connaissances necessaires pour prevenir le plagiat. Des recommandations sont formulees sur la facon de mettre en oe uvre une meilleure formation pour les etudiants par le biais d'une approche-programme.","10.1007/s40979-019-0047-z","19638","0.2974790"
"313","hot_topic_cluster_no_1","rank_3","60","Spatiotemporal patterns of baseflow metrics for basins draining the Oak Ridges Moraine, southern Ontario, Canada","The Oak Ridges Moraine (ORM) is a key hydrogeologic feature in southern Ontario, supplying potable water along with important aquatic ecosystem services. Stream baseflow is a useful indicator of groundwater discharge to streams on the ORM and gives insight into the regional hydrogeologic system. This study examines how climatic and basin attributes explain spatiotemporal patterns in baseflow metrics for streams draining the ORM. Baseflow was separated from streamflow using an automated digital filter, and water-year baseflow and baseflow as a fraction of precipitation and total streamflow (baseflow index, BFI) were determined. Much of the variability in baseflow metrics was explained by such characteristics as water-year precipitation, mean basin slope, spatial extent of outcrops of permeable sands and gravels, and forest and urban cover. Identified thresholds for basins with above- or below-average baseflow and BFI could serve as a screening tool in resource management decisions on the ORM. Precipitation has increased for many basins in the latter part of the twentieth and early part of the twenty-first century, often accompanied by greater baseflow. The rate of baseflow increase was related to the spatial extent of sand and gravel outcrops, implying a nonuniform response of baseflow to rising precipitation for basins across the ORM. Although baseflow from these basins may continue to increase under the wetter climate predicted for southern Ontario in the twenty-first century, such forecasts must consider the role of land-use change (e.g. forest expansion, urban development) in altering the partitioning of precipitation between direct runoff and baseflow contributions to streamflow. La moraine Oak Ridges (ORM) est une caracteristique hydrogeologique importante dans le sud de l'Ontario fournissant en eau potable et delivrant de nombreux services pour l'ecosysteme aquatique. Le debit de base des cours d'eau est un indicateur important de l'ecoulement des eaux souterraines dans l'ORM et donne un apercu du systeme hydrogeologique regional. Cette etude examine l'influence des caracteristiques climatiques et les proprietes du bassin versant sur les variations spatio-temporelles des debits de base des cours d'eau drainants l'ORM. Le debit de base a ete separe du debit d'eau a l'aide d'un filtre numerique automatise alors que le debit de base eau annuel, le debit de base sous forme de precipitation et le debit total (debit de base indice, BFI) ont ete determines experimentalement. Une grande partie de la variabilite des mesures de debit de base s'expliquait par les precipitations annuelles, la pente moyenne du bassin, l'etendue spatiale des affleurements de couches permeables de sable et gravier, et du couvert forestier et urbain. Les seuils identifies pour les bassins avec debit de base superieur ou inferieur au debit de base moyen et BFI pourraient servir d'outil de depistage dans les decisions de gestion des ressources pour l'ORM. Au cours des dernieres decennies, les precipitations ont augmente dans de nombreux bassins versants et elles ont souvent ete accompagnees par de grands debits de base. Les taux d'augmentation du debit de base etaient associes a l'etendue spatiale des affleurements de sable et de gravier, impliquant une reponse non uniforme du debit de base vers la hausse des precipitations pour les bassins a travers l'ORM. Bien que le debit de base de ces bassins pourrait continuer a augmenter avec un climat plus humide comme predit pour le sud de l'Ontario au vingt-et-unieme siecle, les previsions de debit de base devraient considerer le role du changement d'affectation des terres telle que l'expansion des forets et le developpement urbain, et modifier le cloisonnement des precipitations entre ruissellement direct et contribution du debit de base au debit d'eau.","10.1080/07011784.2014.985511","67048","0.3072797"
"314","hot_topic_cluster_no_1","rank_3","60","The Importance of Context: Assessing the Benefits and Limitations of Participatory Mapping for Empowering Indigenous Communities in the Comarca Ngabe-Bugle, Panama","Indigenous communities have been involved in participatory mapping projects to protect their territories and manage their resources for decades. However, while tremendous advances have been achieved in many settings, the use of maps by indigenous peoples is very uneven. Here we present the case of a team of university researchers, indigenous students, and local investigators who used a participatory approach to map cultural landscapes and mature forest cover in the Comarca Ngabe-Bugle of Panama. This article examines the success and limitations of efforts to empower indigenous people in the region to use mapping tools for conservation and resource management. The project, while it provides a useful example of how to build a participatory research team to produce maps that better reflect indigenous points of view, fell short of empowering indigenous authorities to use geographic tools to manage their territories. This is due mainly to the lack of administrative capacity needed to make use of geospatial information. We argue that cartographers involved in participatory projects, while typically attentive to the problems of marginalization, need to pay more attention to the broader socioeconomic contexts of their work and to redouble their efforts to respond to the challenges of the digital divide, which is a symptom of broader socioeconomic and political inequalities stemming from the legacies of colonialism. Depuis des decennies, les communautes autochtones prennent part a des projets de cartographie participative en vue de proteger leurs territoires et de gerer leurs ressources. Toutefois, bien que d'enormes progres aient ete realises dans de nombreux cas, l'utilisation des cartes par les peuples autochtones est tres inegale. Les auteurs exposent le cas d'une equipe de chercheurs universitaires, d'etudiants autochtones et d'investigateurs regionaux qui ont eu recours a une approche participative pour cartographier les paysages culturels et le couvert forestier d'arbres adultes dans la comarque Ngabe-Bugle du Panama. Les auteurs s'interessent au succes et aux limites des efforts visant l'autonomisation des peuples autochtones de la region dans l'utilisation des outils cartographiques pour la conservation et la gestion des ressources. Le projet, bien qu'il s'agisse d'un exemple utile de la facon de batir une equipe de recherche participative pour produire des cartes refletant mieux les points de vue autochtones, n'a pas permis d'autonomiser les autorites autochtones dans l'emploi des outils geographiques en vue de gerer leurs territoires, echec principalement attribuable a l'absence des ressources administratives necessaires a l'utilisation de l'information geospatiale. De l'avis des auteurs, les cartographes qui prennent part a des projets participatifs, bien qu'ils soient generalement attentifs aux problemes de marginalisation, doivent, dans leur travail, accorder davantage d'attention aux contextes socioeconomiques elargis et redoubler d'efforts pour relever les defis du fosse numerique, symptomatique des inegalites socioeconomiques et politiques plus etendues imputables a l'heritage du colonialisme.","10.3138/cart.52.1.3574","48095","0.3245614"
"315","hot_topic_cluster_no_1","rank_3","60","Nonpoint-Source Pollution Reduction for an Iowa Watershed: An Application of Evolutionary Algorithms","Nous avons applique un modele integre de simulation-optimisation pour trouver des combinaisons de pratiques agricoles de conservation efficaces en terme de couts et les endroits ou elles devraient etre adoptees dans un bassin versant agricole typique pour deux types de cibles de reduction des charges d'azote : la surveillance des charges moyennes annuelles d'azote et une contrainte du type ' securite d'abord ', selon laquelle les cibles d'azote doivent etre respectees peu importe les conditions meteorologiques (solutions robustes aux changements meteorologiques). Nous avons elabore des algorithmes evolutifs pour chaque cible relative a la qualite de l'eau. Notre approche a permis de deriver la courbe de cout lie a la reduction marginale et totale de l'azote dans un bassin versant. L'ajustement de la probabilite que les cibles relatives a la qualite de l'eau soient respectees (solutions robustes aux changements meteorologiques) s'est revele significativement plus couteux que de controler les charges moyennes d'azote. Nous avons evalue les deux types de solutions afin de verifier leur performance dans des conditions meteorologiques incertaines : les solutions retenues pour reduire les charges moyennes fonctionnent bien dans des conditions meteorologiques incertaines, tandis que la performance des solutions robustes aux conditions meteorologiques diminue avec le niveau vise de qualite de l'eau.","10.1111/j.1744-7976.2010.01198.x","81436","0.4314286"
"316","hot_topic_cluster_no_1","rank_4","617","Subjective Well-Being of Chinese Sina Weibo Users in Residential Lockdown During the COVID-19 Pandemic: Machine Learning Analysis","Background: During the COVID-19 pandemic, residential lockdowns were implemented in numerous cities in China to contain the rapid spread of the disease. Although these stringent regulations effectively slowed the spread of COVID-19, they may have posed challenges to the well-being of residents. Objective: This study aims to explore the effects of residential lockdown on the subjective well-being (SWB) of individuals in China during the COVID-19 pandemic. Methods: The sample consisted of 1790 Sina Weibo users who were residents of cities that imposed residential lockdowns, of which 1310 users (73.18%) were female, and 3580 users who were residents of cities that were not locked down (gender-matched with the 1790 lockdown residents). In both the lockdown and nonlockdown groups, we calculated SWB indicators during the 2 weeks before and after the enforcement date of the residential lockdown using individuals' original posts on Sina Weibo. SWB was calculated via online ecological recognition, which is based on established machine learning predictive models. Results: The interactions of time (before the residential lockdown or after the residential lockdown) x area (lockdown or nonlockdown) in the integral analysis (N=5370) showed that after the residential lockdown, compared with the nonlockdown group, the lockdown group scored lower in some negative SWB indicators, including somatization (F-1,(5368) =13.593, P<. 001) and paranoid ideation (F-1,(5368) =14.333, P<.001). The interactions of time (before the residential lockdown or after the residential lockdown) x area (developed or underdeveloped) in the comparison of residential lockdown areas with different levels of economic development (N=1790) indicated that the SWB of residents in underdeveloped areas showed no significant change after the residential lockdown (P>.05), while that of residents in developed areas changed. Conclusions: These findings increase our understanding of the psychological impact and cost of residential lockdown during an epidemic. The more negative changes in the SWB of residents in developed areas imply a greater need for psychological intervention under residential lockdown in such areas.","10.2196/24775","   43","0.2097015"
"317","hot_topic_cluster_no_1","rank_4","617","Building a Successful Infection Prevention Program: Key Components, Processes, and Economics","Infection control is the discipline responsible for preventing nosocomial infections. There has been an increasing focus on prevention rather than control of hospital-acquired infections. Individuals working in infection control have seen their titles change from infection control practitioner to infection control professional and most recently to infection preventionist (IP), emphasizing their critical role in protecting patients. The responsibilities of IPs span multiple disciplines including medicine, surgery, nursing, occupational health, microbiology, pharmacy, sterilization and disinfection, emergency medicine, and information technology. This article discusses the structure and responsibilities of an infection control program and the regulatory pressures and opportunities the program faces.","10.1016/j.idc.2010.11.007","80482","0.2280702"
"318","hot_topic_cluster_no_1","rank_4","617","Factors associated with the use and reuse of face masks among Brazilian individuals during the COVID-19 pandemic","Objective: to identify the factors associated with the use and reuse of masks among Brazilian individuals in the context of the COVID-19 pandemic. Method: cross-sectional study conducted in the five Brazilian regions, among adult individuals, via an electronic form disseminated in social media, addressing general information and the use of masks. Bivariate analysis and binary logistic regression were used to identify the factors associated with the use and reuse of masks. Results: 3,981 (100%) individuals participated in the study. In total, 95.5% (CI 95%: 94.8-96.1) reported using masks. Fabric masks were more frequently reported (72.7%; CI 95%: 71.3-74.1), followed by surgical masks (27.8%; CI 95%: 26.5-29.2). The percentage of reuse was 71.1% (CI 95%: 69.7-72.5). Most (55.8%; CI 95%: 51.7-60.0) of those exclusively wearing surgical masks reported its reuse. Being a woman and having had contact with individuals presenting respiratory symptoms increased the likelihood of wearing masks (p=0.001). Additionally, being a woman decreased the likelihood of reusing surgical masks (p=0.001). Conclusion: virtually all the participants reported the use of masks, most frequently fabric masks. The findings draw attention to a risky practice, that of reusing surgical and paper masks. Therefore, guidelines, public policies, and educational strategies are needed to promote the correct use of masks to control and prevent COVID-19.","10.1590/1518-8345.4604.3360","16947","0.2500000"
"319","hot_topic_cluster_no_1","rank_4","617","Zika in Puerto Rico, 2016-2017: II Perspectives on Epidemic Surveillance and Control, Health Communication, Outcomes and Lessons","The social reaction to the Zika epidemic in Puerto Rico reached a confrontational climax regarding aerial fumigation with an organophosphate insecticide. The public drama has obscured multiple simultaneous controversies. This and a companion paper, based mostly on print and digital news reports, provide a context and description of the major controversies and examine the outcomes and their lessons for the protection of the public's health. Part II covers the questions on disease surveillance (what is going on?); health communication and epidemic control (what is an epidemic? is there a way to control an epidemic transmitted by Aedes aegypti?), and the outcomes and lessons from the debates.","","40332","0.2666667"
"320","hot_topic_cluster_no_1","rank_4","617","SEIR modeling of the COVID-19 and its dynamics","In this paper, a SEIR epidemic model for the COVID-19 is built according to some general control strategies, such as hospital, quarantine and external input. Based on the data of Hubei province, the particle swarm optimization (PSO) algorithm is applied to estimate the parameters of the system. We found that the parameters of the proposed SEIR model are different for different scenarios. Then, the model is employed to show the evolution of the epidemic in Hubei province, which shows that it can be used to forecast COVID-19 epidemic situation. Moreover, by introducing the seasonality and stochastic infection the parameters, nonlinear dynamics including chaos are found in the system. Finally, we discussed the control strategies of the COVID-19 based on the structure and parameters of the proposed model.","10.1007/s11071-020-05743-y"," 9371","0.2926829"
"321","hot_topic_cluster_no_1","rank_5","641","Human rights and the city: Including marginalized communities in urban development and smart cities","The idea that the city belongs to all individuals inhabiting the urban space is grounded in the Universal Declaration of Human Rights and the New Urban Agenda, and it is referred to as ""right to the city"" or ""rights in the city."" This article discusses how human rights relate to the city and its inhabitants, examines the meaning of the right to the city and human rights in the city in today's urban environment, and deliberates how to transform cities into spaces that reflect fundamental human rights principles. By looking at the situation of marginalized groups in cities, the article focuses on the questions of how to build inclusive, fair, and accessible cities and how to eliminate inequalities seen in urban communities. Because technology is often cited as one way to foster integration of marginalized communities, special attention will be given to the smart city and the opportunities and challenges presented by information and communication technologies (ICTs) for human rights, accessibility, and inclusion. Using the case of persons with disabilities as an illustration, the article argues that urban development needs to be fundamentally transformed to live up to human rights standards. Only a multi-stakeholder urban design process will produce truly inclusive urban spaces that fulfill the right to the city.","10.1080/14754835.2019.1629887","24188","0.2132530"
"322","hot_topic_cluster_no_1","rank_5","641","Knowledge-based environments in the city: design and urban form in the Helsinki metropolitan area","Knowledge-based development (KBD) is of interest to cities, as it promotes economic growth and boosts dynamic urban image. Creative environments are arguably highly dependent on sociality, and they are usually located near city centres. The focus of this study is on three knowledge-based locations in the Helsinki metropolitan area (HMA) in Finland. The research material composes of field observation data and photographs. Results indicate that knowledge-intensive environments include several elements of good city design, for example, pedestrian-friendly street networks and green areas, but they tended to lack vibrant urban life, that is, human activity. The findings support the claim that information and communication technology (ICT)-based companies are not very dependent on the sociality of environment. As cities are spaces for individuals working, studying and visiting, attention must be paid to the urban design of these areas.","10.1504/IJKBD.2019.101004","30233","0.2177419"
"323","hot_topic_cluster_no_1","rank_5","641","Analyzing Urban Competitiveness Changes in Major Chinese Cities 1995-2008","In this study, a city's competitiveness is measured by economic, social, environmental and external connectivity components using a sustainable development perspective. The study focuses on the dynamic changes of urban competitiveness of 24 major Chinese cities in the period 1995-2008. A total of 59 indicators are used to measure urban competitiveness consistently in this period. It is found that the overall ranks of the top and bottom cities in terms of economic, social, environmental and external connection competitiveness in 2008 were largely consistent with a few exceptions. In the period 1995 to 2008, the urban competitiveness score was increased by over 98 % in all cities. Although the weak cities have enhanced their urban competitiveness scores faster than the strong cities, it will take a long time for them to raise their ranks among Chinese cities due to their low scores of initial urban competitiveness. Excepting for 7 cities with significant changes in the rank of urban competitiveness, the ranks of other cities changed only slightly, ranging from -3 to +3, showing stable hierarchy among 24 major Chinese cities in the period 1995-2008.","10.1007/s12061-014-9114-2","68495","0.2207792"
"324","hot_topic_cluster_no_1","rank_5","641","MODERN URBAN PLANNING: CONCEPTS TO PRACTICE","Modern urban planning has a large number of problems associated with the size of cities and the environmental situation in them. The largest cities continue to expand and develop through some administrative processes. These processes can be rather specific and diverse. Almost every city has its historical center and its industrial suburbs, as well as a residential block-of-flats area surrounding the industrial suburbs. As a result, the transport infrastructure is naturally expanding, thus causing damage to the environment and people's health. To solve this problem, modern urban planning methods should be developed and implemented taking into consideration modern concepts of urban planning. Conventionally, they can be divided into several blocks, such as curbing the influx of new urban residents from rural areas, applying smart city technologies in urban planning, creating zones for restoring people's health in the city, and modernizing the system of administrative and territorial infrastructure.","","12888","0.2300000"
"325","hot_topic_cluster_no_1","rank_5","641","Redefining the digital city for promoting sustainable urban development","In the current information age, digital technology has become an essential part of urban civilisation. The digital city has been transformed from a novel concept to a practical and effective means of supporting urban planning and management. However, there are various definitions of a digital city and each has a unique significance. By comparing these digital city concepts, we examined common aspects of digital city definitions and propose an urban digital operating system (Urban DOS) that will be useful to improve life quality, socioeconomic functions and sustainable development in a city and its surrounding areas. The technical basis for the Urban DOS is the intersection between technology-oriented products (TOPs) and customised application products (CAPs). We then develop a procedure for designing a framework for a digital city based on Urban DOS with TOPs and CAPs. To explain such digital city concepts and applications, we demonstrate the initial development of Urban DOS for Lijiang City.","10.1080/13504509.2011.603762","81216","0.2632353"
"326","hot_topic_cluster_no_10","rank_1","843","Planning of Electric Public Transport System under Battery Swap Mode","Applying battery electric buses (BEBs) in the city is a good means to reduce the increasing greenhouse gas emissions and crude oil dependence. Limited by the driving range and charging time, battery swap station seems to be the best option for battery electric buses to replenish energy currently. This paper presents a novel method to plan and design an electric public transport system under battery swap mode, which comprised of battery electric buses, routes, scheduling, battery swap station, etc. Thus, new routing and scheduling strategies are proposed for the battery electric bus fleets. Based on swapping and charging demand analysis, this paper establishes an algorithm to calculate the optimal scales of battery swap station, including scales of battery swapping system, battery charging system and battery packs, and power capacity of output. Regarding the case of Xuejiadao battery swap station serving 6 BEB routes in Qingdao, China, a numerical simulation program is established to evaluate the validity of our methods. The results reflect that our methods can optimize the system scales meeting an equivalent state of operation demand. In addition, sensitivity analyses are made to the scales under different values of battery capacity and charging current. It suggests that the scales and cost of battery swap station can be effectively reduced with the development of power battery manufacture and charging technology in future.","10.3390/su10072528","36566","0.2814815"
"327","hot_topic_cluster_no_10","rank_1","843","Dynamic harvesting- and energy-aware real-time task scheduling","Energy harvesting, along with effective storage of the energy, is a very common approach to attain sustainable computing in today's embedded systems. Employing a hybrid energy storage system (HESS), which constitutes of two or more types of energy storage systems (ESSs), helps to compensate for the weaknesses of one ESS type using the strengths of another type. The capacity of an ESS, and thus that of a HESS, can be modeled by dividing it into Instantly Available Charge (IAC) and Instantly Unavailable Charge (IUC) parts; the existing charge in an ESS always flows from the part with higher voltage to the other one. The main idea of this study is to intelligently control the flows in the HESS to maximizing either the IAC or the IUC charge. We propose the HLPF real-time task scheduling algorithm to do so through deciding to execute the tasks in the ascending or descending order of their power requirements. Extensive simulations show impressive lifetime improvements of up to 20 % in comparison to the classical real-time task scheduling algorithms. ","10.1016/j.suscom.2020.100413","  585","0.2907895"
"328","hot_topic_cluster_no_10","rank_1","843","Balanced Control Strategies for Interconnected Heterogeneous Battery Systems","This paper develops new balanced charge/discharge strategies that distribute charge or discharge currents properly so that during operation, battery systems maintain uniform state-of-charge (SOC) all the time. The proposed balanced charge/discharge control strategies are useful for interconnected heterogeneous battery systems that can be built from battery modules with different types, ages, and power/capacity ratings. Both voltage-based and SOC-based balanced charge/discharge strategies are developed. Their convergence properties are rigorously established, and illustrative examples using production batteries demonstrate their convergence behavior under different charging current profiles. The approach will be especially useful for battery storage systems to support power grids with renewable energy sources where the battery systems are required to operate continuously.","10.1109/TSTE.2015.2487223","62152","0.2910714"
"329","hot_topic_cluster_no_10","rank_1","843","Optimal sizing of a wind-energy storage system considering battery life","A battery energy storage system (BESS) can smooth the fluctuation of output power for micro-grid by eliminating negative characteristics of uncertainty and intermittent for renewable energy for power generation, especially for wind power. By integrated with lithium battery storage system the utilization and overall energy efficiency can be improved. However, this target could be obtained only if the BESS is optimal matched. For this issue, the degradation of battery capacity has a significant impact on the operating costs of Wind-ESS system. The research focus on the optimal method for components sizing of BESS in Wind-ESS system with independent system operators. We present an operating cost model for the hybrid energy storage system considering capacity fading of lithium battery in the cycle life. For the optimal objective of component sizing, the global optimization method of dynamic programming (DP) is adopted by setting operating costs and capacity degradation as optimal objectives under the constrains of performance for lithium battery and requirement for grid operation. Based on the DP algorithm and capacity degradation of battery model, the optimal output of the wind power is obtained. The rule based method and genetic algorithm are also be used for simulation. The simulation results show that compared with other two optimal approaches, capacity degradation and operation cost of energy storage for wind power generation system are significantly reduced. ","10.1016/j.renene.2019.09.123","13797","0.2971963"
"330","hot_topic_cluster_no_10","rank_1","843","Optimal Scheduling of Energy Resources and Management of Loads in Isolated/Islanded Microgrids","Plug-in electric vehicles (PEVs) present a promising solution to mitigate greenhouse gas emissions but on the other hand, their increased penetration can impact power system operation, particularly so in an isolated microgrid. Similarly, demand response (DR) has the potential to provide significant flexibility in the operation of an isolated microgrid with limited generation capacity, by altering the demand and introducing an elasticity effect. This paper proposes a new mathematical model for optimal scheduling of energy resources and smart management of loads which includes smart charging of PEVs, DR, and operation of battery energy storage systems (BESSs), for isolated microgrids. Different case studies are developed to examine isolated microgrid operations when the demand increases, and how the energy management model copes with such increase. The proposed model develops energy management strategies considering the network constraints and different objective functions from the perspective of the microgrid operator as well as from the owners of PEVs and BESS.","10.1109/CJECE.2017.2753219","35523","0.3217391"
"331","hot_topic_cluster_no_10","rank_2","449","Stochastic optimized intelligent controller for smart energy efficient buildings","Smart buildings are rising concept for energy resource management in the buildings to reduce energy consumption and wastage. Energy management and control system comprises of variety of technologies, which affect human working comfort. The challenging task of the building control is to achieve interior building environment comfort with high-energy efficiency. In this study, multi-agent control system has been developed in combination with stochastic optimization using genetic algorithm (GA). The corresponding simulations of effective management of energy and consumer comfort are presented. The developed control system provides significant improvement in energy consumption and interior environmental comfort in smart buildings. ","10.1016/j.scs.2014.04.005","69266","0.2938776"
"332","hot_topic_cluster_no_10","rank_2","449","Collaborative optimization between passive design measures and active heating systems for building heating in Qinghai-Tibet plateau of China","Due to cold winter and cool summer in Qinghai-Tibet plateau, heating energy consumption accounts for a large proportion in the building energy consumption and reducing heating energy consumption is one of the main energy-saving methods for buildings. In the process of building heating, passive design measures and active heating systems (AHS) are always working together. To optimize the heating performance, it is necessary to coordinate the relationship between passive design measures and the AHS. However, it is impossible to obtain the specific quantitative optimized relationship between passive design measures and AHS based on existing research methods. To address this lack of knowledge, a collaborative optimization design method (CODM) is proposed in this paper to optimize the cost and the energy consumption of heating in the life cycle of the building. In CODM, comprehensive effects of passive design measures and the AHS on the total cost and the total energy consumption for building heating are analyzed. A railway passenger station is selected as a case study and results show that compared with initial design, the optimal total heating cost and total heating energy consumption for building heating can reduce 1948 (sic)/m(2) and 2292 kW h/m(2) for the life cycle of the building, respectively. ","10.1016/j.renene.2019.09.031","14346","0.3085366"
"333","hot_topic_cluster_no_10","rank_2","449","Systematic approach to provide building occupants with feedback to reduce energy consumption","Many technical solutions have been developed to reduce buildings' energy consumption, but limited efforts have been made to adequately address the role or action of building occupants in this process. Our earlier investigations have shown that occupants play a significant role in buildings' energy consumption: It was shown that savings of up to 20% could be achieved by modifying occupant behavior thorough direct feedback and recommendations. Studying the role of occupants in building energy consumption requires an understanding of the interrelationships between climatic conditions; building characteristics; and building services and operation. This paper describes the development of a systematic procedure to provide building occupants with direct feedback and recommendations to help them take appropriate action to reduce building energy consumption. The procedure is geared toward developing a Reference Building (RB) (an energy-efficient building) for a specific given building. The RB is then compared against its given building to inform the occupants of the given building how they are using end-use loads and how they can improve them. The RB is generated using a data-mining approach, which involves clustering analysis and neural networks. The framework is based on clustering similar buildings by effects unrelated to occupant behavior. The buildings are then grouped based on their energy consumption, and those with lower consumption are combined to generate the RB. Performance evaluation is determined by comparison of a given building with an RB. This comparison provides feedback that can lead occupants to take appropriate measures (e.g., turning off unnecessary lights or heating, ventilation, and air conditioning (HVAC), etc.) to improve building energy performance. More accurate, scalable, and realistic results are achiveable through current methodology which is shown through comparison with existing literature. ","10.1016/j.energy.2019.116813","13910","0.3155172"
"334","hot_topic_cluster_no_10","rank_2","449","Development of a ranking procedure for energy performance evaluation of buildings based on occupant behavior","Identifying the impacts of occupants on building energy consumption has become an important issue in recent years. This is due to the interrelationship of influencing factors such as urban climate, building characteristics, occupant behavior, and building services and operation, which makes it challenging to identify the role of occupants in energy consumption. The research problem in this study lies in the fact that the occupants of a building may not be cautious regarding energy savings, and there exists no ground to assess their energy consumption behavior. One solution is the development of a systematic comparison procedure between similar buildings. This paper introduces a new procedure for comparison between occupants of several buildings to show the rank of each building among others and suggest occupants on reducing their energy consumption and improving their rank. The proposed framework is developed based on multiple data-mining methods, including clustering, association rules mining, and neural networks. The proposed methodology is composed of two levels. The first considers the amount of energy usage by occupants after filtering effects unrelated to the occupant behavior. The second ranks the buildings in terms of achieved and potential savings during the time under investigation. To demonstrate the application, the methodology was applied on a set of monitored residential buildings in Japan. Results suggest that the proposed method enhances the evaluation of buildings' energy-saving potential by revealing the occupants' contribution. It also provides diverse and prioritized strategies to help occupants manage their energy consumption by revealing the building energy end-use patterns. ","10.1016/j.enbuild.2018.11.050","29947","0.3281250"
"335","hot_topic_cluster_no_10","rank_2","449","Energy Analyses of Serbian Buildings with Horizontal Overhangs: A Case Study","It is well known that nowadays a significant part of the total energy consumption is related to buildings, so research for improving building energy efficiency is very important. This paper presents our investigations about the dimensioning of horizontal overhangs in order to determine the minimum annual consumption of building primary energy for heating, cooling and lighting. In this investigation, embodied energy for horizontal roof overhangs was taken into account. The annual simulation was carried out for a residential building located in the city of Belgrade (Serbia). Horizontal overhangs (roof and balcony) are positioned to provide shading of all exterior of the building. The building is simulated in the EnergyPlus software environment. The optimization of the overhang size was performed by using the Hooke Jeeves algorithm and plug-in GenOpt program. The objective function minimizes the annual consumption of primary energy for heating, cooling and lighting of the building and energy spent to build overhangs. The simulation results show that the building with optimally sized roof and balcony overhangs consumed 7.12% lessprimary energy for heating, cooling and lighting, compared to the house without overhangs. A 44.15% reduction in cooling energy consumption is also achieved.","10.3390/en13174577"," 5964","0.3922222"
"336","hot_topic_cluster_no_10","rank_3","552","Reactive Power Control Method for Grid-Tie Inverters Using Current Measurement of DG Output","The penetration ratio of distributed generation (DG) has increased due to the depletion of fossil fuels and the impacts of global warming. Thus, overvoltage at the point of common coupling may occur since reverse current flows with the installment of DG. To solve this problem, this paper presents a reactive power control method based on the measured current at the point of common coupling of DG. The purpose of this paper is not to provide methods to improve the voltage profiles of distribution lines through the reactive power control, but to maintain the voltage after the DG connection as before the connection. By applying the proposed method in this paper, it is possible to significantly improve the penetration ratio of DG by restricting the voltage rise. Case studies, using MATLAB simulation, are performed to verify the feasibility of the new reactive power control method.","10.1007/s42835-019-00104-1","28734","0.2884615"
"337","hot_topic_cluster_no_10","rank_3","552","Differential Power Processing for DC Systems","This paper introduces an approach to dc power delivery that reduces power loss by minimizing redundant energy conversion. Existing power distribution techniques tend to increase the number of cascaded conversion stages, which limits overall efficiency. Differential power processing enables independent load regulation, while processing only a small portion of the total load power. Bulk power conversion occurs once. Load voltage domains are connected in series, and differential converters act as controllable current sources to regulate intermediate nodes. This enables independent, low supply voltages, which can reduce system energy consumption, especially in digital circuits and solid-state lighting. Since differential voltage regulators process a fraction of the load power, decreased size, cost, and conversion losses are attainable. Under balanced load conditions, secondary differential converters do not process any power. This paper analyzes several differential power delivery architectures that can be applied to homogenous and heterogeneous loads at various levels: chip, board, blade, etc. A variety of operating conditions for a test system with four series voltage domains are examined in simulation and verified with experimental hardware. Results in a reference application show a 7-8% decrease in input power and 6-7 percentage points increase in overall conversion efficiency as compared to a conventional cascaded approach.","10.1109/TPEL.2012.2214402","74951","0.3018349"
"338","hot_topic_cluster_no_10","rank_3","552","Network Coordinated Power Point Tracking for Grid-Connected Photovoltaic Systems","Maximum power point tracking (MPPT) achieves maximum power output for a photovoltaic (PV) system under various environmental conditions. It significantly improves the energy efficiency of a specific PV system. However, when an increasing number of PV systems are connected to a distribution grid, MPPT poses several risks to the grid: 1) over-voltage problem, i.e., voltage in the distribution grid exceeds its rating; and 2) reverse power-flow problem, i.e., power that flows into the grid exceeds an allowed level. To solve these problems, power point tracking of all PV systems in the same distributed grid needs to be coordinated via a communication network. Thus, coordinated power point tracking (CPPT) is studied in this paper. First, an optimization problem is formulated to determine the power points of all PV systems, subject to the constraints of voltage, reverse power flow, and fairness. Conditions that obtain the optimal solution are then derived. Second, based on these conditions, a distributed and practical CPPT scheme is developed. It coordinates power points of all PV systems via a communication network, such that: 1) voltage and reverse power flow are maintained at a normal level; and 2) each PV system receives a fair share of surplus power. Third, a wireless mesh network (WMN) is designed to support proper operation of the distributed CPPT scheme. CPPT is evaluated through simulations that consider close interactions between WMN and CPPT. Performance results show that: 1) CPPT significantly outperforms MPPT by gracefully avoiding both overvoltage and reverse power-flow problems; 2) CPPT achieves fair sharing of surplus power among all PV systems; and 3) CPPT can be reliably conducted via a WMN.","10.1109/JSAC.2014.2332120","69938","0.3033113"
"339","hot_topic_cluster_no_10","rank_3","552","An Adjustable DC Link Voltage-Based Control of Multifunctional Grid Interfaced Solar PV System","This paper presents a grid supported solar energy conversion system with an adjustable dc link voltage for common point of interconnection (CP) voltage variations. A two-stage circuit topology is proposed, wherein the first stage is a boost converter, which serves for maximum power point tracking, and the second stage is a grid tied voltage source converter (VSC), which not only feeds extracted solar photovoltaic (PV) energy into the three-phase distribution system but also serves for harmonics mitigation, reactive power compensation, and grid current balancing. An interweaved double-frequency second-order generalized integrator-based control algorithm is proposed for control of this multifunctional VSC, which possesses the feature of good steady-state performance along with fast dynamic response even under sudden load changes at CPI. Moreover, a feed-forward term for the solar PV contribution is used to enhance the dynamic response for climatic changes and CPI voltage variation. An adjustable dc link voltage structure is used to accommodate CPI voltage variation, which helps in reduction of losses in the power circuit. To implement adjustable dc link voltage structure, the reference dc link voltage is adjusted with variation in CPI voltage in real time. A proportional-integral controller is used to regulate dc link voltage to set reference value. A wide range of experimental results are shown to demonstrate the features of the proposed system. The total harmonic distortion of grid current has been found well under IEEE-519 standard even under nonlinear loads at CPI.","10.1109/JESTPE.2016.2627533","46697","0.3141667"
"340","hot_topic_cluster_no_10","rank_3","552","Deterministic Algorithm for Selective Shunt Active Power Compensators According to IEEE Std. 1459","This paper proposes a deterministic algorithm to scale the reference currents of a shunt active power compensator (SAPC) based on IEEE Std. 1459 power decomposition when SAPC maximum output compensating current is going to be exceeded. The selective SAPC is proposed to improve power quality and energy efficiency in power networks by means of the cancelation or reduction of the non-efficient powers (Q(1)(+), S-U1, S-eN). The non-efficient powers can be reduced in six possible sequences according to the priority of compensation. When SAPC maximum output current capacity is exceeded, the proposed algorithm limits the SAPC output compensating currents and the non-efficient currents can only be partially reduced in the power network. The reduction of the non-efficient powers depends on the selected compensation sequence. Experimental results for several compensation sequences demonstrate the appropriate operation of the selective SAPC using the proposed scaling algorithm.","10.3390/en10111791","43526","0.3151899"
"341","hot_topic_cluster_no_10","rank_4","520","Smart grid scenarios and their impact on strategic plan-A case study of Omani power sector","Electrical energy consumption if reduced during peak hours can result in the deferment of generation, transmission and distribution capacity addition. The postponement of capacity addition, or ""avoided cost"" is of promising value to electric utilities who can redirect financial resources for other purposes due to these offset costs. The reduction in energy consumption is achievable through smart grid implementation. Therefore, the utilities need to investigate whether upgrading their grid system to make it smarter is economically justifiable or not. Electricity companies have used the smart grid maturity model to assess their rating/ranking in different domains. The paper provides a framework for establishing future strategies and work plans as they pertain to smart grid implementations. The objective of this paper is to evaluate the demand-side management (DSM), energy efficiency measures and distributed generation benefits of smart grid in Oman. The developed scenarios include grid enhancement, customer contribution to the grid and both of these options simultaneously. The scenarios are analyzed for peak reduction and their benefits are calculated in terms of avoided cost of generation, transmission, distribution, and environmental costs.","10.1016/j.scs.2017.11.015","39882","0.2415584"
"342","hot_topic_cluster_no_10","rank_4","520","Integrated multiperiod power generation and transmission expansion planning with sustainability aspects in a stochastic environment","This paper presents a multistage stochastic programming model to address sustainable power generation and transmission expansion planning. The model incorporates uncertainties about future electricity demand, fuel prices, greenhouse gas emissions, as well as possible disruptions to which the power system is subject. A number of sustainability regulations and policies are considered to establish a framework for the social responsibility of the power system. The proposed model is applied to a real-world case, and several sensitivity analyses are carried out to provide managerial insights into different aspects of the model. The results emphasize the important role played by sustainability policies on the configuration of the power grid. ","10.1016/j.energy.2015.02.047","65111","0.2432432"
"343","hot_topic_cluster_no_10","rank_4","520","Cyber Security and Privacy Issues in Smart Grids","Smart grid is a promising power delivery infrastructure integrated with communication and information technologies. Its bi-directional communication and electricity flow enable both utilities and customers to monitor, predict, and manage energy usage. It also advances energy and environmental sustainability through the integration of vast distributed energy resources. Deploying such a green electric system has enormous and far-reaching economic and social benefits. Nevertheless, increased interconnection and integration also introduce cyber-vulnerabilities into the grid. Failure to address these problems will hinder the modernization of the existing power system. In order to build a reliable smart grid, an overview of relevant cyber security and privacy issues is presented. Based on current literatures, several potential research fields are discussed at the end of this paper.","10.1109/SURV.2011.122111.00145","78867","0.2551724"
"344","hot_topic_cluster_no_10","rank_4","520","Smart Grid Communication: Its Challenges and Opportunities","The necessity to promote smart grid (SG) has been recognized with a strong consensus. The SG integrates electrical grids and communication infrastructures and forms an intelligent electricity network working with all connected components to deliver sustainable electricity supplies. Many advanced communication technologies have been identified for SG applications with a potential to significantly enhance the overall efficiency of power grids. In this paper, the challenges and applications of communication technologies in SG are discussed. In particular, we identify three major challenges to implement SG communication systems, including standards interoperability, cognitive access to unlicensed radio spectra, and cyber security. The issues to implement SG communications on an evolutional path and its future trends are also addressed. The aim of this paper is to offer a comprehensive review of state-of-the-art researches on SG communications.","10.1109/TSG.2012.2225851","75149","0.2631579"
"345","hot_topic_cluster_no_10","rank_4","520","The Role of Energy Storage in Development of Smart Grids","The adoption of Smart Grid devices throughout utility networks will effect tremendous change in grid operations and usage of electricity over the next two decades. The changes in ways to control loads, coupled with increased penetration of renewable energy sources, offer a new set of challenges in balancing consumption and generation. Increased deployment of energy storage devices in the distribution grid will help make this process happen more effectively and improve system performance. This paper addresses the new types of storage being utilized for grid support and the ways they are integrated into the grid.","10.1109/JPROC.2011.2116752","80091","0.2641026"
"346","hot_topic_cluster_no_10","rank_5","462","Adaptive energy-aware scheduling method in a meteorological cloud","This paper focuses on the energy-aware scheduling problem of moldable non-linear parallel tasks in a meteorological cloud. Such a meteorological Cloud mainly provides computing resources for the execution of meteorological models, such as Weather Research and Forecasting model (WRF). In a meteorological Cloud, the parallelism of tasks (i.e., meteorological models) can only be configured in the beginning, and the assigned resources retained exclusively until all sub-tasks have been finished. For the scheduling of those tasks, one key challenge is how to reduce the average energy consumption while guaranteeing others requirements of such tasks. We address this challenge by considering simultaneously the deadlines of tasks, the energy consumption, the system load, and the non-linear speedup of parallel tasks when we make the scheduling decision. Specifically, we propose an adaptive energy-aware scheduling method called ASSD, that is based on the Dynamic Voltage and Frequency Scaling (DVFS) model of computing resources and the speedup of tasks under different parallelisms. We evaluate our method via simulations on a meteorological cloud. Our results show that the proposed method not only increases the number of completed tasks but also significantly reduces the average energy consumption. ","10.1016/j.future.2019.07.061","19491","0.2376623"
"347","hot_topic_cluster_no_10","rank_5","462","Energy-Efficient Task Execution for Application as a General Topology in Mobile Cloud Computing","Mobile cloud computing has been proposed as an effective solution to augment the capabilities of resource-poor mobile devices. In this paper, we investigate energy-efficient collaborative task execution to reduce the energy consumption on mobile devices. We model a mobile application as a general topology, consisting of a set of fine-grained tasks. Each task within the application can be either executed on the mobile device or on the cloud. We aim to find out the execution decision for each task to minimize the energy consumption on the mobile device while meeting a delay deadline. We formulate the collaborative task execution as a delay-constrained workflow scheduling problem. We leverage the partial critical path analysis for the workflow scheduling; for each path, we schedule the tasks using two algorithms based on different cases. For the special case without execution restriction, we adopt one-climb policy to obtain the solution. For the general case where there are some tasks that must be executed either on the mobile device or on the cloud, we adopt Lagrange Relaxation based Aggregated Cost (LARAC) algorithm to obtain the solution. We show by simulation that the collaborative task execution is more energy-efficient than local execution and remote execution.","10.1109/TCC.2015.2511727","36534","0.2385542"
"348","hot_topic_cluster_no_10","rank_5","462","A collaborative cloud-edge computing framework in distributed neural network","The emergence of edge computing provides a new solution to big data processing in the Internet of Things (IoT) environment. By combining edge computing with deep neural network, it can make better use of the advantages of multi-layer architecture of the network. However, the current task offloading and scheduling frameworks for edge computing are not well applicable to neural network training tasks. In this paper, we propose a task model offloading algorithm by considering how to optimally deploy neural network model into the edge nodes. An adaptive task scheduling algorithm is also designed to adaptively optimize the task assignment by using the improved ant colony algorithm. Based on them, a collaborative cloud-edge computing framework is proposed, which can be used in the distributed neural network. Moreover, this framework sets up some mechanisms so that the cloud can collaborate with edge computing in the work. The simulation results show that the framework can reduce time delay and energy consumption, and improve task accuracy.","10.1186/s13638-020-01794-2"," 3168","0.2409836"
"349","hot_topic_cluster_no_10","rank_5","462","Dynamic voltage scaling of mixed task sets in priority-driven systems","This paper describes dynamic voltage scaling (DVS) algorithms for real-time systems with both periodic and aperiodic tasks. Although many DVS algorithms have been developed for real-time systems with periodic tasks, none of them can be used for a system with both periodic and aperiodic tasks because of the arbitrary temporal behaviors of aperiodic tasks. This paper proposes off-line and on-line DVS algorithms that are based on existing DVS algorithms. The proposed algorithms utilize the execution behaviors of scheduling servers for aperiodic tasks. Since there is a tradeoff between the energy consumption and the response time of aperiodic tasks, the proposed algorithms focus on bounding the response time degradation of aperiodic tasks although they delay the response time by stretching the task execution to get high energy savings in mixed task sets. Experimental results show that the proposed algorithms reduce the energy consumption by 48% and 35% over the non-DVS scheme under rate monotonic (RM) scheduling and earliest deadline first (EDF) scheduling, respectively.","10.1109/TCAD.2005.853706","87901","0.2500000"
"350","hot_topic_cluster_no_10","rank_5","462","Energy-aware dynamic task offloading and collective task execution in mobile cloud computing","There is a good opportunity for enlightening the services of the mobile devices by introducing computational offloading using cloud technology. Offloading is a process for managing the complexity of the mobile environment by migrating computational load to the cloud. The mobile devices oblige the quick response for the offloading requests; it is dependent on network connectivity. The cloud services take long set-up time irrespective of network connectivity. In this paper, new system architecture for the dynamic task offloading in the mobile cloud environment is proposed. The architecture includes the offloading algorithm that concentrates on energy consumption of the tasks both in the local and remote environment. The proposed algorithm formulates a collective task execution model for minimizing the energy consumption. The architecture concentrates on the network model by considering the task completion time in three different network scenarios. The experimental results show the efficiency of the suggested architecture in reducing the energy consumption and completion time of the tasks.","10.1002/dac.3914"," 5111","0.2698630"
"351","hot_topic_cluster_no_11","rank_1","494","Context specific adaptation grammars for climate adaptation in urban areas","In the context of climate adaptation planning there are relationships between adaptation drivers and adaptation measures, which makes the selection and implementation of the adaptation measures a challenging task. This challenge may be addressed by: structuring the adaptation problem using a multiple perspective adaptation framework; and applying a context specific precedence grammar logic for selecting and evaluating adaptation measures. Precedence grammar logic is a set of rule based algorithms (grammar) that are based on the relationships in a local adaptation context. This paper demonstrates the application of a context specific precedence grammar logic in an adaptation context in Can Tho, Vietnam. Adaptation pathways comprising flood adaptation measures (i.e. dike heightening) for this case were generated using rule based algorithms based on the relationships between the drivers and the adaptation measures. The results show that complex adaptation issues that are structured, can be resolved using a context specific adaptation grammar approach. ","10.1016/j.envsoft.2017.12.016","38866","0.2083333"
"352","hot_topic_cluster_no_11","rank_1","494","Slum upgrading and climate change adaptation and mitigation: Lessons from Latin America","Residents of informal settlements are some of the most vulnerable urban groups to the effects of climate change. Different responses have emerged to intervene in these communities and slum upgrading is currently considered the best framework. How slum upgrading interventions engage with climate change adaptation and mitigation is not fully explored in the literature. This article scrutinizes three recent slum upgrading programs in Latin America to uncover the ways these tend to engage or not with climate change adaptation and mitigation. The analysis revealed that slum upgrading is both a policy mechanism to address socio-economic issues and an instrument by which built-environment interventions can enhance adaptation and mitigation in informal settlements. Six key areas emerged from the analysis of the case studies that link slum upgrading to climate change adaptation and mitigation through infrastructure and policies: (1) security of tenure, (2) relocation and barriers, (3) public space, (4) energy-efficient and improved architecture, (5) connectivity, and (6) land management. Slum upgrading can be an effective mechanism that combines mitigation and adaptation efforts whilst addressing sustainable development priorities in disadvantaged territories.","10.1016/j.cities.2020.102791"," 6380","0.2087500"
"353","hot_topic_cluster_no_11","rank_1","494","Comparison between meteorological data and farmer perceptions of climate change and vulnerability in relation to adaptation","How farmers perceive climate change has an influence on how they adapt to climate change. Climate change perception and vulnerability were assessed based on the household survey information collected from randomly selected 118 farmers of Kalapara subdistrict in Bangladesh. This paper identified the socio-economic covariates of climate change perception and vulnerability in relation to agricultural adaptation. It was also determined whether their perception was consistent with meteorological information. Findings revealed that the farmers had a moderate level of perception of and vulnerability to climate change. An overwhelming majority (98%) of the respondents perceived a wanner summer and 96% of them observed a colder winter compared to the past. Among the fanners, 91% believed that rainfall had increased and 97% thought that the timing of rainfall had changed. The belief of increase in soil salinity and associated loss was prevailing among 98 and 99% of them, respectively. Observed climate data were mostly aligned with the fanners' perception with respect to temperature, rainfall, floods, droughts and salinity. Positive correlations were found among the perception of climate change, the perception of vulnerability and the number of adopted adaptation practices. Farmers' level of understanding of climate change, vulnerability and adaptation practices could be improved by involving them in different organizations, such as climate field school and farmer associations. It could accelerate the dissemination of agricultural adaptation practices among them to cope with adverse agricultural impacts of climate change.","10.1016/j.jenvman.2019.02.028","26921","0.2210000"
"354","hot_topic_cluster_no_11","rank_1","494","Implications of climate change for semi-arid dualistic agriculture: a case study in Central Chile","The nexus between climate change, agriculture, and poverty has become a major topic of concern, especially for dry regions, which represent a large share of the world's population and ecosystems vulnerable to climate change. In spite of this, to date, few studies have examined the impacts of climate change on agriculture and the adaptation strategies of vulnerable farmers from emerging semi-arid regions with dualist agriculture, in which subsistence farms coexist with commercial farms. This study aims to assess the micro-level impact of climate change and the farm household adaptation strategies in a semi-arid region in Central Chile. To this end, we develop a modelling framework that allows for (1) the assessment of farm-household responses to both climate change effects and adaptation policy scenarios and (2) the identification of local capacities and adaptation strategies. Aggregated results indicate that climate change has a substantial economic impact on regional agricultural income, while the micro-level analysis shows that small-scale farm households are the most vulnerable group. We observe that household characteristics determine to a large extent the adaptation capacity, while an unexpected result indicates that off-farm labour emerges as a powerful option for adapting to climate change. As such, our approach is well suited for ex ante micro-level adaptation analysis and can thereby provide useful insights to guide smart climate policy-making.","10.1007/s10113-018-1380-0","31787","0.2216495"
"355","hot_topic_cluster_no_11","rank_1","494","Case for equity between Paris Climate agreement's Co-benefits and adaptation","There are heightened debates on limited opportunity of the global adaptation policy goals of the Paris Climate Agreement (PaCA) to match efforts at mitigation and adaptation. This has been attributed partially to the overstatement in Article 7 Paragraph 4 of the PaCA that ""greater levels of mitigation"" reduces the cost of additional adaptation through mitigation Co-benefits. Therefore, the paper explores how Article 7 of the PaCA partially faults the natural synergy between mitigation and adaptation to equally reduce aggregate emission, although mitigation could help reduce adaptation to physical exposure. Co-benefits are non-climate ancillary benefits from emission reduction that is human-centered. Article 7 of the PaCA overtly favors efforts at mitigation compared to adaptation, yet how much mitigation benefits match adaptation cost including human dimension issues remain speculative and also constrained emission leakages. Thus, the sole attribution of avoiding additional adaptation cost to increased mitigation efforts is far from the reality as adaptation could offset its own additional cost through benefits that reduce emissions, and synonymous to mitigation Co-benefits. For example, the adaptation intentions of ecosystem-based adaptation (Eba), urban NEXUS, integrated water resources management (IWRM) and climate smart agriculture (CSA) in aspects of biodiversity conservation, energy redistribution from human activity, water purification and nutrient recycling are also major sources of emission sink. Therefore, the Article 7 of the PaCA could be enhanced by broadening the definition of Co-benefits to reflect the two-way equity-bound efforts at mitigation and adaptation towards reduced emission leakages and additional adaptation cost. ","10.1016/j.scitotenv.2018.11.333","28184","0.2500000"
"356","hot_topic_cluster_no_11","rank_2","879","Knowledge management technology for organized crime risk assessment","Information and communication technology is applied by law enforcement agencies in intelligence and investigation work. Assessment of risk caused by criminal gangs is an example of risk and technology in policing. This paper applies the knowledge management technology stage model to criminal risk assessment.","10.1007/s10796-009-9178-8","82128","0.2105263"
"357","hot_topic_cluster_no_11","rank_2","879","A concise review of ecological risk assessment for urban ecosystem application associated with rapid urbanization processes","Urban ecological risk (UER) caused by rapid urbanization means potential threat to urban ecosystem structure, pattern and services. The scales of ecological risk assessment (ERA) have been expanded from individual organisms to watersheds and regions. The types of stressor range from chemical to physical, biological and natural events. However, the application of ERA in urban ecosystems is relatively new. Here, we summarize the progress of urban ERA and propose an explicit framework to illumine future ERA based on UER identification, analysis, characterization, modeling, projection and early warning and management. The summary includes six urban ERA-relevant methods: weight-of-evidence (WoE), procedure for ecological tiered assessment of risks (PETAR), relative risk model (RRM), multimedia, multi-pathway, multi-receptor risk assessment (3MRA), landscape analysis and ecological models. Furthermore, we review critical cases of urban ERA in landscape ecology, soil, air, water and solid waste. Based on the Internet of Things (IoT) and cloud computing, an urban ERA management platform integrates various urban ERA methodologies that can be developed to provide better implementation strategies of UER for urban ecosystem managers and stakeholders. We develop a conceptual model of urban ERA based on the urban characteristics in China. The future applications of urban ERA include uncertainty analysis using Monte Carlo techniques on the basis of geospatial techniques and comprehensive urban ERA using nonlinear models or process models.","10.1080/13504509.2016.1225269","46610","0.2198473"
"358","hot_topic_cluster_no_11","rank_2","879","Decomposition Algorithms for Risk-Averse Multistage Stochastic Programs with Application to Water Allocation under Uncertainty","We study a risk-averse approach to multistage stochastic linear programming, where the conditional value-at-risk is incorporated into the objective function as the risk measure. We consider five decompositions of the resulting risk-averse model to solve it via the nested L-shaped method. We introduce separate approximations of the mean and the risk measure and also investigate the effectiveness of multiple cuts. As an application, we formulate a water allocation problem by risk-averse multistage programming, which has the advantage of controlling high-risk severe water shortage events. We apply the proposed formulation to the southeastern portion of Tucson, AZ to best use the limited water resources available to that region. In numerical experiments we (1) present a comparative computational study of the risk-averse nested L-shaped variants and (2) analyze the risk-averse approach to the water allocation problem.","10.1287/ijoc.2015.0684","58833","0.2327869"
"359","hot_topic_cluster_no_11","rank_2","879","Overview of Risk Management System of Commercial Bank Data Center","In nowadays, the trend of the economic globalization is increasing evidently. Without exception, all of the global banking industries take information technology as a necessary condition for survival in the future and the core of the competition. Aimed at two risks, 'operational risk' and 'compliance risk', which commercial banks data center must face to, this article constructs a risk management system, represents a management style of 'One management framework, One set of risk baseline, Three kinds of control methods, Three improvement mechanisms establishes risk baselines as the basic of risk management, consolidates risk assessment experience, raises the standardization level of the risk assessment, improves risk baselines with continuous effort to adapt the updated security environment, identifies operational risk and compliance risk comprehensively within the unified management framework, achieves risk management standardized and sustainable, provides strong support to the commercial bank on the development and robust operation around the whole world.","10.14257/ijsia.2016.10.3.23","60182","0.2426829"
"360","hot_topic_cluster_no_11","rank_2","879","Visual analysis method for cultural heritage site risk assessment","Many significant cultural heritage sites are at risk caused by natural environment. A unique type of natural risk to heritage sites is deterioration risk. Conservators and managers of heritage sites are attempting to develop a risk management approach to reduce this type of risk. Risk assessment is the essential component part of risk management process. However, it is hindered by several challenges resulting from the complexity of deterioration risk. We propose the use of visual analysis method for deterioration risk assessment focusing on matching the major needs and objectives of deterioration risk analysis. Our purpose is to facilitate risk analysis which consists of perceiving risk as basis, risk level estimate, and risk cause analysis. A spatial view of deterioration risk is designed for the discovery of distribution patterns. Based on clustering technique, we propose a visual analytics method for risk level analysis. Lastly, the proposed multidimensional data analysis technique is used to detect the causes of deterioration risks.","10.1007/s12650-015-0325-7","58146","0.3138889"
"361","hot_topic_cluster_no_11","rank_3","820","Economic and Internet Growth Effect on Electricity Consumption in the BRICS Countries","This study investigated the impact of information communication technology and economic growth (measured as gross domestic product) on electricity consumption for a panel consisting of five emerging economic powerhouses, namely Brazil, Russia, India, China and South Africa. A dynamic panel data model for the period 1990-2014 was used for the analysis. The first main finding shows a positive and statistically significant effect of information communication technology uptake on electricity consumption. The second finding indicates that economic growth also has a positive and statistically significant effect on electricity consumption in these five countries.","10.1007/s11294-019-09743-6","23769","0.1451613"
"362","hot_topic_cluster_no_11","rank_3","820","Is information diffusion a threat to market power for financial access? Insights from the African banking industry","This study assesses how information diffusion dampens the adverse effect of market power on the price and quantity of loans provided by a panel of 162 banks from 39 African countries for the period 2001-2011. First, from the Generalised Method of Moments results, a mobile phone penetration rate of 54.29, rising to 57 per 100 people are predicted to neutralise the adverse effect of market power on the average loan price and quantity respectively. Second, from the Quantile Regressions, mobile phone penetration rates of 56.20, 52.04 and 42.76 per 100 people is needed to nullify the negative effect of market power on loan quantity at the 10th decile, 25th quartile and 90th decile respectively. Third, a considerably lower internet penetration rate of 9.49 per 100 people is required to counteract the negative impact of market power on loan quantity at the 90th decile. Policy implications are discussed. ","10.1016/j.mulfin.2018.04.005","37315","0.1515152"
"363","hot_topic_cluster_no_11","rank_3","820","The Effect of Emotion on Prosocial Tendency: The Moderating Effect of Epidemic Severity Under the Outbreak of COVID-19","During the outbreak of COVID-19, information on the epidemic inundated people's lives and led to negative emotions (e.g., tension, anxiety, and fear) in many people. This study aims to explore the effect of various emotions on prosocial tendencies during the COVID-19 outbreak and the moderating effect of the severity of the epidemic. We explore these effects by conducting a text analysis of the content of posts by 387,730 Weibo users. The results show that the severity of the epidemic promotes prosocial tendencies; anger motivates prosocial tendencies significantly; and the severity of the epidemic moderates the effects of three emotions-anger, sadness, and surprise-on prosocial tendencies. These findings provide a reference for exploring the positive significance of major disasters.","10.3389/fpsyg.2020.588701","54988","0.1549020"
"364","hot_topic_cluster_no_11","rank_3","820","Including second order effects in environmental assessments of ICT","Information and Communication Technology (ICT) can have both negative and positive impacts on the environment. Immediate negative environmental impacts arise due to the production, use and disposal of ICT products, while positive effects can arise because ICT products and services replace other products. Other, more indirect consequences of introducing new technologies include e.g. that money saved by reducing costs due to ICT-induced energy efficiency, is being used in consumption of other goods and services that also need energy in their production. Such effects are examined within different disciplines under headings such as rebound effects, indirect effects, second order effects and ripple effects. This paper presents a review and discussion of different second order effects that can be linked to ICT usage in general, using e-commerce as an example. This is a first necessary step in developing methods which include second order effects when analysing the environmental impacts of ICT. ","10.1016/j.envsoft.2014.02.005","70280","0.1590164"
"365","hot_topic_cluster_no_11","rank_3","820","Financial literacy and gender difference in loan performance","We use data from a major peer-to-peer lending marketplace in China to study whether female and male investors evaluate loan performance differently. Controlling for variables of investor demographics, investor experience, and loan characteristics, we find that loans invested by female investors are more likely to default and have lower loan return in the future than loans invested by male investors. We define abnormal default or abnormal loan return as the part of the loan default or the part of loan return that is not explained by loan characteristics and find that the loans invested by female investors have higher abnormal default and lower abnormal loan return than the loans invested by male investors. Furthermore, female investors perform similarly to male investors in abnormal default or abnormal loan return when investors have high levels of education or income or when investors work in finance or information technology industries.","10.1016/j.jempfin.2018.06.004","35196","0.1703704"
"366","hot_topic_cluster_no_11","rank_4","234","Secular Trends in the Achievement of Physical Activity Guidelines: Indicator of Sustainability of Healthy Lifestyle in Czech Adolescents","(1) Background: The increasing socio-economic and educational demands on adolescents should be reflected in their lifestyles accordingly. The aim of the study was therefore to identify the trends in the achievement of physical activity (PA) guidelines by Czech adolescents through objective and subjective PA monitoring. (2) Methods: The research was carried out between 2010 and 2017 and involved 49 secondary schools, 2 higher vocational schools, and 8 universities in the Czech Republic. In total, the projects involved 1129 girls and 779 boys. PA monitoring was performed by Yamax SW-700 pedometers and IPAQ-long questionnaires. (3) Results: The results according to the average number of steps/day confirm a decrease in the amount of PA in boys and girls and in the achievement of the recommended 11,000 steps/day. However, the estimates of meeting the recommended weekly PA expressed as MET-min/week based on the IPAQ-long questionnaire are not so convincing about the decrease. (4) Conclusions: The combination of objective monitoring of weekly PA using wearables and subjective estimates of weekly PA using a questionnaire regarding the ease of application, appears to be a sufficient indicator of the status of and trend in PA and thus the sustainability of a healthy lifestyle in youths.","10.3390/su12125183","10041","0.1987500"
"367","hot_topic_cluster_no_11","rank_4","234","The associations between neighborhood walkability attributes and objectively measured physical activity in older adults","A limited number of studies have used objective measures to examine the associations between the built environment and physical activity (PA) among older adults. This study aimed to examine geographic information systems-derived neighborhood walkability attributes and accelerometer measured PA in older adults. Data were collected from 124 older Taiwanese adults aged over 60 years (mean age: 69.9). Adjusted multiple linear regression was performed to explore the associations between five neighborhood walkability factors (population density, street connectivity, sidewalk availability, access to destinations, and public transportation) and five metrics of accelerometer-measured physical activity (total PA, moderate-to-vigorous PA, light PA, long moderate-to-vigorous PA bouts, and daily step counts). After adjusting for potential confounders, we found that greater sidewalk availability was positively associated with daily step counts in older adults (beta = 0.165; 95% confidence interval: 0.006, 0.412; P = 0.043). No associations between other neighborhood environment attributes and PA metrics were observed. In conclusion, high sidewalk availability in the neighborhood may be supportive for older adults' daily step counts. Further longitudinal research is needed to establish the causality between the built environment and objectively measured PA in older adults.","10.1371/journal.pone.0222268","22362","0.2161290"
"368","hot_topic_cluster_no_11","rank_4","234","Schoolyard Characteristics, Physical Activity, and Sedentary Behavior: Combining GPS and Accelerometry","BACKGROUNDPhysical activity (PA) is decreasing among children, while sedentary behavior (SB) is increasing. Schoolyards seem suitable settings to influence children's PA behavior. This study investigated the associations between schoolyard characteristics and moderate-to-vigorous physical activity (MVPA) and SB of children aged 8-11 years at schoolyards. METHODSTwenty primary schools in the Netherlands were involved. A total of 257 children wore an accelerometer and global positioning system (GPS) device for 5 consecutive days to objectively assess their PA levels and presence at the schoolyard, respectively. Accelerometer and GPS data were merged using the personal activity and location measurement system. Multilevel linear regression analyses were used to study correlates of MVPA and SB at schoolyards. RESULTSOn average, children spent 54 minutes a day at the schoolyard, 9 minutes of which were spent in MVPA and 20 minutes in SB. Boys engaged in MVPA longer than girls at the schoolyard. Fixed equipment, such as high bars and soccer goals, teacher-initiated activities, and the presence of a ball games policy were correlates of more MVPA and less SB. CONCLUSIONWell-designed schoolyards, including PA-enhancing fixed equipment, a supportive PA climate created by teachers, and supportive schoolyard policies may contribute to increased PA and decreased SB during school recess among school-aged children.","10.1111/josh.12459","56321","0.2264151"
"369","hot_topic_cluster_no_11","rank_4","234","Neighborhood Walkability Perceptions: Associations With Amount of Neighborhood-Based Physical Activity by Intensity and Purpose","Background: Built environments are associated with physical activity (PA), but most studies to date have employed acontextual PA outcome measures. The purposes of this study were to examine the proportion of PA that occurred within participants' neighborhoods and associations between neighborhood walkability attributes and different intensities and purposes of PA episodes occurring specifically within neighborhoods. Methods: 384 community residents completed 7 subscales of the Neighborhood Environment Walkability Scale (NEWS) and a detailed 7-day PA log-booklet that included the duration, intensity, and purpose of all episodes. Results: Only one-third of reported PA episodes occurred in participants' neighborhoods. Higher ratings for 5 of the 7 walkability variables were associated with an increased likelihood of engaging in at least some moderate-intensity neighborhood PA (versus none), but were not significantly associated with engaging in greater levels of neighborhood PA (150+ versus 1-149 minutes). Land use mix access, street connectivity, and aesthetics were significant predictors of transportation-related neighborhood PA, but only aesthetics was significantly associated with neighborhood recreational PA. Conclusions: Improving neighborhood walkability may be a stimulus for increased neighborhood PA, especially among largely sedentary individuals, but different attributes are associated with transportation-related and recreational activity.","10.1123/jpah.7.1.3","83125","0.2477273"
"370","hot_topic_cluster_no_11","rank_4","234","Why neighborhood park proximity is not associated with total physical activity","This study explored how parks within the home neighborhood contribute to total physical activity (PA) by isolating park-related PA. Seattle-area adults (n = 634) were observed using time-matched accelerometer, Global Positioning System (GPS), and travel diary instruments. Of the average 42.3 min of daily total PA, only 11% was related to parks. Both home neighborhood park count and area were associated with park-based PA, but not with PA that occurred elsewhere, which comprised 89% of total PA. This study demonstrates clear benefits of neighborhood parks for contributing to park-based PA while helping explain why proximity to parks is rarely associated with overall PA.","10.1016/j.healthplace.2018.05.011","36383","0.2500000"
"371","hot_topic_cluster_no_11","rank_5","403","Effect of Communication Practices on Volunteer Organization Identification and Retention","Volunteering has taken on growing significance as a benefit to society and in initiatives to promote sustainability; it is therefore important to understand the factors driving its success. One increasingly studied variable with a positive effect on volunteer behavior and retention is organization identification. The antecedents influencing the organization identification variable, however, have not yet been explored in the volunteer literature. We address this gap by implementing a survey among volunteers at the OUR HOUSE Grief Support Center in Los Angeles and analyzing results via simple and multivariate linear regression analyses. Specifically, we investigate whether or not communication factors affect both organization identification and volunteer intention to continue. We find that specific communication factors, including a relationship with one's supervisor, internal communication, and external social media postings significantly increase the level of organization identification and retention. Our findings are consistent with the theories of leader-member exchange and absorption capacity. Practitioners and nonprofits can improve the organizational environment of volunteers by optimizing these communication practices.","10.3390/su11092467","26599","0.1402597"
"372","hot_topic_cluster_no_11","rank_5","403","Analysis of a new business model to fundraise non-governmental organizations using fuzzy cognitive maps","Fundraising is one of the most critical issues for non-governmental organizations (NGOs) to carry out their projects. In this paper, a search engine project which aims to find additional financial sources and increase donations for NGOs is proposed. The proposed search engine project is analyzed using fuzzy cognitive maps (FCMs) to define and manage factor influences on the success of the project. FCMs are useful tools to define long term effects of important factors for a system. First casual relations of the factors are determined and then using sigmoid function for learning algorithm, the equilibrium state for the system is obtained. It is found that the factors generating monetary values are the most important ones for the project to be successful in long term.","10.3233/JIFS-189092","16397","0.1444444"
"373","hot_topic_cluster_no_11","rank_5","403","Designing of an environmental assessment algorithm for surface mining projects","This paper depicts the method used to quantify the environmental impact of mining activities in surface mine projects. The affected environment was broken down into thirteen components, such as Human health and immunity, Surface water, Air quality, etc. The effect of twenty impacting factors from the mining and milling activities was then calculated for each Environmental Component. Environmental assessments are often performed by using matrix methods in which one dimension of the matrix is the ""Impacting Factor"" and the other one is the ""Environmental Components"". For the presented matrix method, each Impacting Factor was first given a magnitude between -10 and 10. These factors are used to set up a matrix named Impacting Factor Matrix, whose elements represent the Impacting Factor values. The effects of each Impacting Factor on each Environmental Component were then quantified by multiplying the Impacting Factor Matrix by Weighting Factor Matrix. The elements of the weighting factors matrix reflect the effects of each Impacting Factor on each Environmental Component. The outlined method was originally developed for a mining and milling operation in Iran, but it can successfully be used for mining ventures and more general industrial activities in other countries in accordance to their environmental regulations and laws. ","10.1016/j.jenvman.2008.12.007","84033","0.1455556"
"374","hot_topic_cluster_no_11","rank_5","403","Interannual and Seasonal Vegetation Changes and Influencing Factors in the Extra-High Mountainous Areas of Southern Tibet","The ecosystem of extra-high mountain areas is very fragile. Understanding local vegetation changes is crucial for projecting ecosystem dynamics. In this paper, we make a case for Himalayan mountain areas to explore vegetation dynamics and their influencing factors. Firstly, the interannual trends of the normalized difference vegetation index (NDVI) were extracted by the Ensemble Empirical Mode Decomposition (EEMD) algorithm and linear regression method. Moreover, the influence of environmental factors on interannual NDVI trends was assessed using the Random Forests algorithm and partial dependence plots. Subsequently, the time-lag effects of seasonal NDVI on different climatic factors were discussed and the effects of these factors on seasonal NDVI changes were determined by partial correlation analysis. The results show that (1) an overall weak upward trend was observed in NDVI variations from 1982 to 2015, and 1989 is considered to be the breakpoint of the NDVI time series; (2) interannual temperature trends and the shortest distance to large lakes were the most important factors in explaining interannual NDVI trends. Temperature trends were positively correlated with NDVI trends. The relationship between the shortest distance to large lakes and the NDVI trend is an inverted U-shaped; (3) the time-lags of NDVI responses to four climatic factors were shorter in Autumn than that in Summer. The NDVI responds quickly to precipitation and downward long-wave radiation; (4) downward long-wave radiation was the main climate factor that influenced NDVI changes in Autumn and the growing season because of the warming effect at night. This study is important to improve the understanding of vegetation changes in mountainous regions.","10.3390/rs11111392","25800","0.1475806"
"375","hot_topic_cluster_no_11","rank_5","403","Conceptualizing E-Inclusion in Europe: An Explanatory Study","The aim of this article is to conceptualize e-Inclusion and identify factors affecting it. A critical review of the literature is conducted to identify and categorize the factors influencing e-Inclusion into a comprehensive taxonomy. Using a survey questionnaire, the impact of these factors in influencing citizens' adoption of e-government services was examined. The findings highlight a number of factors under demographic, political, economic social, cultural, and infrastructural dimensions that can have a significant influence on e-Inclusion.","10.1080/10580530.2012.716992","78325","0.2129032"
"376","hot_topic_cluster_no_12","rank_1","274","Daily and Monthly Suspended Sediment Load Predictions Using Wavelet Based Artificial Intelligence Approaches","In the current study, the efficiency of Wavelet-based Least Square Support Vector Machine (WLSSVM) model was examined for prediction of daily and monthly Suspended Sediment Load (SSL) of the Mississippi River. For this purpose, in the first step, SSL was predicted via ad hoc LSSVM and Artificial Neural Network (ANN) models; then, streamflow and SSL data were decomposed into sub-signals via wavelet, and these decomposed sub-time series were imposed to LSSVM and ANN to simulate discharge-SSL relationship. Finally, the ability of WLSSVM was compared with other models in multi-step-ahead SSL predictions. The results showed that in daily SSL prediction, LSSVM has better outcomes with Determination Coefficient (DC)=0.92 than ad hoc ANN with DC=0.88. However unlike daily SSL, in monthly modeling, ANN has a bit accurate upshot. WLSSVM and wavelet-based ANN (WANN) models showed same consequences in daily and different in monthly SSL predictions, and adding wavelet led to more accuracy of LSSVM and ANN. Furthermore, conjunction of wavelet to LSSVM and ANN evaluated via multi-step-ahead SSL predictions and, e.g., DCLSSVM = 0.4 was increased to the DCWLSSVM =0.71 in 7-day ahead SSL prediction. In addition, WLSSVM outperformed WANN by increment of time horizon prediction.","10.1007/s11629-014-3121-2","67647","0.2539326"
"377","hot_topic_cluster_no_12","rank_1","274","Comparative study of different wavelet-based neural network models to predict sewage sludge quantity in wastewater treatment plant","In this study, artificial neural networks (ANNs) including feed forward back propagation neural network (FFBP-NN) and the radial basis function neural network (RBF-NN) were applied to predict daily sewage sludge quantity in wastewater treatment plant (WWTP). Daily datasets of sewage sludge have been used to develop the artificial intelligence models. Six mother wavelet (W) functions were employed as a preprocessor in order to increase accuracy level of ANNs. In this way, a 4-day lags were considered as input variables to conduct training and testing stages for the proposed W-ANNs. To compare performance of W-ANNs with traditional ANNs, coefficient of correlation (R), root mean square error (RMSE), mean absolute error (MAE), and Nash-Sutcliffe efficiency coefficient (NSE) were considered. In the case of all wavelet functions, it was found that W-FFBP-NN (R=0.99 and MAE=5.78) and W-RBF-NN (R=0.99 and MAE=6.69) models had superiority to the FFBP-NN (R = 0.9 and MAE = 21.41) and RBF-NN (R = 0.9 and MAE = 20.1) models. Furthermore, the use of DMeyer function to improve ANNs indicated that W-FFBP-NN (RMSE=7.76 and NSE=0.98) and W-RBF-NN (RMSE = 9.35 and NSE=0.98) approaches stood at the highest level of precision in comparison with other mother wavelet functions used to develop the FFBP-NN and RBF-NN approaches. Overall, this study proved that application of various mother wavelet functions into architecture of ANNs led to increasing accuracy of artificial neural networks for estimation of sewage sludge volume in the WWTP.","10.1007/s10661-019-7196-7","28878","0.2605263"
"378","hot_topic_cluster_no_12","rank_1","274","Modeling the performance of ""up-flow anaerobic sludge blanket"" reactor based wastewater treatment plant using linear and nonlinear approaches-A case study","The paper describes linear and nonlinear modeling of the wastewater data for the performance evaluation of an up-flow anaerobic sludge blanket (UASB) reactor based wastewater treatment plant (WWTP). Partial least squares regression (PLSR), multivariate polynomial regression (MPR) and artificial neural networks (ANNs) modeling methods were applied to predict the levels of biochemical oxygen demand (BOD) and chemical oxygen demand (COD) in the UASB reactor effluents using four input variables measured weekly in the influent wastewater during the peak (morning and evening) and non-peak (noon) hours over a period of 48 weeks. The performance of the models was assessed through the root mean squared error (RMSE), relative error of prediction in percentage (REP), the bias, the standard error of prediction (SEP), the coefficient of determination (R-2), the Nash-Sutcliffe coefficient of efficiency (E-f), and the accuracy factor (A(f)) computed from the measured and model predicted values of the dependent variables (BOD, COD) in the WWTP effluents. Goodness of the model fit to the data was also evaluated through the relationship between the residuals and the model predicted values of BOD and COD. Although, the model predicted values of BOD and COD by all the three modeling approaches (PLSR, MPR, ANN) were in good agreement with their respective measured values in the WWTP effluents, the nonlinear models (MPR, ANNs) performed relatively better than the linear ones. These models can be used as a tool for the performance evaluation of the WWTPs. ","10.1016/j.aca.2009.11.001","82794","0.2631068"
"379","hot_topic_cluster_no_12","rank_1","274","Modeling of the activated sludge process by using artificial neural networks with automated architecture screening","In this study, a MATLAB script was developed to aid in the development of artificial neural network (ANN) models by Screening Out the better ANN architectures for the cases studied. Then, the script was applied for modeling of activated sludge process (ASP) for two different cases. In the first one, a hypothetical wastewater treatment plant (WWTP) was considered. The input and Output data for the training of the ANN models were generated using a simulation model, which was an implementation of the Activated Sludge Model No. 1 (ASM 1). The results indicated high correlation coefficient (R) between the observed and predicted output variables, reaching up to 0.980. In the second case, ANN modeling of ASP in the Iskenderun Wastewater Treatment Plant (IskWWTP) was studied. Resulting maximum R value was 0.795 for the predicted effluent chemical oxygen demand (CODeff) Values. Moreover, CODeff was forecasted using another effluent parameter. ","10.1016/j.compchemeng.2008.01.008","85054","0.2744681"
"380","hot_topic_cluster_no_12","rank_1","274","Time series prediction of under-five mortality rates for Nigeria: comparative analysis of artificial neural networks, Holt-Winters exponential smoothing and autoregressive integrated moving average models","Background Accurate forecasting model for under-five mortality rate (U5MR) is essential for policy actions and planning. While studies have used traditional time series modeling techniques (e.g., autoregressive integrated moving average (ARIMA) and Holt-Winters smoothing exponential methods), their appropriateness to predict noisy and non-linear data (such as childhood mortality) has been debated. The objective of this study was to model long-term U5MR with group method of data handling (GMDH)-type artificial neural network (ANN), and compare the forecasts with the commonly used conventional statistical methods-ARIMA regression and Holt-Winters exponential smoothing models. Methods The historical dataset of annual U5MR in Nigeria from 1964 to 2017 was obtained from the official website of World Bank. The optimal models for each forecasting methods were used for forecasting mortality rates to 2030 (ending of Sustainable Development Goal era). The predictive performances of the three methods were evaluated, based on root mean squared errors (RMSE), root mean absolute error (RMAE) and modified Nash-Sutcliffe efficiency (NSE) coefficient. Statistically significant differences in loss function between forecasts of GMDH-type ANN model compared to each of the ARIMA and Holt-Winters models were assessed with Diebold-Mariano (DM) test and Deming regression. Results The modified NSE coefficient was slightly lower for Holt-Winters methods (96.7%), compared to GMDH-type ANN (99.8%) and ARIMA (99.6%). The RMSE of GMDH-type ANN (0.09) was lower than ARIMA (0.23) and Holt-Winters (2.87). Similarly, RMAE was lowest for GMDH-type ANN (0.25), compared with ARIMA (0.41) and Holt-Winters (1.20). From the DM test, the mean absolute error (MAE) was significantly lower for GMDH-type ANN, compared with ARIMA (difference = 0.11, p-value = 0.0003), and Holt-Winters model (difference = 0.62, p-value< 0.001). Based on the intercepts from Deming regression, the predictions from GMDH-type ANN were more accurate (beta(0) = 0.004 +/- standard error: 0.06; 95% confidence interval: - 0.113 to 0.122). Conclusions GMDH-type neural network performed better in predicting and forecasting of under-five mortality rates for Nigeria, compared to the ARIMA and Holt-Winters models. Therefore, GMDH-type ANN might be more suitable for data with non-linear or unknown distribution, such as childhood mortality. GMDH-type ANN increases forecasting accuracy of childhood mortalities in order to inform policy actions in Nigeria.","10.1186/s12874-020-01159-9","  314","0.3350318"
"381","hot_topic_cluster_no_12","rank_2","856","Comparison of Prediction Models for Power Draw in Grinding and Flotation Processes in a Gold Treatment Plant","As one of the principal anticipated goals in 2015, government and scientists have been paying increasing attention to energy saving. Energy-saving potentials play an important role in economical and sustainable development in the gold industry. Through analyzing the factors that significantly influence energy consumption in the grinding and flotation processes in a gold treatment plant, three models for energy consumption prediction are established based on large amounts of actual production data. The multiple linear regression model demonstrates low prediction accuracy. In consideration of the advantages of artificial neural networks (ANNs), a back-propagation (BP) neural network model is built to provide higher prediction accuracy. Moreover, a hybrid GA-BP neural network model is established combining the typical characteristics of a genetic algorithm (GA) and a BP neural network. Subsequently, validation and comparison of the relative prediction errors, as well as the RMSE of the three models illustrate that the hybrid GA-BP neural network model presents the highest prediction accuracy. The total shift percentage of the hybrid GA-BP neural network model is 98% and 80%, when the relative prediction errors of the model are within +/- 5% and +/- 3%, respectively, and its prediction results show a minimum RMSE of 1.29. In contrast, of the three models, the hybrid GA-BP neural network model can provide the highest prediction accuracy of energy consumption, and consequently, can offer a positive reference for real production.","10.1252/jcej.15we127","60683","0.2541176"
"382","hot_topic_cluster_no_12","rank_2","856","Annual Rainfall Forecasting Using Hybrid Artificial Intelligence Model: Integration of Multilayer Perceptron with Whale Optimization Algorithm","Rainfall, as one of the key components of hydrological cycle, plays an undeniable role for accurate modelling of other hydrological components. Therefore, a precise forecasting of annual rainfall is of the high importance. In this regard, several studies have been tried to predict annual rainfall of different climate zones using machine learning and soft computing algorithms. This study investigates the application of an innovative hybrid method, namely Multilayer Perceptron-Whale Optimization Algorithm (MLP-WOA) to predict annual rainfall comparatively to the ordinary Multilayer Perceptron models (MLP). The models were developed by using 3-Input variables of annual rainfall at lag1, 2 and 3 corresponding to Pt-1, Pt-2 and P-t-3,P- respectively of two synoptic stations of Senegal (Fatick and Goudiry) in the time period of 1933-2013. 75% of the dataset were utilized for training and the other 25% for testing the studied models Accurateness of the mentioned models was examined using root mean squared error, correlation coefficient, and KlingGupta efficiency. Results showed that MLP-WOA3 and MLP3 using both Pt-1, Pt-2 and Pt-3 as inputs presented the most accurate forecasting in Fatick and Goudiry stations, respectively. In Fatick station, MLP-WOA3 decreased the RMSE value of MLP3 by 18.3% and increased the R and KGE values by 3.0% and 130%, respectively in testing period. But, in Goudiry station, MLP-WOA3 increased the RMSE value of MLP3 by 3.9% and increased the R and KGE values by 10.2% and 91% in testing period. Therefore, it can be realized that the MLP-WOA3 could not able to reduce the RMSE value of correspondent MLP model in Goudiry station. The conclusive results indicated that MLP-WOA slightly improved the accuracy of correspondent MLP models and may be recommended for annual rainfall forecasting.","10.1007/s11269-019-02473-8","18189","0.2569892"
"383","hot_topic_cluster_no_12","rank_2","856","Prediction of SO2 and PM concentrations in a coastal mining area (Zonguldak, Turkey) using an artificial neural network","In this study, artificial neural networks are proposed to predict the concentrations of SO, and PM at two different stations in Zonguldak city, a major coastal mining area in Turkey. The established artificial neural network models involve meteorological parameters and historical data on observed SO2, PM as input variables. The models are based on a three-layer neural network trained by a back-propagation algorithm. The models accurately measure the trend of SO2, and PM concentrations. The results obtained through the proposed models show that artificial neural networks can efficiently be used in the analysis and prediction of air quality.","","87260","0.2628571"
"384","hot_topic_cluster_no_12","rank_2","856","Modeling and predicting biological performance of contact stabilization process using artificial neural networks","In this paper, the microfauna distribution data of a contact stabilization process were used in a neural network system to model and predict the biological activity of the effluent. Five uncorrelated components of the microfauna were used as the artificial neural network model input to predict the dehydrogenase activity of the effluent (DAE) using back-propagation and general regression algorithms. The models' optimum architectures were determined for the back-propagation neural network (BPNN) model by varying the number of hidden layers, hidden transfer functions, test set size percentages, and initial weights. Comparison of the two model prediction results showed that the genetic general regression neural network model demonstrated the ability to calibrate the multicomponent microfauna, and yielded reliable DAE close to that resulting from direct experimentation, and thus was judged superior to BPNN models.","10.1061/(ASCE)0887-3801(2004)18:4(341)","88986","0.2701754"
"385","hot_topic_cluster_no_12","rank_2","856","Modeling of ammonia emission in the USA and EU countries using an artificial neural network approach","Ammonia emissions at the national level are frequently estimated by applying the emission inventory approach, which includes the use of emission factors, which are difficult and expensive to determine. Emission factors are therefore the subject of estimation, and as such they contribute to inherent uncertainties in the estimation of ammonia emissions. This paper presents an alternative approach for the prediction of ammonia emissions at the national level based on artificial neural networks and broadly available sustainability and economical/agricultural indicators as model inputs. The Multilayer Perceptron (MLP) architecture was optimized using a trial-and-error procedure, including the number of hidden neurons, activation function, and a back-propagation algorithm. Principal component analysis (PCA) was applied to reduce mutual correlation between the inputs. The obtained results demonstrate that the MLP model created using the PCA transformed inputs (PCA-MLP) provides a more accurate prediction than the MLP model based on the original inputs. In the validation stage, the MLP and PCA-MLP models were tested for ammonia emission predictions for up to 2 years and compared with a principal component regression model. Among the three models, the PCA-MLP demonstrated the best performance, providing predictions for the USA and the majority of EU countries with a relative error of less than 20 %.","10.1007/s11356-015-5075-5","62859","0.2707317"
"386","hot_topic_cluster_no_12","rank_3","372","Data center growth in the United States: decoupling the demand for services from electricity use","Data centers are energy intensive buildings that have grown in size and number to meet the increasing demands of a digital economy. This paper presents a bottom-up model to estimate data center electricity demand in the United States over a 20 year period and examines observed and projected electricity use trends in the context of changing data center operations. Results indicate a rapidly increasing electricity demand at the turn of the century that has significantly subsided to a nearly steady annual electricity use of about 70 billionkWh in recent years. While data center workloads continue to grow exponentially, comparable increases in electricity demand have been avoided through the adoption of key energy efficiency measures and a shift towards large cloud-based service providers. Alternative projections from the model illustrate the wide range in potential electricity that could be consumed to support data centers, with the US data center workload demand estimated for 2020 requiring a total electricity use that varies by about 135 billion kWh, depending on the adoption rate of efficiency measures during this decade. While recent improvements in data center energy efficiency have been a success, the growth of data center electricity use beyond 2020 is uncertain, as modeled trends indicate that the efficiency measures of the past may not be enough for the data center workloads of the future. The results show that successful stabilization of data center electricity will require new innovations in data center efficiency to further decouple electricity demand from the ever-growing demand for data center services.","10.1088/1748-9326/aaec9c","32702","0.2365217"
"387","hot_topic_cluster_no_12","rank_3","372","On energy consumption of switch-centric data center networks","Data center network (DCN) is the core of cloud computing and accounts for 40% energy spend when compared to cooling system, power distribution and conversion of the whole data center (DC) facility. It is essential to reduce the energy consumption of DCN to ensure energy-efficient (green) data center can be achieved. An analysis of DC performance and efficiency emphasizing the effect of bandwidth provisioning and throughput on energy proportionality of two most common switch-centric DCN topologies: three-tier (3T) and fat tree (FT) based on the amount of actual energy that is turned into computing power are presented. Energy consumption of switch-centric DCNs by realistic simulations is analyzed using GreenCloud simulator. Power-related metrics were derived and adapted for the information technology equipment processes within the DCN. These metrics are acknowledged as subset of the major metrics of power usage effectiveness and data center infrastructure efficiency, known to DCs. This study suggests that although in overall FT consumes more energy, it spends less energy for transmission of a single bit of information, outperforming 3T.","10.1007/s11227-017-2132-5","42412","0.2444444"
"388","hot_topic_cluster_no_12","rank_3","372","Low-power task scheduling algorithm for large-scale cloud data centers","How to effectively reduce the energy consumption of large-scale data centers is a key issue in cloud computing. This paper presents a novel low-power task scheduling algorithm (LTSA) for large-scale cloud data centers. The winner tree is introduced to make the data nodes as the leaf nodes of the tree and the final winner on the purpose of reducing energy consumption is selected. The complexity of large-scale cloud data centers is fully consider, and the task comparson coefficient is defined to make task scheduling strategy more reasonable. Experiments and performance analysis show that the proposed algorithm can effectively improve the node utilization, and reduce the overall power consumption of the cloud data center.","10.1109/JSEE.2013.00101","73234","0.2880000"
"389","hot_topic_cluster_no_12","rank_3","372","Cost-Aware Multi-Domain Virtual Data Center Embedding","Virtual data center is a new form of cloud computing concept applied to data center. As one of the most important challenges, virtual data center embedding problem has attracted much attention from researchers. In data centers, energy issue is very important for the reality that data center energy consumption has increased by dozens of times in the last decade. In this paper, we are concerned about the cost-aware multi-domain virtual data center embedding problem. In order to solve this problem, this paper first addresses the energy consumption model. The model includes the energy consumption model of the virtual machine node and the virtual switch node, to quantify the energy consumption in the virtual data center embedding process. Based on the energy consumption model above, this paper presents a heuristic algorithm for cost-aware multi-domain virtual data center embedding. The algorithm consists of two steps: inter-domain embedding and intra-domain embedding. Inter-domain virtual data center embedding refers to dividing virtual data center requests into several slices to select the appropriate single data center. Intra-domain virtual data center refers to embedding virtual data center requests in each data center. We first propose an inter-domain virtual data center embedding algorithm based on label propagation to select the appropriate single data center. We then propose a cost-aware virtual data center embedding algorithm to perform the intra-domain data center embedding. Extensive simulation results show that our proposed algorithm in this paper can effectively reduce the energy consumption while ensuring the success ratio of embedding.","","32583","0.3129032"
"390","hot_topic_cluster_no_12","rank_3","372","Energy and Utility-based Algorithm for Scheduling Virtual Machine in Could Computing","One of the pressing issues that the cloud data center needs to solve is how to improve its utility while reducing the energy consumption in the data center. This paper presents a virtual machine scheduling model based on multi-objective optimization VMSA-EU, which aims to minimize energy consumption and maximize the practicality of the data center. And proposed a virtual machine scheduling algorithm based on NSGAII to solve the model. Simulation results show that our models and algorithms can reduce energy consumption while improving utility. Compared with other similar existing algorithms, the algorithm has made some progress in execution time and scheduling results.","10.14257/ijgdc.2018.11.3.11","39245","0.3406250"
"391","hot_topic_cluster_no_12","rank_4","261","The Benefits and Challenges of Using Real-Time Data in the Classroom: Perspectives From the Students, Educators, and Researchers","This note discusses the advantages of using real-time data (RID) from the perspective of the student, educator, and researcher; our experience in the classroom and field using RID; and a quick survey of the spectrum of RID available to students. We also present some of the challenges sometimes associated with using RID and offer strategies that could improve the use of RID in the classroom. Our focus is on authentic aquatic data that stream in either real-time or near-real-time or are slightly dated and archived.","","79543","0.1562500"
"392","hot_topic_cluster_no_12","rank_4","261","Study on Real-Time Monitoring Method of Marine Ecosystem Micro-Plastic Pollution","In order to improve the real-time monitoring ability of marine ecosystem micro-plastic pollution, a real-time monitoring method of marine ecosystem micro-plastic pollution is proposed, which is based on the fusion of marine ecosystem micro-plastic pollution data and fuzzy correlation detection of pollutants. Firstly, a data acquisition system for real-time monitoring of micro-plastic pollution in marine ecosystem is designed. On the basis of data collection, big data's statistical analysis model of micro-plastic pollution in marine ecosystem is constructed. The data fusion of marine ecosystem micro-plastic pollution is carried out by the method of block detection and gray-scale quantitative statistical analysis, and the cluster analysis and detection of marine ecosystem micro-plastic pollution monitoring data are carried out with the method of correlation feature extraction. Achieve real-time monitoring of micro-plastic pollution in marine ecosystem. The simulation results show that the time-effectiveness and accuracy of real-time monitoring of micro-plastic pollution in marine ecosystem by this method are good.","10.2112/SI95-201.1","17550","0.1571429"
"393","hot_topic_cluster_no_12","rank_4","261","A methodology for real-time data sustainability in smart city: Towards inferencing and analytics for big-data","A city gets evolved into the smart city by improving citizen's wellbeing, sustainability and work efficiency with the help of latest Information Communication Technology (ICT) and Internet of Things (IoT). Automated system monitoring tasks for a smart city plays a crucial aspect in the fields of ICT and IoT. Whereas, these monitoring should be adaptive to real-time data processing concerns to perform data analytics fast and accurate. High frequency and volume of big-data involved in the smart city require information projection to be sustainable while maintaining its representation for producing real-time inferencing and analytical results. Data modeling as reforming into most suitable forms for inferencing and analytics is a challenging and costly task while considering time constraints. Their natural representations are well suited for real-time data analytics and inferencing in IoT-based information on the Web. This study aims to collect information from smart city sensors and transform this information through data modeling by reforming it into data forms like RDF and JSON. A case study of the weather based dataset is shown to get the outcome in the said forms of RDF and JSON. This way data sustainability for both inferencing and big-data analytics can be promised at real-time.","10.1016/j.scs.2017.11.031","37967","0.1639535"
"394","hot_topic_cluster_no_12","rank_4","261","REAL-TIME DATA AND FISCAL POLICY ANALYSIS: A SURVEY OF THE LITERATURE","This paper surveys the empirical research on fiscal policy analysis based on real-time data. This literature can be broadly divided into four groups that focus on: (1) the statistical properties of revisions in fiscal data; (2) the political and institutional determinants of projection errors by governments; (3) the reaction of fiscal policies to the business cycle and (4) the use of real-time fiscal data in structural vector autoregression (VAR) models. It emerges that, first, fiscal revisions are large and initial releases are biased estimates of final values. Secondly, strong fiscal rules and institutions lead to more accurate releases of fiscal data and smaller deviations of fiscal outcomes from government plans. Thirdly, the cyclical stance of fiscal policies is estimated to be more counter-cyclical' when real-time data are used instead of ex post data. Fourthly, real-time data can be useful for the identification of fiscal shocks. Finally, it is shown that existing real-time fiscal data sets cover only a limited number of countries and variables. For example, real-time data for developing countries are generally unavailable. In addition, real-time data on European countries are often missing, especially with respect to government revenues and expenditures. Therefore, more work is needed in this field.","10.1111/joes.12099","60021","0.1694737"
"395","hot_topic_cluster_no_12","rank_4","261","Visualization of real-time monitoring datagraphic of urban environmental quality","Quality of urban environment directly affects people health, and it is important to understand the real-time status of urban air quality. Air quality monitoring, data analysis, and visualization can grasp the concentration data of air pollutants in cities. In view of the current air quality monitoring using digital displays, it is difficult for users to intuitively determine the air pollution level with unsatisfied interaction mode of the data query. Using the real-time monitoring data of 23 observation points in Beijing, the work based on Google Earth applied Keyhole Markup Language (KML) for the visualization of air monitoring data. The interactive query makes it easier for users to query air quality, and gradually varied color can visually highlight the air quality level. Visualization of data has stronger expression (more images and more intuitive) than the original data table, which is beneficial for further analysis of data.","10.1186/s13640-019-0443-6","29161","0.2013889"
"396","hot_topic_cluster_no_12","rank_5","735","Context-Based Roles and Competencies of Data Curators in Supporting Research Data Lifecycle Management: Multi-Case Study in China","Focusing on the main research question of what the critical roles and competencies of data curation are in supporting research data life cycle management, this paper adopts a multi-case study method, with data governance frameworks, to analyze stakeholders and data curators, and their competencies, based on different contexts from cases from enterprises and academic libraries in mainland China. Via the context and business analysis on different cases, critical roles such as data supervisor, data steward, and data custodian in guaranteeing data quality and efficiency of data reuse are put forward. Based on the general factor framework summarized via existing literature, suggestions for empowering data curators' competencies are raised according to the cases. The findings of this paper are as follows: besides digital archiving and preservation, more emphasis should be placed on data governance in the field of data curation. Data curators are closely related but not equivalent to stakeholders of data governance. The different roles of data curators would play their own part in the process of data curation and can be specified as data supervisor, data steward, and data custodian according to given contexts. The roles, competencies, and empowerment strategies presented in this paper might have both theoretical and practical significance for the fields of both data curation and data governance.","10.1515/libri-2018-0065","25974","0.2177083"
"397","hot_topic_cluster_no_12","rank_5","735","Big spatial data for urban and environmental sustainability","Eighty percent of big data are associated with spatial information, and thus are Big Spatial Data (BSD). BSD provides new and great opportunities to rework problems in urban and environmental sustainability with advanced BSD analytics. To fully leverage the advantages of BSD, it is integrated with conventional data (e.g. remote sensing images) and improved methods are developed. This paper introduces four case studies: (1) Detection of polycentric urban structures; (2) Evaluation of urban vibrancy; (3) Estimation of population exposure to PM2.5; and (4) Urban land-use classification via deep learning. The results provide evidence that integrated methods can harness the advantages of both traditional data and BSD. Meanwhile, they can also improve the effectiveness of big data itself. Finally, this study makes three key recommendations for the development of BSD with regards to data fusion, data and predicting analytics, and theoretical modeling.","10.1080/10095020.2020.1754138","12307","0.2180556"
"398","hot_topic_cluster_no_12","rank_5","735","Detecting Problems in Survey Data Using Benford's Law","""It is 15:00 in Nairobi. Do You know where Your enumerators are??"" Good quality data is paramount for applied economic research. If the data are distorted, corresponding conclusions may be incorrect. We demonstrate how Beaford's law, the distribution that first digits of numbers in certain data sets should follow, can be used to test for data abnormalities. We conduct an analysis of nine commonly used data sets and find that much data from developing countries is poor quality while data from the United States seems to be of better quality. Female and male respondents give data of similar quality.","","83333","0.2483871"
"399","hot_topic_cluster_no_12","rank_5","735","Using paradata to collect better survey data: Evidence from a household survey in Tanzania","Data are a key component in the design, implementation, and evaluation of economic and social policies. Monitoring data quality is an essential part of any serious, large-scale data collection process. The purpose of this article is to show how paradata should be used before, during, and after data collection to monitor and improve data quality. To do this we use timestamps, global positioning system (GPS) coordinates, and other paradata collected from an 800-household survey conducted in Tanzania in 2016. We demonstrate how key paradata can be used during each phase of a research project to identify and prevent issues in the data and the methods used to collect it. Our results corroborate the importance of collecting and analyzing paradata to monitor fieldwork and ensuring data quality for micro data collection in developing countries. Based on these findings we also make recommendations as to how researchers can make better use of paradata in the future to manage and improve data quality. We argue for an expansion in the understanding and use of varied paradata among researchers, and a greater focus on its use for improving data quality.","10.1111/rode.12583","26864","0.2508475"
"400","hot_topic_cluster_no_12","rank_5","735","An integrated architecture for future studies in data processing for smart cities","Data processing for Smart Cities become more challenging, facing with different handling steps: data collection from different heterogeneous sources, processing sometimes in real-time and then delivered to high level services or applications used in Smart Cities. Applications used for intelligent transportation systems, crowd management, water resources management, noise and air pollution management, require different data processing techniques. The main subject of this paper is to propose an architecture for data processing in Smart Cities. The architecture is oriented on the flow of data from the source to the end user. We describe seven steps of data processing: collection of data from heterogeneous sources, data normalization, data brokering, data storage, data analysis, data visualization and decision support systems. We consider two case studies on crowd management in smart cities and on Intelligent Transportation Systems (ITS) and we provide experimental highlights. ","10.1016/j.micpro.2017.03.004","45779","0.2578125"
"401","hot_topic_cluster_no_2","rank_1","667","Trajectory of urban sustainability concepts: A 35-year bibliometric analysis","In recent decades, our cities are increasingly expected to become more sustainable urban forms, with many added determinants. A multitude of city concepts has therefore been contrived. The most time-honored and prominent concept is the ""sustainable city,"" which is depicted as a model urban form and thereafter more city concepts have come into being. However, it is not clear for all the concepts, for instance, ""eco-cities,"" ""smart city,"" ""sustainable city,"" and ""resilient city,"" what are the underpinning building blocks within each concept and how these concepts correlate with each other. This bibliometric study organizes this in conducting a descriptive summary, a clustering analysis, and multidimensional scaling of major city concepts, by establishing a co-word matrix of high-frequency keywords occurring in the Science Citations Index (SCI) and Social Science Citations Index (SSCI) databases. In addition to summarizing the evolution of these concepts, it analyzes the composition of each city concept and the core issues addressed by each city type. Also investigated are the correlations between the city concepts with a statistical analysis of the clusters of literature in one concept that overlap or connect to other clusters in another. From this, it is shown that, under the two umbrella terms of ""sustainable city"" and ""smart city,"" the ""?-city"" literature has developed in a variety of distinctive ways. ","10.1016/j.cities.2016.08.003","48748","0.2988372"
"402","hot_topic_cluster_no_2","rank_1","667","Social Innovations in Smart Cities - Case of Poprad","The following paper is discussing the concept of smart cities in the nexus with social innovations. The objective of these two concepts should be increased quality of life in the urban areas. Paper critically examines smart cities and focuses on their use of Information and Communication Technologies (ICT) as tools for reaching the objective of better and sustainable cities. Case study method is utilized in order to examine how the environment for social innovations can be fostered and how ICT can be used to achieve change in citizens behavior and improve the quality of life in the city of Poprad in Slovakia.","10.1007/s11036-018-01209-z","19340","0.3000000"
"403","hot_topic_cluster_no_2","rank_1","667","What makes a city ""smart'?","Taking advantage of information and communications technology tools and techniques for city administration, whether it is for urban planning activities, for transport solutions or many other purposes, is not a new concept. However, in order for a city to be classified as smart', a synthesis of intelligence that transcends mere utilisation is essential. This article analyses the increasing use of information and communications technology and sensing technologies in cities by examining this new way of city governing from a critical perspective. Existing projects and initiatives were investigated to find out how, and to what extent, these tools are being employed by cities. The advantages and the current shortcomings of smart city are also discussed in order to understand the viability of using these tools.","10.1177/1478077116670744","56034","0.3078947"
"404","hot_topic_cluster_no_2","rank_1","667","A Conceptual Smart City Framework for Future Industrial City in Indonesia","In Indonesia, the growth of cities from various big cities and industrial cities can cause many challenges. To face this challenge, policy makers can apply the concept of smart cities. This paper aims to analyze many studies that discuss prospective industrial city planning in a smart city perspective. This research uses information from research, models, frameworks, and tools that discuss IoT, smart cities, and industrial cities. This research provides the latest insight into smart city frameworks for industrial cities. In this study found the pillars forming the smart city for industrial cities. This framework can also be used by governments such as Kulonprogo District in the Special Region of Yogyakarta, Indonesia in preparation to transform itself into a smart industrial city. The latest use of information technology in this concept and with implementation priority steps is recommended.","","24574","0.3250000"
"405","hot_topic_cluster_no_2","rank_1","667","The Mode of Urban Renewal Base on the Smart City Theory under the Background of New Urbanization","The Smart City, coordinated development of economy, ecology and society, is the future of cities. Urban renewal is an important way to build the Smart City. Based on the Smart City theory, the authors propose the ""12345"" model of urban renewal, which means developing a tailored proposal of the city and citizen, and building ""one city management system, two supporting safeguard measures, three information infrastructure platforms, four urban management modes, and five application service systems."" The city will gain the ability of self-adjustment and self-improvement. Smart urban renewal model not only solves the problems of the city at this stage, but also promotes the all-round development of the city, and access to sustainable development.","10.15302/J-FEM-2015035","66182","0.3333333"
"406","hot_topic_cluster_no_2","rank_2","439","The Impact of Home Literacy and Family Factors on Screen Media Use Among Dutch Preteens","This study examined preteens' screen media use and potential differences in media use by child and family demographics among 1464 Dutch preteens. The results demonstrated that watching TV is still a very popular activity among children. However, other electronic media are also popular within this age group as 72 % of preteens had a cellphone. Children who spoke a language other than Dutch and whose parents were born abroad were heavier media users. Children with more books in the home and who read more frequently tended to be lighter media users. Boys spent more time on screen media than girls and were more likely to play videogames while girls preferred using social media. This study demonstrated that child and home characteristics play a significant role in children's engagement with screen media and literacy.","10.1007/s10826-016-0584-5","48582","0.2344828"
"407","hot_topic_cluster_no_2","rank_2","439","Media Exposure and Anxiety during COVID-19: The Mediation Effect of Media Vicarious Traumatization","The rapid spread and high death rates of the COVID-19 pandemic resulted in massive panic and anxiety all over the world. People rely heavily on media for information-seeking during the period of social isolation. This study aimed to explore the relationship between media exposure and anxiety, and highlighted the underlying mechanisms mediated by the media vicarious traumatization effect. A total of 1118 Chinese citizens participated in the online survey, who were from 30 provinces in mainland China. Results showed that all four types of media (official media, commercial media, social media, and overseas media) cause vicarious traumatization to their audiences to different degrees. It was also found that the impact of media exposure on anxiety was mediated by media vicarious traumatization: there were full mediation effects for commercial media exposure and overseas media exposure, while there were indirect-only mediation effects for official media exposure and social media exposure. Audiences staying in cities with a relatively severe pandemic were more susceptible to the vicarious traumatization caused by commercial media compared to those staying in Hubei. This study expanded the concept and application of vicarious traumatization to the mediated context, and the findings provided insightful advice to media practitioners in the face of major crisis.","10.3390/ijerph17134720"," 8754","0.2358696"
"408","hot_topic_cluster_no_2","rank_2","439","Issue-Specific Engagement: How Facebook Contributes to Opinion Leadership and Efficacy on Energy and Climate Issues","Although social media are increasingly studied for their political impact, not enough is known about how distinct forms of Facebook activity, such as general news consumption and expression vs. issue-specific engagement, explain orientations toward a particular issue. Using a Republican sample, we demonstrate that only issue-specific engagement on Facebook-and not other forms of online behaviors-is consistently associated with a greater sense of personal influence on the issue of climate change and energy, which suggests that distinguishing between types of Facebook activity is important.","10.1080/19331681.2015.1034910","68077","0.2625000"
"409","hot_topic_cluster_no_2","rank_2","439","Beyond voice: audience-making and the work and architecture of listening as new media literacies","Considerable attention in communication, media and social science scholarship is focused on voice, which is considered as an important form of social capital and necessary for social equity. Studies have extensively examined access to communication technologies and various forums such as the public sphere, as well as media literacy required to have a voice. Despite continuing concern over a digital divide, the emergence of Web 2.0-based new media, also referred to as social media, is seen as an empowering development contributing to the democratization of voice. However, based on two studies of online public consultation and critical analysis of the literature on voice and listening, this article argues that two important corollaries of voice, as it is commonly conceptualized, are overlooked. To matter, as Nick Couldry says it should, voice needs to have an audience and, second, audiences must listen. While considerable attention is paid by mass media to creating, maintaining and engaging audiences, comparatively little attention is paid to audiences and listening in discussions of new media and social media. In an environment of proliferating channels for speaking coinciding with demassification and fragmentation of audiences, engaging audiences and the work of listening have become problematic and are important media literacies required to make voice matter.","10.1080/10304312.2013.736950","75379","0.2654321"
"410","hot_topic_cluster_no_2","rank_2","439","Strategic Uses of Facebook in Zika Outbreak Communication: Implications for the Crisis and Emergency Risk Communication Model","While social media has been increasingly used for communication of infectious disease outbreaks, little is known about how social media can improve strategic communication across various stages of the health crisis. The Crisis and Emergency Risk Communication Model (Reynolds & Seeger, 2005; CERC) outlines strategies across different crisis phases and can guide crisis communication on social media. This research therefore investigates how social media can be utilized to implement and adapt the CERC model, by examining the strategic uses of Facebook in communicating the recent Zika epidemic by health authorities in Singapore. Zika-related Facebook posts of three main Singapore health agencies published within the one year period from January 2016 to December 2016 were thematically analysed. Results suggest that Facebook was used to communicate the crisis strategically, which supported and added to the CERC model. Novel uses of Facebook for outbreak communication were demonstrated, including promoting public common responsibility for disease prevention and expressing regards to the public for cooperation. Results also suggested that preparedness messages might be the most effective, as they produced a great level of public engagement. The adaptability of the CERC model in social media contexts to improve crisis communication is discussed.","10.3390/ijerph15091974","34955","0.3230769"
"411","hot_topic_cluster_no_2","rank_3","747","The relationship between ICT and student literacy in mathematics, reading, and science across 44 countries: A multilevel analysis","This study conceptualized ICT as multi-level (country-, school-, and student-level) constructs and examined their relationships with student mathematics, reading, and scientific literacy. Three level hierarchical linear models (HLM) were employed to analyse the Programme for International Student Assessment (PISA) 2015 data of 305,414 15-year-old students from 11,075 schools across 44 countries. The findings indicated that (i) national ICT skills had a more positive effect on student academic performance than did national ICT access and use; (ii) students ICT availability at school positively associated with student academic success, whereas student ICT availability at home negatively associated with student academic success; (iii) student ICT academic use negatively correlated with student performance, while ICT entertainment use positively correlated with student performance; and (v) student attitudes toward ICT demonstrated mixed effects on student academic success specifically, student interest, competence, and autonomy in using ICT had positive correlations, while student enjoyment of social interaction around ICT had a negative correlation with student academic performance.","10.1016/j.compedu.2018.05.021","34153","0.2847826"
"412","hot_topic_cluster_no_2","rank_3","747","Building Technology Competency: an Evidence-Based Approach to Improving Student Technology Skills","A Student Resource Center was created at a medium-sized public comprehensive university in the Mid-West as a solution to lack of technology competency in the student body. This Student Resource Center was designed to provide technology skill training in select technology tools in use by faculty, staff, and students. The ADDIE model was used to build a curriculum of technology skill training for the current student body based on data collected from surveys sent to university alumni, faculty, and students in addition to employers who hire a large number of university graduates. Participants were asked to identify the technology skills needed or learned during the time at the university and technology skills needed upon entering the work place. Results of the study indicated that alumni and students felt they needed additional technology training and that employers found new hires generally lacking in technology competency.","10.1007/s41686-017-0001-5","46625","0.2911765"
"413","hot_topic_cluster_no_2","rank_3","747","Internet skills of medical faculty and students: is there a difference?","BackgroundThe shift from a more didactic to student-centred pedagogical approach has led to the implementation of new information communication technology (ICT) innovations and curricula. Consequently, analysis of the digital competency of both faculty and students is of increasing importance. The aim of this research is to measure and compare the internet skills of medical school faculty and students and to investigate any potential skills gap between the two groups.MethodsA survey of medical school faculty and students across three universities in Ireland was carried out using a validated instrument (Internet Skills Scale) measuring five internet skills (Operational, Information Navigation, Social, Creative and Mobile). Three focus groups comprising a total of fifteen students and four semi-structured interviews with faculty across three institutions were carried out to explore further findings and perceptions towards digital literacy, give further insight and add context to the findings.ResultsSeventy-eight medical faculty (response rate 45%) and 401 students (response rate 15%) responded to the survey. Mean scores for each internet skill were high (above 4 out of 5) for all skills apart from Creative (mean of 3.08 for students and 3.10 for faculty). There were no large differences between student and faculty scores across the five skills.Qualitative results supported survey findings with a deeper investigation into topics such as online professionalism, use of licencing and mobile application development. Needs based skills training and support were highlighted as areas for faculty development.ConclusionBoth medical educators and students tend to have similar competencies with respect to internet skills. When implementing online and distance learning methodologies however, medical schools need to ensure appropriate skills training and support for faculty as well as providing targeted training to improve the creative skills of both their educators and students.","10.1186/s12909-019-1475-4","29861","0.2922414"
"414","hot_topic_cluster_no_2","rank_3","747","Perceptions and expectations in the university students from adaptation to the virtual teaching triggered by the COVID-19 pandemic","Introduction: Confinement has fostered the debate on digitalization and virtual teaching. The objective is to expose the disadvantages of the telematic education system adopted during the quarantine on subjects like equal opportunities, well-being and satisfaction, and teaching quality; focusing on the university education level and taking into account the management of communications with the students. Methodology: Two surveys, both distributed by online means, were conducted on university students, one at the beginning of the confinement (n: 1612), and another in the examination period (n: 872). Results: 90% of students prefer face-to-face teaching and 80% consider that their university has not adapted adequately. Most believe that they will learn less, that their academic record will be affected, and that they will have more difficulties while finding a job. More than a half of the students have suffered close experiences related to coronavirus and they feel a lack of understanding on the university side. Conclusions: Universities have shown a poor institutional communication performance during the pandemic, both in communication with students and in listening to them.","10.4185/RLCS-2020-1470","16575","0.2957143"
"415","hot_topic_cluster_no_2","rank_3","747","University Students' Readiness for Using Digital Media and Online Learning-Comparison between Germany and the USA","The year 2020 brought many changes to our everyday life but also our education system. Universities needed to change their teaching practices due to the COVID-19 pandemic. Words like ""digital media"", ""online teaching"" and ""online learning"" were present in all of the discussions. The main issues here were the technical infrastructure of students and universities all over the world. However, to have good technical infrastructure does not mean that everybody is also ready to use it. Thus, the present study focused on the issue of university students' readiness for online learning. The quantitative research goal was to evaluate German university students' readiness for using digital media and online learning in their tertiary education and compare them with students from the United States. Overall, 72 students from the researchers' university in Germany and 176 students from multiple universities in the United States completed the Student Readiness of Online Learning (SROL) questionnaire. Results show substantial differences between the two groups of students, with U.S. students being more ready for online learning. The results and limitations were discussed, and practical implications and further ideas were provided.","10.3390/educsci10110313"," 2552","0.3027778"
"416","hot_topic_cluster_no_2","rank_4","427","Adherence to epidemiological measures and related knowledge and attitudes during the coronavirus disease 2019 epidemic in Croatia: a cross-sectional study","Aim To assess the use of personal protective equipment (PPE) and related knowledge and attitudes during the coronavirus disease 2019 epidemic in Croatia. Methods The online survey, conducted on social media in May 2020, yielded 1393 responses across the country (66% from the Adriatic area). The questionnaire consisted of socio-demographic questions and questions on the knowledge, attitudes, and behaviors related to PPE use. The chi(2) test, t test, and multivariate logistic regression were used in data analysis. Results As many as 84.0% of participants reported the compliance with social distancing measures, while 52.8% reported using PPE (mask and/or gloves) when shopping or visiting friends and family. Participants demonstrated good knowledge (mean of 10.4 [95% CI 10.3-10.4] correct answers out of 13 questions) and neutral to moderately positive attitude about PPE use (mean of 36.6 [36.1-37.1] out of 50 points). Participants with higher education, women, and health care workers had a greater probability for having a high knowledge score. Women, older individuals, public transport users, people with more positive PPE use attitude, and those who complied with social distancing had a higher probability of PPE use, while health care workers and highly educated participants had a reduced probability of PPE use in public. Conclusions Croatians had good knowledge and neutral to moderately positive attitudes about PPE use. Nevertheless, health authorities need to promote positive attitudes about PPE use in order to retain trust and compliance with epidemiological measures.","10.3325/cmj.2020.61.508","  363","0.2403846"
"417","hot_topic_cluster_no_2","rank_4","427","Does Too Much News on Social Media Discourage News Seeking? Mediating Role of News Efficacy Between Perceived News Overload and News Avoidance on Social Media","Drawing upon Bandura's self-efficacy theory, this study conceptualizes ""social media news efficacy"" and examines how news efficacy connects perceived news overload on social media to news avoidance and social filtering. Findings from a two-wave panel survey of South Korean adults show that news overload is significantly related to a decrease of news efficacy, which in turn increases news avoidance on social media. The analysis also finds that news efficacy mediates the positive link between perceived news overload and social filtering over time.","10.1177/2056305119872956","24577","0.2434783"
"418","hot_topic_cluster_no_2","rank_4","427","Psychological status and behavior changes of the public during the COVID-19 epidemic in China","Background A cluster of pneumonia cases were reported by Wuhan Municipal Health Commission, China in December 2019. A novel coronavirus was eventually identified, and became the COVID-19 epidemic that affected public health and life. We investigated the psychological status and behavior changes of the general public in China from January 30 to February 3, 2020. Methods Respondents were recruited via social media (WeChat) and completed an online questionnaire. We used the State-Trait Anxiety Inventory, Self-rating Depression Scale, and Symptom Checklist-90 to evaluate psychological status. We also investigated respondents' behavior changes. Quantitative data were analyzed byt-tests or analysis of variance, and classified data were analyzed with chi-square tests. Results In total, 608 valid questionnaires were obtained. More respondents had state anxiety than trait anxiety (15.8% vs 4.0%). Depression was found among 27.1% of respondents and 7.7% had psychological abnormalities. About 10.1% of respondents suffered from phobia. Our analysis of the relationship between subgroup characteristics and psychological status showed that age, gender, knowledge about COVID-19, degree of worry about epidemiological infection, and confidence about overcoming the outbreak significantly influenced psychological status. Around 93.3% of respondents avoided going to public places and almost all respondents reduced Spring Festival-related activities. At least 70.9% of respondents chose to take three or more preventive measures to avoid infection. The three most commonly used prevention measures were making fewer trips outside and avoiding contact (98.0%), wearing a mask (83.7%), and hand hygiene (82.4%). Conclusions We need to pay more attention to public psychological stress, especially among young people, as they are likely to experience anxiety, depression, and psychological abnormalities. Different psychological interventions could be formulated according to the psychological characteristics of different gender and age groups. The majority of respondents followed specific behaviors required by the authorities, but it will take time to observe the effects of these behaviors on the epidemic.","10.1186/s40249-020-00678-3","10686","0.2572464"
"419","hot_topic_cluster_no_2","rank_4","427","Exploring Information Seeking Anxiety among Research Students in Pakistan","This study explored information seeking anxiety among 31 Pakistani university research students using the critical incident technique. Face to face interviews were conducted for data collection by visiting the participants in their departments. The results indicated that information seeking anxiety among Pakistani research students manifested in eight dimensions, namely: (a) procedural anxiety, (b) information overload, (c) resource anxiety, (d) library anxiety, (e) competence anxiety, (f) ICT anxiety, (g) language anxiety, and (h) thematic anxiety. These participants also exhibited certain avoidance behaviours, search avoidance, task avoidance, and even research avoidance, along with inferiority complex. The results provided useful insights that could be used as a guide by information professionals, especially those engaged in managing information literacy instruction. In addition, this research would make a worthwhile contribution to the existing research on information behaviour in general and information seeking anxiety in particular.","10.1515/libri-2015-0047","59996","0.2944444"
"420","hot_topic_cluster_no_2","rank_4","427","Anxiety, pandemic-related stress and resilience among physicians during the COVID-19 pandemic","Background Physicians play a crucial frontline role in the COVID-19 pandemic, which may involve high levels of anxiety. We aimed to investigate the association between pandemic-related stress factors (PRSF) and anxiety and to evaluate the potential effect of resilience on anxiety among physicians. Methods A self-report digital survey was completed by 1106 Israeli physicians (564 males and 542 females) during the COVID-19 outbreak. Anxiety was measured by the 8-item version of the Patient-Reported Outcomes Measurement Information System. Resilience was evaluated by the 10-item Connor-Davidson Resilience Scale. Stress was assessed using a PRSF inventory. Results Physicians reported high levels of anxiety with a mean score of 59.20 +/- 7.95. We found an inverse association between resilience and anxiety. Four salient PRSF (mental exhaustion, anxiety about being infected, anxiety infecting family members, and sleep difficulties) positively associated with anxiety scores. Conclusions Our study identified specific PRSF including workload burden and fear of infection that are associated with increased anxiety and resilience that is associated with reduced anxiety among physicians.","10.1002/da.23085"," 6777","0.3171053"
"421","hot_topic_cluster_no_2","rank_5","139","Exploring Students' Knowledge and Practice of Digital Citizenship in Higher Education","The purpose of this study is to investigate undergraduate students' knowledge and practice of eight of the nine elements of digital citizenship: digital commerce, digital communication, digital literacy, digital etiquette, digital law, digital rights and responsibility, digital health and wellness and digital security. The sample population for this descriptive study comprised 204 undergraduate students selected by purposeful sampling. The results show that undergraduate students have an insufficient level of knowledge about good digital citizenship. A significant number of undergraduate students do observe eight digital citizenship elements through several ethical practices; however, the study revealed several concerns among participants in regards to security and safety, such as verifying the reliability and credibility of digital resources, checking the accuracy of information on the Internet, interpreting laws and penalties related to using digital resources, reporting irresponsible behaviour to the appropriate authorities and limiting the time and duration of daily digital device use. The results of this study show that an individual's extent of experience using the Internet is not a factor that affects the level of knowledge and practice of digital citizenship among undergraduate students. Conversely, the nature of academic specialisation, particularly technology-heavy courses (e.g. Educational Technology), are among the factors that affect the knowledge and practice of good digital citizenship. This article offers several recommendations for future study, policy development and practice.","10.3991/ijet.v15i19.15611","16803","0.2317757"
"422","hot_topic_cluster_no_2","rank_5","139","Digital Citizenship in Ontario Education: A Concept Analysis","Digital citizenship indicates one's place in digitized society; however academics have not established a cohesive understanding about how digital citizenship is characterized. The Ontario Ministry of Education also does not provide a central conceptualization of digital citizenship and instead encourages Ontario school boards to construct and communicate ideas of digital citizenship. Accordingly, Ontario policymakers, educators, and students use differing understandings of digital citizenship, which ultimately impedes educational initiatives and hinders the overall development of the concept. For this paper, therefore, I inquired as to how Ontario public school boards portray digital citizenship. Using concept analysis, I examined digital citizenship documents from the 10 largest English Ontario public school boards. The results suggest that digital citizenship is predominately characterized by responsible and ethical technology use. I conclude with a discussion about how this representation relates to democratic citizenship more broadly and the implications this may have on youth civic engagement.",""," 5364","0.2513514"
"423","hot_topic_cluster_no_2","rank_5","139","Validation of a digital intelligence quotient questionnaire for employee of small and medium-sized Thai enterprises using exploratory and confirmatory factor analysis","Purpose The purpose of this paper is to develop a digital intelligence quotient (DIQ) scale questionnaire that encompasses the digital identity, digital use, digital safety, digital security, digital emotional intelligence, digital communication, digital literacy and digital rights. Design/methodology/approach DIQ research was conducted in two phases to develop an assessment scale. First, 33 questions were developed based on previous DIQ concepts and theories. These questions were then validated using exploratory factor analysis into eight dimensions as digital identity, digital use, digital safety, digital security, digital emotional intelligence, digital communication, digital literacy and digital rights. A survey was conducted comprising 409 admins and clerks in SMEs. Second, confirmatory factor analysis and convergent validity were tested along the eight digital dimensions. Findings This study extended the DIQ concept to provide theoretical contribution for DIQ with intelligence study. Eight dimensions were developed to measure DIQ, including aspects of digital identity, digital use, digital safety, digital security, digital emotional intelligence, digital communication, digital literacy and digital rights. Research limitations/implications The DIQ questionnaire was a single-source, self-assessed data collection, as the sample included only employees of SMEs in Thailand. Results showed a good fit but require further refinement and validation using a larger sample size and various supplementary sampling contexts. Practical implications The eight DIQ dimensions and questionnaire results will assist organisations and supervisors to focus on employees' DIQ using both work and lifestyle parameters. This knowledge will help supervisors to encourage employees to increase their DIQ for more effective usage of digital literacy. Researchers and academics will be able to apply this instrument in future studies. Originality/value The DIQ questionnaire is a new instrument which comprehensively explores relevant dimensions to increase employee understanding of digital identity, digital use, digital safety, digital security, digital emotional intelligence, digital communication, digital literacy and digital rights.","10.1108/K-01-2019-0053","16138","0.2567416"
"424","hot_topic_cluster_no_2","rank_5","139","Screening outcome for consecutive examinations with digital breast tomosynthesis versus standard digital mammography in a population-based screening program","Objectives To retrospectively investigate early performance measures of digital breast tomosynthesis (DBT) versus standard digital mammography (DM) for consecutive screening rounds. Methods We included information about 35,736 women screened in BreastScreen Norway, 2008-2016, with at least two consecutive screening examinations. The pair of two consecutive screening examinations was the unit of analysis, and results from the subsequent examination were the measure of interest. Screening technique changed during the study period, resulting in four study groups: DM after DM, DBT after DM, DM after DBT, and DBT after DBT. We compared selected early performance measures between the study groups. Results Recall for DM after DM was 3.6% and lower for all other study groups (p < 0.001). The rate of screen-detected breast cancer was 4.6/1000 for DM after DM; for DBT after DM and DBT after DBT, it was 9.9/1000 and 8.3/1000, respectively (p < 0.001 relative to DM after DM), and for DM after DBT 4.3/1000. The rate of tubular carcinoma was higher for DBT after DBT or after DM compared with DM after DM (p < 0.01). The rate of histologic grade 1 tumors was higher for DBT after DM compared with DM after DM (p < 0.001). We did not observe any statistical difference in the interval cancer rates. Conclusions Lower recall and higher cancer detection rates for screening with DBT were sustainable over two consecutive screening rounds. Positive predictive values were higher for DBT than DM. There were no differences in the interval cancer rates between the study groups. Key Points There is limited knowledge about early performance measures for screening with digital breast tomosynthesis beyond one screening round. A decline in recall rate and an incline in the rate of screen-detected breast cancer were observed for women screened with DBT compared with DM, irrespective of prior screening technique. The interval breast cancer rate did not differ statistically for women screened with DBT versus DM. Tumor characteristics tended to be prognostic favorable for DBT compared with DM with no differences in rates of more advanced cancers. The clinical significance of increased cancer detection and the potential for future mortality reduction remain unknown.","10.1007/s00330-019-06264-y","19545","0.2618357"
"425","hot_topic_cluster_no_2","rank_5","139","Defining digital sustainability","This paper investigates what is meant by digital sustainability and establishes that it encompasses a range of issues and concerns that contribute to the longevity of digital information. A significant and integral part of digital sustainability is digital preservation, which has focused on one technical concern after another as issues and fashions have shifted over the last twenty years. Digital sustainability is demonstrated as providing an appropriate context for digital preservation because it requires consideration of the overall life cycle, technical, and socio-technical issues associated with the creation and management of digital items.","","86773","0.2689655"
"426","hot_topic_cluster_no_3","rank_1","59","A model of sustainable household technology acceptance","There is an expanding range of technologies used in a residential setting to enable sustainable living, including 'smart' technology that uses learning and connectivity to modify household behaviours. Understanding what drives the adoption of sustainable household technology will allow product developers, marketers and policy makers to use technology to reduce the environmental impact of homes. Based on the United Theory of Acceptance and Use of Technology, a model that explains predictors of intention to adopt sustainable household technology was developed and tested via an online survey of 592 US consumers. The results from structural equation modelling demonstrate that product attributes of sustainable household technology including performance, compatibleness and hedonic expectancy as well as consumer characteristics, in specific, sustainable innovativeness significantly predicts adoption intent. Conversely, the model testing shows that effort expectancy as well as social pressure and environmentalism are not significant predictors of adoption intention Theoretical and practical implications are discussed.","10.1111/ijcs.12217","62023","0.3027027"
"427","hot_topic_cluster_no_3","rank_1","59","South African millennials' acceptance and use of retail mobile banking apps: An integrated perspective","Mobile banking is gaining prominence as an innovative delivery channel for financial services, especially in developing countries where access to banking services is low. Mobile banking through an application (app) is a recent technological innovation with enormous potential to improve the banking experience of retail bank customers and to streamline the banks' operations. So retail banks are interested in promoting the rapid acceptance of this innovation among their customers. Millennials, who rely heavily on technology, are more likely to adopt mobile banking apps; yet there is scant research on the acceptance and use of mobile banking apps among this cohort, particularly from the perspective of African emerging economies. This study analyses the determinants of mobile banking app acceptance and use from a sample of millennial retail banking customers. An innovative multi-perspective framework is used, based on the unified theory of acceptance and use of technology 2 (UTAUT2), multi-dimensional institution-based trust, and risk. The findings suggest that performance expectancy, facilitating conditions, habit, perceived risk, and institution-based trust are significantly associated with millennials' intention to adopt mobile banking apps, and that facilitating conditions, perceived risk, and behavioural intention have a significant direct influence on millennials' mobile banking app behaviour. These findings bridge an important gap in research into millennials' mobile banking behaviour, and provide retail banks with useful insights to increase the uptake of mobile banking apps by this consumer cohort.","10.1016/j.chb.2020.106405"," 4620","0.3118644"
"428","hot_topic_cluster_no_3","rank_1","59","Understanding the intention to use mobile banking by existing online banking customers: an empirical study","The Indian banking sector can take advantage of the proliferation of smartphones as well as the government's encouragement of cashless transactions to accelerate the use of mobile and online banking. The purpose of this study is to understand the initial acceptance of mobile banking by existing online banking users. Few studies have focused on online banking users' behavioural intention to use similar services (such as mobile banking) in India. To this end, a theoretical model was developed using the technology acceptance model, which was extended to cover the adoption factors that influence users of online banking to use mobile banking. These adoption factors comprise perceived ease of use, perceived security, mobile self-efficacy, social influence and customer support. The dependent variable is customers' behavioural intention to use mobile banking. A partial least squares structural equation modelling analysis was used to test the theoretical model with sample data from 420 online banking customers of various public, private, foreign and co-operative banks in India. The study found that the adoption factors had a significant impact on customers' behavioural intention to use mobile banking. The findings of this study provide insight into digital banking channels, contribute to existing research on digital banking adoption and will educate banks and financial institutions on the adoption of mobile banking in India.","10.1057/s41264-020-00074-w"," 9156","0.3126214"
"429","hot_topic_cluster_no_3","rank_1","59","Adoption factors of cleaner production technology in a developing country: energy efficient lighting in Malaysia","To address the environmental impact of lighting systems, new technologies, such as light-emitting diodes, are gaining interest, as they are more energy efficient and result in lower carbon emissions than traditional lighting methods. However, adoption of new energy-efficient technologies is slow, resulting in delay in the decrease of ongoing damage to the environment. This study investigates factors that may play significant roles in successful adoption of light-emitting diodes-based lighting in Malaysia. In defining these factors the modified Unified Theory of Acceptance and Use of Technology is used. Relationships between factors for light-emitting diodes purchase decision were examined using path analysis, and a research model of influential factors is presented. Results from a survey of 221 respondents from multiple cities in Malaysia were used to test the model and hypotheses. The Partial Least Square test was chosen to quantitatively evaluate the impact of the key constructs of the modified Unified Theory of Acceptance and Use of Technology model: behavioral intention, effort expectancy, facilitating conditions, performance expectancy and social influence. The results confirmed that a modified Unified Theory of Acceptance and Use of Technology model could be used to determine behavioral intention of consumers and predict adoption of light-emitting diodes technology in Malaysia. Results show that Performance expectancy, Effort expectancy, Social influence, Facilitating conditions and Behavioral intention are the main factors in adoption of light emitting diodes-based lighting in Malaysia. ","10.1016/j.jclepro.2016.05.070","57394","0.3357143"
"430","hot_topic_cluster_no_3","rank_1","59","Extending the UTAUT model to understand the customers' acceptance and use of internet banking in Lebanon A structural equation modeling approach","Purpose - A number of studies have shown that internet banking (IB) implementation is not only determined by banks or government support, but also by perceptions and experience of IB users. IB studies have showed encouraging results from academics in developed countries. Yet little is known about the user adoption of IB in Lebanon. The purpose of this paper is to investigate the factors that may hinder or facilitate the acceptance and usage of IB in Lebanon. Design/methodology/approach - A conceptual framework was developed through extending the unified theory of acceptance and use of technology (UTAUT) by incorporating two additional factors namely; perceived credibility (PC) and task-technology fit (TTF). A quantitative approach based on cross-sectional survey was used to collect data from 408 IB consumers. Data were analysed using structural equation modelling based on AMOS 20.0. Findings - The results of the structural path revealed that performance expectancy (PE), social influence, PC and TTF to be significant predictors in influencing customers' behavioural intention (BI) to use IB and explained 61 per cent of its variance, with PE was found the strongest antecedent of BI. Contrary to the UTAUT, the effect of effort expectancy on BI was insignificant. In addition, both BI and facilitating conditions were found to affect the actual usage behaviour and explained 64 per cent of its variance Practical implications - This study would be helpful for bank managers and policy makers to explain the currently relatively low penetration rate of IB in formulating strategies to encourage the adoption and acceptance of IB by Lebanese customers, where IB is still considered an innovation. Originality/value - This study is the first research that extend the UTAUT by incorporating two additional factors namely; PC and TTF to study the IB in the Lebanese context. This study contributes to the research on computer technology usage by looking at IB adoption and incorporation into the lives of customers via the BI to use and actual usage of IB in Lebanon.","10.1108/ITP-02-2014-0034","61339","0.3509677"
"431","hot_topic_cluster_no_3","rank_2","4","SUSTAINABLE SOFTWARE ENGINEERING EDUCATION CURRICULA DEVELOPMENT","Climate change risk and environmental degradation are the most critical issues of our society. Our technology influenced daily life style involves many software and apps which are used by large society and their use is increasing than ever before. Sustainability is a significant topic for future professionals and more so for Information Technology (IT) professionals and software engineers due to its impact on the society. It is significant to motivate and raise concern among students and faculty members regarding sustainability by including it into Software Engineering curriculum.","","17557","0.2117647"
"432","hot_topic_cluster_no_3","rank_2","4","The Contribution of ICT Adoption to the Sustainable Information Society","This article aims to advance information society research and practice by examining and understanding the information and communication technologies (ICT) adoption in enterprises for improving the sustainable information society (SIS). This study employs a quantitative approach to investigate how enterprises adopt ICT, and how this adoption influences different types of sustainability and improves the sustainable information society. The survey questionnaires were used, and data collected from 396 enterprises were analyzed to understand the correlations between the ICT adoption and the sustainability in the SIS. The research findings reveal that the ICT quality, ICT management, and information culture have a significant impact on the sustainability in the SIS, whereas the outlay on ICT does not have such an impact. This study advances the information society research and practice by developing a model to depict the dimensions of ICT adoption and their impact on different types of sustainability in the SIS.","10.1080/08874417.2017.1312635","28290","0.2149254"
"433","hot_topic_cluster_no_3","rank_2","4","Information Technology and Sustainability in the Information Society","The sustainability concept has developed in a policy context. Its main relevance has been in policy forums such as the United Nations Conference on Environment and Development and the United Nations Conference on Sustainable Development. In the realm of information and communication technologies (ICTs), sustainability has played a policy role in the context of the World Summit on the Information Society. This article asks: How can we think of sustainability and ICTs in the context of a critical theory of society? How is the sustainability of ICTs related to capitalism and class? It provides a critique of the dominant reductionist and dualistic understandings of information technology sustainability in an information society context. The question that arises in this context is whether, from a critical theory perspective, the sustainability concept should be discarded. The view advanced in this article is that a critical social theory should provide an ideology critique of information technology sustainability; at the same time, it should not discard, but transform, the sustainability concept into a critical notion of un/sustainable information technology sustainability in the context of the information society.","","55042","0.2179487"
"434","hot_topic_cluster_no_3","rank_2","4","SUSTAINABLE DEVELOPMENT, TECHNOLOGICAL AND INDUSTRIAL IMPACTS ON ENGINEERING EDUCATION","The past industrial revolutions had negative effects on our world especially on environmental and social aspects. Hence, our societies must be able to steer the continued industrial revolution into the direction of sustainability. In particular, the current industrial revolution relies on the technologies of the Internet of Things, which open the ways to the development of sustainable solutions in order to meet the needs of the present without compromising the needs of the future. In the transition towards a sustainable society, teaching sustainability is necessary to ensure sustainable design and preserve the ecosystem. Consequently, educating engineering students on sustainable development is wide spreading and is actually taking place worldwide in many modern faculties and universities. This article examines the teaching methods for a sustainability subject and builds on the experience of others and a wide spectrum of methods in order to provide guidelines for curriculum design. The design is based on innovations in technologies to cover sustainability along with environmental and social implications. The article also provides a criterion for evaluating the impact of executing the proposed sustainable development curriculum.","10.7906/indecs.16.2.3","41563","0.2393939"
"435","hot_topic_cluster_no_3","rank_2","4","Measuring sustainability through ecological sustainability and human sustainability: A machine learning approach","Nowadays, sustainability is recognized as one of the most important development paradigms and included in the international and national strategies of almost all organizations. Sustainability assessment methods have been important for monitoring sustainability performance. These methods are developed to help decision-makers in their attempts to make society more sustainable. Various methods have been proposed for assessing the sustainability performance, however, this research is the first attempt to employ fuzzy clustering and supervised machine learning techniques to country sustainability assessment. This research tries to extend the previous sustainability assessment systems by the use of these techniques to reveal the relationships between human sustainability, ecological sustainability and overall sustainability performance by discovering the decision rules. The decision rules discovered from the Sustainability Assessment by Fuzzy Evaluation data of 128 countries are used to predict the country sustainability performance. The method proposed in this paper is flexible to accept large number indicators of sustainability to be used in the assessment of countries sustainability. The results of our analysis on countries sustainability data showed that the proposed method has potential to be used as a decision-making tool for sustainability assessment through a large set of indicators. ","10.1016/j.jclepro.2019.118162","18874","0.2472973"
"436","hot_topic_cluster_no_3","rank_3","522","THE USE OF SOCIAL MEDIA BY COMPANIES LISTED ON THE CORPORATE SUSTAINABILITY INDEX","The corporate image is one of the most valuable resources for business sustainability. Considering a scenario of economic recession and greater demand for companies' social role, it is necessary to incorporate values of socio-environmental responsibility. The purpose of this study is to analyze the use of social media by companies listed on the Corporate Sustainability Index (ISE, in portuguese), in order to verify the alignment between the message transmitted by these companies with the principles of corporate governance and corporate sustainability. Posts made during the period of one year on the main social networks (Facebook, Instagram and Youtube) of the 28 (twenty eight) companies that make up the ISE portfolio were analyzed. The results found indicate that Youtube is the channel where corporate responsibility messages are more frequent and have a higher volume of accesses. Regarding the engagement of companies listed on the ISE, companies in the financial sector stand out.","10.23925/2179-3565.2020v11i3p62-72","11239","0.3016393"
"437","hot_topic_cluster_no_3","rank_3","522","Corporate social responsibility and challenges for corporate sustainability in first part of the 21st century","The term corporate sustainability represents a crucial part of the concept of sustainable development. The main goal of the companies, which adopted corporate sustainability practice is to fulfil a sustainable development agenda and brings a balanced approach to economic and social progress and environmental management. The purpose of the article is to highlight the importance and issues of corporate sustainability in the context of awareness and approaches to social responsibility in the Fourth Industrial Revolution. The paper aims to make order in the plurality of definitions of concept ""corporate sustainability"" and its practical understanding in terms of ""corporate social responsibility"". It does so through a literature review and the analysis of different attempts of definition. It identifies the main challenges these concepts bring with them.","10.13128/cambio-8486","16704","0.3227273"
"438","hot_topic_cluster_no_3","rank_3","522","The Nature of the Relationship Between Corporate Identity and Corporate Sustainability: Evidence from The Retail Industry","This article addresses the nature of the interface between corporate sustainability and corporate identity at both the strategic and instrumental levels. We developed an empirical qualitative study in two countries in Southern Europe addressing retailers who are actively engaged in pursuing corporate-sustainability strategies. Data sources include in-depth interviews, observations, and physical artifacts of identity (digital and printed documents). Findings reveal that, at a strategic level, corporate sustainability is embedded in corporate identity reflecting the company's strategy. Companies also instrumentally use corporate identity to operationalize corporate sustainability strategies. Organizations show different patterns in the way they bridge corporate sustainability and identity. The contributions of this article are threefold: it reports the symbiotic relationship between corporate sustainability and corporate identity; it scrutinizes how corporate sustainability and corporate identity are integrated at the strategic and operational levels; and it establishes distinct patterns at the interface of corporate sustainability and corporate identity.","10.1017/beq.2017.15","45874","0.3379747"
"439","hot_topic_cluster_no_3","rank_3","522","Sustainable planning of e-waste recycling activities using fuzzy multicriteria decision making","This paper develops a new approach to sustainable planning of e-waste recycling activities for meeting the best sustainability interests of an e-recycling company. A fuzzy multicriteria decision making algorithm is developed to evaluate alternative recycling activities of an e-waste recycling job in terms of their corporate sustainability performance on identified sustainability criteria under the environmental, economic, and social dimensions. A series of optimal weighting models are developed to determine the optimal weights for the three sustainability dimensions and their associated criteria. In consideration of an e-recycling company's subjective sustainability preferences, the optimal weights reflect the best corporate sustainability performance of the e-waste products processed by the company under its current operational settings. The approach represents an original contribution to the methodological development of weighting the three corporate sustainability dimensions for planning decisions. In practice, it provides e-recycling companies with a proactive mechanism for incorporating the concept of corporate sustainability into their regular planning decisions. An empirical study on a leading e-recycling company is conducted to illustrate how the approach works. ","10.1016/j.jclepro.2013.03.003","73891","0.3533333"
"440","hot_topic_cluster_no_3","rank_3","522","Smart technologies and corporate sustainability: The mediation effect of corporate sustainability strategy","The focus of this paper is on the relation between smart technologies and corporate sustainability. Specifically, the purpose of this paper is to empirically examine the mediating role of corporate sustainability strategy between smart technologies and corporate sustainability. By building on a survey of 280 SMEs, the results show that corporate sustainability strategy fully mediates the relation between smart technologies and environmental sustainability, and smart technologies and social sustainability. Moreover, smart technologies have a direct significant influence on economic sustainability, but the relationship is also partly mediated by corporate sustainability strategy. Smart technologies do not have a direct influence on environmental or social sustainability. ","10.1016/j.compind.2019.03.003","26098","0.3673077"
"441","hot_topic_cluster_no_3","rank_4","8","Sharing economy: a bibliometric analysis of the state of research","The sharing economy has received increased attention in entrepreneurship research, resulting in a complex research landscape that is hard to overlook. Using a bibliometric analysis, we aim to further synthesise the field by: 1) summarising the most important definitions given by extant literature to capture the common understanding of the sharing economy; 2) identifying three thematic clusters based on the top 20 most cited publications; 3) conducting a citation analysis to show interdependencies between all authors; and 4) identifying the research methods used in the SE publications. Our results show: 1) many definitions with different emphases; 2) conceptualisation, collaborative consumption/ownership and the disruptive character of the sharing economy as three dominant research clusters; 3) a fairly even citation practice allowing for unbiased future research; and 4) that conceptual publications and quantitative as well as qualitative studies are fairly evenly published.","","16151","0.2462963"
"442","hot_topic_cluster_no_3","rank_4","8","A SCIENTOMETRIC ANALYSIS OF THE EMERGING TOPICS IN GENERAL COMPUTER SCIENCE","Citations have been an acceptable journal performance metric used by many indexing databases for inclusion and discontinuation of journals in their list. Therefore, editorial teams must maintain their journal performance by increasing article citations for continuous content indexing in the databases. With this aim in hand, this study intended to assist the editorial team of the Journal of Information and Communication Technology (JICT) in increasing the performance and impact of the journal. Currently, the journal has suffered from low citation count, which may jeopardise its sustainability. Past studies in library science suggested a positive correlation between keywords and citations. Therefore, keyword and topic analyses could be a solution to address the issue of journal citation. This article described a scientometric analysis of emerging topics in general computer science, the Scopus subject area for which JICT is indexed. This study extracted bibliometric data of the top 10% journals in the subject area to create a dataset of 5,546 articles. The results of the study suggested ten emerging topics in computer science that can be considered by the journal editorial team in selecting articles and a list of highly used keywords in articles published in 2019 and 2020 (as of 15 April 2020). The outcome of this study might be considered by the JICT editorial team and other journals in general computer science that suffer from a similar issue.",""," 3913","0.2511628"
"443","hot_topic_cluster_no_3","rank_4","8","Corporate Social Responsibility (CSR): A Survey of Topics and Trends Using Twitter Data and Topic Modeling","Corporate social responsibility (CSR) is an essential business practice in industry and a popular topic in academic research. Several studies have attempted to understand topics or categories in CSR contexts and some have used qualitative techniques to analyze data from traditional communication channels such as corporate reports, newspapers, and websites. This study adopts computational content analysis for understanding themes or topics from CSR-related conversations in the Twitter-sphere, the largest microblogging social media platform. Specifically, a probabilistic topic modeling-based computational text analysis framework is introduced to answer three questions: (1) What CSR-related topics are being communicated in the Twitter-sphere and what are the prevalent topics or themes in CSR conversation? (topic prevalence); (2) How are those topics interrelated? (topic correlation); (3) How have those topics changed over time? (topic evolution). The topic modeling results are discussed, and the direction for future research is presented.","10.3390/su10072231","36559","0.2756757"
"444","hot_topic_cluster_no_3","rank_4","8","A bibliometric overview of International Journal of Machine Learning and Cybernetics between 2010 and 2017","International Journal of Machine Learning and Cybernetics (IJMLC) is one of the influential journals in the area of computer science, and it published its first issue in 2010. On the one hand, taking the 544 IJMLC publications between 2010 and 2017 as the research object, this paper uses bibliometric methods to study the citation characteristics, international cooperation and institutional cooperation, the author's cooperation rate and cooperation degree, geographical distribution of the IJMLC publications. On the other hand, CiteSpace and Vosviewer, two data visualization software tools, are used to make the comprehensive analysis of the co-occurrence of the author keywords of the IJMLC publications. The document co-citation clusters visualization and burst detection of keywords are also presented to explore the development of the research trends. The research results in this paper provide a basis for further improving the academic level and quality of the IJMLC.","10.1007/s13042-018-0875-9","23061","0.2800000"
"445","hot_topic_cluster_no_3","rank_4","8","Literature Trend Identification of Sustainable Technology Innovation: A Bibliometric Study Based on Co-Citation and Main Path Analysis","In the past 20 years, there have been increasingly more studies on sustainable technology innovation (STI), possessing a significance for sustainable development. This paper aims to provide a research landscape, since the systematic understanding of STI is still inadequate. Through bibliometric analysis, it explores the literature distribution characteristics and the literature citation network. Based on the relevant literature data in the Web of Science (WOS), the study visually analyzes the development trend, topic distribution, burst literature, and co-citation network of the research literature, and extracts the evolution path of literature citation by using the main path analysis method. Through the analysis of co-citation and main path, 13 clusters in the co-citation network are found, which are further extracted as the main path network containing 82 nodes. Furthermore, this paper summarized the bibliometric landscape and discussed the frontier STI research topics. The comprehensive framework contributes to the understanding of STI themes and identifying future research agenda.","10.3390/su12208664"," 4184","0.2849315"
"446","hot_topic_cluster_no_3","rank_5","226","Stakeholder engagement and dialogic accounting Empirical evidence in sustainability reporting","Purpose The purpose of this paper is to explain how sustainability reporting and stakeholder engagement processes serve as vehicles of dialogic accounting (DA), a form of critical accounting that creates opportunities for stakeholders to express their opinions, and the influence of dialogic interactions on the content of sustainability reports. Design/methodology/approach Content analysis is used to investigate reports published by 299 companies that have adopted Global Reporting Initiative guidelines. This paper studies how organizations engage stakeholders, the categories of stakeholders that are being addressed, the methods used to support stakeholder engagement, and other features of the stakeholder engagement process. Companies that disclose stakeholder perceptions, the difficulties met in engaging stakeholders, and actions aimed at creating opportunities for different groups of stakeholders to interact were subjects of discussion in a series of semi-structured interviews that focus on DA. Findings Companies often commit themselves to two-way dialogue with their stakeholders, but fully developed frameworks for DA are rare. However, signs of DA emerged in the analysis, thus confirming that sustainability reporting can become a platform for DA systems if stakeholder engagement is effective. Originality/value The findings contribute to the accounting literature by discussing if and how sustainability reporting and stakeholder engagement can serve as vehicles of DA. This is accomplished via a research design that is based on in-depth interviews and content analysis of various sustainability reports.","10.1108/AAAJ-09-2017-3158","25267","0.2038462"
"447","hot_topic_cluster_no_3","rank_5","226","Stakeholder involvement in Union Digital Centres in Bangladesh","This paper evaluates opportunities and challenges of the Union Digital Centre (UDC) project, a Bangladeshi telecentre initiative, based on a Public-Private-People's Partnership (PPPP) scheme. It investigates how each of the stakeholders contributes. In the face of many closures, it is a timely contribution. A stakeholder management framework focused on telecentres is used to understand stakeholder behaviour and to provide a hypothesis about the contribution of each to the operation. This study surveyed stakeholders such as entrepreneurs, central government officials and local government representatives. The roles of stakeholders are discussed and opportunities are identified for external support from government bodies to support the operation of UDCs. The potential demands and benefits of a massive client base are also examined. Application of a structural equation model highlights that current progress hinges more on external support and people's participation than on entrepreneurial investment. The dependency on philanthropic support, combined with problems in stakeholder engagement, poses challenges to entrepreneurship development, a feature necessary for sustainability. To engage them, effective interplay of relevant stakeholders is required.","10.1177/0266666919832069","10500","0.2089744"
"448","hot_topic_cluster_no_3","rank_5","226","Information Systems Interoperability in Public Administration: Identifying the Major Acting Forces through a Delphi Study","Information Systems (IS) interoperability in Public Administration (PA) is a main goal and a major challenge for PA professionals. Achieving interoperability among IS that are technologically disparate and that exist in different organizational contexts is a complex task, being affected by multiple aspects, not yet satisfactorily known and characterized. The aim of this paper is to unveil the forces that influence IS interoperability initiatives in PA. The inquiry was inspired by Lewin's Field Theory. The data generation process was based on a Delphi study involving 55 experts from PA, IS/IT industry, and academy. A set of 31 forces were identified and ranked based on the level of importance they assume in IS interoperability initiatives. Thirty eight propositions describing restraining and driving influences were also formulated, as well as 24 propositions about forces' configuration in the current context of Portuguese PA that represent the specific constellation of forces acting in IS interoperability initiatives in that country. The results of this study provide an understanding of the complex of forces acting in IS interoperability, contributing to improve the study, management, and implementation of these initiatives and, consequently, to the establishment of a PA with more adequate, sustained, and sustainable levels of interoperability.","10.4067/S0718-18762011000100006","80422","0.2217391"
"449","hot_topic_cluster_no_3","rank_5","226","Creating value through governing IT deployment in a public/private-sector inter-organisational context: a human agency perspective","Harnessing value from Information Technology (IT) has long been a focus of research, but evidence is lacking about how effective practice of Information Technology Governance (ITG) contributes to creating value for stakeholders in inter-organisational contexts. This is especially so for public/private sector partnerships. In this study we used ISO/IEC 38500:2008, the corporate governance of IT standard, to direct analysis of how ITG was practised in deployment of a large IT project in an inter-organisational public/private sector context. The findings demonstrate that ITG strategies related to human agency contribute to the realisation of value for participating stakeholders, particularly through pre-emptive stakeholder participation in evaluating IT functionality of the old system and iteratively in deployment of the new system. Further, our investigation shows that ISO/IEC 38500:2008 has merit as an analytical framework to objectively evaluate corporate governance of IT, although there is need for some enhancement.","10.1057/ejis.2012.21","73600","0.2507042"
"450","hot_topic_cluster_no_3","rank_5","226","A strategic framework for good governance through e-governance optimization","Purpose - The purpose of this paper is to attempt to find out whether the new information and communication technologies can make a significant contribution to the achievement of the objective of good governance. The study identifies the factors responsible for creating a conducive environment for effective and successful implementation of e-governance for achieving good governance and the possible barriers in the implementation of e-governance applications. Based on the comprehensive analysis it proposes a strategic policy framework for good governance in Punjab in India. Punjab is a developed state ranked amongst some of the top states of India in terms of per capita income and infrastructure. Design/methodology/approach - The study designs a framework for good governance by getting the shared vision of all stakeholders about providing good quality administration and governance in the Indian context through ""Participatory Stakeholder Assessment"". The study uses descriptive statistics, perception gap, ANOVA and factor analysis to identify the key factors for good governance, the priorities of public regarding e-services, the policy makers' perspectives regarding good governance to be achieved through e-governance. Findings - The study captures the good governance factors mainly contributing to the shared vision. The study further highlights that most Indian citizens in Punjab today believe in the power of information and communication technology (ICT) and want to access e-governance services. Major factors causing pain and harassment to the citizens in getting the services from various government departments include: unreasonable delay, multiple visits even for small services; poor public infrastructure and its maintenance in government offices. In the understanding of citizens the most important factors for the success of e-governance services are: overall convenience and experience of the citizens; reduction in the corruption levels by improvement in the transparency of government functioning and awareness about the availability of service amongst general masses. Originality/value - The present study has evolved a shared vision of all stakeholders on good governance in the Indian context. It has opened up many new possibilities for the governments, not only to use ICTs and help them in prioritizing the governance areas for focused attention, but also help to understand the mindset of the modern citizenry, their priorities and what they consider as good governance. The study will help policy makers focus on these factors for enhancing speedy delivery of prioritized services and promote good governance in developing countries similar to India.","10.1108/PROG-12-2013-0067","67538","0.2573171"
"451","hot_topic_cluster_no_4","rank_1","805","Prediction of Precipitation Based on Recurrent Neural Networks in Jingdezhen, Jiangxi Province, China","Precipitation is a critical input for hydrologic simulation and prediction, and is widely used for agriculture, water resources management, and prediction of flood and drought, among other activities. Traditional precipitation prediction researches often established one or more probability models of historical data based on the statistical prediction methods and machine learning techniques. However, few studies have been attempted deep learning methods such as the state-of-the-art for Recurrent Neural Networks (RNNs) networks in meteorological sequence time series predictions. We deployed Long Short-Term Memory (LSTM) network models for predicting the precipitation based on meteorological data from 2008 to 2018 in Jingdezhen City. After identifying the correlation between meteorological variables and the precipitation, nine significant input variables were selected to construct the LSTM model. Then, the selected meteorological variables were refined by the relative importance of input variables to reconstruct the LSTM model. Finally, the LSTM model with final selected input variables is used to predict the precipitation and the performance is compared with other classical statistical algorithms and the machine learning algorithms. The experimental results show that the LSTM is suitable for precipitation prediction. The RNN models, combined with meteorological variables, could predict the precipitation accurately in Jingdezhen City and provide sufficient time to prepare strategies against potential related disasters.","10.3390/atmos11030246","13936","0.2063830"
"452","hot_topic_cluster_no_4","rank_1","805","Prediction of the Electrical Strength and Boiling Temperature of the Substitutes for Greenhouse Gas SF6 Using Neural Network and Random Forest","Finding substitutes for sulfur hexafluoride (SF6), a gas with extremely high global warming potential, has been a persistent effort for years in the field of high voltage power equipment, which focuses on the evaluation of the electrical strength and boiling temperature for the practical purpose. Following up the previous proposed linear regression models, this work introduces machine learning algorithms including artificial neural network (ANN) and random forest (RF) as the potential approaches to predict the electrical strength and boiling temperature. Based on a series of descriptors derived from the molecular structure of 74 molecules, the performance of three different methods: multiple linear regression, artificial neural network and random forest are compared and assessed in terms of the sensitivity to the sample size, prediction accuracy and stability, and the interpretability of predictors. Considering the available data are limited, random forest shows superior performance with higher robustness and efficiency. The same approaches were applied to the boiling temperature and random forest produced better results as well. Besides, the variable importance ranked by RF improves understanding of the correlation between the molecular properties and electrical strength. It provides important insights to analyze the properties of the SF6 substitutes during the design and synthesis of the new eco-friendly gases in power equipment.","10.1109/ACCESS.2020.3004519","17174","0.2085366"
"453","hot_topic_cluster_no_4","rank_1","805","Predicting energy consumption in multiple buildings using machine learning for improving energy efficiency and sustainability","Buildings must be energy efficient and sustainable because buildings have contributed significantly to world energy consumption and greenhouse gas emission. Predicting energy consumption patterns in buildings is beneficial to utility companies, users, and facility managers because it can help to improve energy efficiency. This work proposed a Random Forests (RF) - based prediction model to predict the short-term energy consumption in the hourly resolution in multiple buildings. Five one-year datasets of hourly building energy consumption were used to examine the effectiveness of the RF model throughout the training and test phases. The evaluation results presented that the RF model exhibited a good prediction accuracy in the prediction. In four evaluation scenarios, the mean absolute error (MAE) values ranged from 0.430 to 0.501 kWh for the 1-step-ahead prediction, from 0.612 to 0.940 kWh for the 12-steps-ahead prediction, and from 0.626 to 0.868 kWh for the 24-steps-ahead prediction. The RF model was superior to the M5P and Random Tree (RT) models. The RF was better about 49.21%, 46.93% in the MAE and mean absolute percentage error (MAPE) than the RT model in forecasting 1-step-ahead building energy consumption. The RF model approved the outstanding performance with the improvement of 49.95% and 29.29% in MAE compared to the M5P model in the 12-steps-ahead, and 24-steps-ahead energy use, respectively. Thus, the proposed RF model was an effective prediction model among the investigated machine learning (ML) models. This study contributes to (i) the state of the knowledge by examining the generalization and effectiveness of ML models in predicting building energy consumption patterns; and (ii) the state of practice by proposing an effective tool to help the building owners and facility managers in understanding building energy performance for enhancing the energy efficiency in buildings. ","10.1016/j.jclepro.2020.121082"," 9064","0.2219298"
"454","hot_topic_cluster_no_4","rank_1","805","Development of an Improved Model to Predict Building Thermal Energy Consumption by Utilizing Feature Selection","Humans spend approximately 90% of the daytime in buildings, and greenhouse gases (GHGs) emitted by buildings account for approximately 20% of total GHG emissions. As the energy consumed during building operation from a building life-cycle perspective amounts to approximately 70-90% of the total energy, it is essential to accurately predict the energy consumption of buildings for their efficient operation. This study aims to optimize a model for predicting the thermal energy consumption of buildings by (i) first extracting major variables through feature selection and deriving significant variables in addition to the collected data and (ii) predicting the thermal energy consumption using a machine learning model. Feature selection using random forest was performed, and 11 out of 17 available data were selected. The accuracy of the prediction model was significantly improved when the hour of day variable was added. The prediction model was constructed using an artificial neural network (ANN), and the improvement in the prediction accuracy was analyzed by comparing different cases of variable combinations. The ANN prediction accuracy was improved by 15% using the feature selection process compared to when all data were used as input data, and 25% coefficient of variation of the root mean square error (CVRMSE) accuracy was achieved.","10.3390/en12214187","20353","0.2230769"
"455","hot_topic_cluster_no_4","rank_1","805","Prediction of Influent Flow Rate: Data-Mining Approach","this paper, models for short-term prediction of influent flow rate in a wastewater-treatment plant are discussed. The prediction horizon of the model is up to 180 min. The influent flow rate, rainfall rate, and radar reflectivity data are used to build the prediction model by different data-mining algorithms. The multilayer perceptron neural network algorithm has been selected to build the prediction models for different time horizons. The computational results show that the prediction model performs well for horizons up to 150 min. Both the peak values and the trends are accurately predicted by the model. There is a small lag between the predicted and observed influent flow rate for horizons exceeding 30 min. The lag becomes larger with the increase of the prediction horizon. ","10.1061/(ASCE)EY.1943-7897.0000103","74386","0.2403846"
"456","hot_topic_cluster_no_4","rank_2","227","Nondestructive gender identification of silkworm cocoons using X-ray imaging with multivariate data analysis","A rapid, reliable and nondestructive method for the gender discrimination of silkworm cocoons is of great importance for the production of high-quality silk by the mulberry silkworm industry. This study aimed to determine the feasibility of using soft X-ray imaging with multivariate data analysis to discriminate the gender of silkworm cocoons. X-ray images of silkworm cocoons were obtained and preprocessed and then the region of interest of the chrysalises was segmented. In total, 11 morphological characters of the chrysalises were extracted and compressed by principal component analysis to visualize cluster trends. In developing the discrimination classifiers, four different types of algorithm, including the K nearest neighbors, linear discriminant analysis, back propagation artificial neural network and support vector machine algorithms, were used comparatively; the performances of these models were optimized by cross-validation. The results indicate that the correct identification rates of these classifiers were all high and ranged from 92.57% (obtained via the support vector machine algorithm) to 93.68% (obtained via the K nearest neighbor algorithm). With respect to the running time, the linear discriminant analysis classifier had the best accuracy of 93.31%. These results indicate that this X-ray imaging technique with multivariate analysis could be used successfully for the screening of the gender of silkworm cocoons in the mulberry silkworm industry.","10.1039/c4ay00940a","71895","0.2000000"
"457","hot_topic_cluster_no_4","rank_2","227","Estimating urban impervious surfaces from Landsat-5 TM imagery using multilayer perceptron neural network and support vector machine","In recent years, the urban impervious surface has been recognized as a key quantifiable indicator in assessing urbanization impacts on environmental and ecological conditions. A surge of research interests has resulted in the estimation of urban impervious surface using remote sensing studies. The objective of this paper is to examine and compare the effectiveness of two algorithms for extracting impervious surfaces from Landsat TM imagery; the multilayer perceptron neural network (MLPNN) and the support vector machine (SVM). An accuracy assessment was performed using the high-resolution WorldView images. The root mean square error (RMSE), the mean absolute error (MAE), and the coefficient of determination (R-2) were calculated to validate the classification performance and accuracies of MLPNN and SVM. For the MLPNN model, the RMSE, MAE, and R-2 were 17.18%, 11.10%, and 0.8474, respectively. The SVM yielded a result with an RMSE of 13.75%, an MAE of 8.92%, and an R2 of 0.9032. The results indicated that SVM performance was superior to that of MLPNN in impervious surface classification. To further evaluate the performance of MLPNN and SVM in handling the mixed-pixels, an accuracy assessment was also conducted for the selected test areas, including commercial, residential, and rural areas. Our results suggested that SVM had better capability in handling the mixed-pixel problem than MLPNN. The superior performance of SVM over MLPNN is mainly attributed to the SVM's capability of deriving the global optimum and handling the over-fitting problem by suitable parameter selection. Overall, SVM provides an efficient and useful method for estimating the impervious surface. ","10.1117/1.3539767","80621","0.2000000"
"458","hot_topic_cluster_no_4","rank_2","227","Pre-processing of imbalanced samples and the effective contribution in fault diagnosis in wastewater treatment plants","Fault diagnosis by machine learning techniques is of great importance in wastewater treatment plants (WWTPs). A key factor influencing the accuracy of fault diagnosis lies in the imbalance between the sample data in minority classes (i.e. faulty situations) and that in majority classes (i.e. normal situations), which may cause misjudgments of faults and lead to failure in practical use. This study proposes a novel pre-processing method with a fast relevance vector machine (Fast RVM) reducing the data of majority class samples and the synthetic minority over-sampling technique expanding the minority class samples. A case study indicates that this pre-processing method could be a promising solution for imbalanced data classification in WWTPs and the pre-processed data can be well diagnosed by back-propagation neural networks, support vector machine, RVM and Fast RVM models.","10.2166/hydro.2017.206","47996","0.2000000"
"459","hot_topic_cluster_no_4","rank_2","227","Genetic Optimization of Energy Consumption of Pellet Shaft Furnace Combustor Based on Support Vector Machine (SVM)","Investigation on optimization of pellet shaft furnace based on the combination of genetic algorithm and support vector machine (SVM) is carried out. A SVM classifier model is developed to map the complex nonlinear relationship between operating parameters and the quality indexes of fired pellet, and a genetic algorithm is adapted in the energy optimization with the fitness function based on the SVM classifier model. This method can reduce the energy consumption while maintaining the fired pellet quality stable. The results show that the accuracy of the SVM classifier model is satisfied and the gas consumption can be reduced by 4% per ton of green pellets with this optimization method.","10.1515/ijcre-2013-0117","71073","0.2148936"
"460","hot_topic_cluster_no_4","rank_2","227","Land-Cover Classification of Coastal Wetlands Using the RF Algorithm for Worldview-2 and Landsat 8 Images","Wetlands are one of the world's most important ecosystems, playing an important role in regulating climate and protecting the environment. However, human activities have changed the land cover of wetlands, leading to direct destruction of the environment. If wetlands are to be protected, their land cover must be classified and changes to it monitored using remote sensing technology. The random forest (RF) machine learning algorithm, which offers clear advantages (e.g., processing feature data without feature selection and preferable classification result) for high spatial image classification, has been used in many study areas. In this research, to verify the effectiveness of this algorithm for remote sensing image classification of coastal wetlands, two types of spatial resolution images of the Linhong Estuary wetland in Lianyungang-Worldview-2 and Landsat-8 images-were used for land cover classification using the RF method. To demonstrate the preferable classification accuracy of the RF algorithm, the support vector machine (SVM) and k-nearest neighbor (k-NN) methods were also used to classify the same area of land cover for comparison with the results of RF classification. The study results showed that (1) the overall accuracy of the RF method reached 91.86%, higher than the SVM and k-NN methods by 4.68% and 4.72%, respectively, for Worldview-2 images; (2) at the same time, the classification accuracies of RF, SVM, and k-NN were 86.61%, 79.96%, and 77.23%, respectively, for Landsat-8 images; (3) for some land cover types having only a small number of samples, the RF algorithm also achieved better classification results using Worldview-2 and Landsat-8 images, and (4) the addition texture features could improve the classification accuracy of the RF method when using Worldview-2 images. Research indicated that high-resolution remote sensing images are more suitable for small-scale land cover classification image and that the RF algorithm can provide better classification accuracy and is more suitable for coastal wetland classification than the SVM and k-NN algorithms are.","10.3390/rs11161927","23694","0.2229508"
"461","hot_topic_cluster_no_4","rank_3","579","Conceptualizing big data practices","Purpose The purpose of this paper is to provide a conceptual understanding of Big Data practices in organizations, which will enable exploring the operational and strategic roles of Big Data in organizational performance. Design/methodology/approach Both academic and non-academic literature studies on Big Data were reviewed so as to capture what was known about Big Data practices. Qualitative interviews were conducted with firm executives about Big Data practices in their organizations. Both literature review and interview results were analyzed based on the dynamic capabilities perspective. Findings The analysis of the results suggests that Big Data capability develops when the resources parts of Big Data and the skill and competency parts are integrated and then grow into a dynamic capability. Research limitations/implications - This study contributes to the literature with the concept of Big Data capability that best characterizes Big Data practices in organizations. Validity of this concept should be tested in empirical studies. Originality/value The development of the concept of Big Data capability helps to fill a gap in the research literature that theoretical understanding of big data practices is lacking or needs to be updated. It motivates practitioners to develop this capability so as to create and maintain their strategic advantage.","10.1108/IJAIM-12-2018-0154","14411","0.1805195"
"462","hot_topic_cluster_no_4","rank_3","579","Adoption of Big Data analytics in construction: development of a conceptual model","Purpose Big Data (BD) is being increasingly used in a variety of industries including construction. Yet, little research exists that has examined the factors which drive BD adoption in construction. The purpose of this paper is to address this gap in knowledge. Design/methodology/approach Data collected from literature (55 articles) were analyzed using content analysis techniques. Taking a two-pronged approach, first study presents a systematic perspective of literature on BD in construction. Then underpinned by technology-organization-environment theory and supplemented by literature, a conceptual model of five antecedent factors of BD adoption for use in construction is proposed. Findings The results show that BD adoption in construction is driven by a number of factors: first, technological: augmented BD-BIM integration and BD relative advantage; second, organizational: improved design and execution efficiencies, and improved project management capabilities; and third, environmental: augmented availability of BD-related technology for construction. Hypothetical relationships involving these factors are then developed and presented through a new model of BD adoption in construction. Research limitations/implications - The study proposes a number of adoption factors and then builds a new conceptual model advancing theories on technologies adoption in construction. Practical implications - Findings will help managers (e.g. chief information officers, IT/IS managers, business and senior managers) to understand the factors that drive adoption of BD in construction and plan their own BD adoption. Results will help policy makers in developing policy guidelines to create sustainable environment for the adoption of BD for enhanced economic, social and environmental benefits. Originality/value This paper develops a new model of BD adoption in construction and proposes some new factors of adoption process.","10.1108/BEPAM-05-2018-0077","22357","0.1815789"
"463","hot_topic_cluster_no_4","rank_3","579","How Sustainable Is Big Data?","The rapid growth of big data provides tremendous opportunities for making better decisions, where better can be defined using any combination of economic, environmental, or social metrics. This essay provides a few examples of how the use of big data can precipitate more sustainable decision-making. However, as with any technology, the use of big data on a large scale will have some undesirable consequences. Some of these are foreseeable, while others are entirely unpredictable. This essay highlights some of the sustainability-related challenges posed by the use of big data. It does not intend to suggest that the advent of big data is an undesirable development. However, it is not too early to start asking what the unwanted repercussions of the big data revolution might be.","10.1111/poms.12837","35327","0.1829268"
"464","hot_topic_cluster_no_4","rank_3","579","Ethical Issues in the Big Data Industry","Big Data combines information from diverse sources to create knowledge, make better predictions and tailor services. This article analyzes Big Data as an industry, not a technology, and identifies the ethical issues it faces. These issues arise from reselling consumers' data to the secondary market for Big Data. Remedies for the issues are proposed, with the goal of fostering a sustainable Big Data Industry.(1,2)","","65255","0.1925926"
"465","hot_topic_cluster_no_4","rank_3","579","PHYSICAL EXAMS FOR THE CITY - APPLICATIONS FOR BIG DATA IN URBAN AND RURAL PLANNING","In this article, the interviewee, senior urban planner Peng Wang, suggests a broader definition for Big Data, considering which as a methodology. Combining introduction of case studies, he offers insights on the issues of how Big Data influences planners' practice, the misunderstandings in planning industry about Big Data, the obstacles in the research and application of Big Data, as well as Big Data's future development.","10.1007/slaf-0020-15004","64627","0.2576923"
"466","hot_topic_cluster_no_4","rank_4","211","Development of an Adaptive Approach for Precision Agriculture Monitoring with Drone and Satellite Data","For better agricultural productivity and food management, there is an urgent need for precision agriculture monitoring at larger scales. In recent years, drones have been employed for precision agriculture monitoring at smaller scales, and for past few decades, satellite data are being used for land cover classification and agriculture monitoring at larger scales. The monitoring of agriculture precisely over a large scale is a challenging task. In this paper, an approach has been proposed for precision agriculture monitoring, i.e., the classification of sparse and dense fields, which is carried out using freely available satellite data (Landsat 8) along with drone data. Repeated usage of drone has to be minimized and hence an adaptive classification approach is developed, which works with image statistics of the selected region. The proposed approach is successfully tested and validated on different spatial and temporal Landsat 8 data.","10.1109/JSTARS.2017.2746185","42935","0.1660714"
"467","hot_topic_cluster_no_4","rank_4","211","A Concept Paper on Smart River Monitoring System for Sustainability in River","River is a major source of water in Malaysia and one of the major threats to its sustainability is pollution. The existing methods for monitoring of water quality in rivers are manual monitoring and continuous monitoring. These methods are costly and less efficient. Hence, we propose a smart river monitoring system (SRMS) that uses unmanned aerial vehicles (UAVs) or drones and low power wide area (LPWA) communication technology. The Internet of Things (IoT) and data analytic are promising techniques which provide real-time monitoring and enhances efficiency. However, due to the span of river that needs to be monitored, conventional communication technology such as Wi-Fi, Zigbee, Bluetooth are not suitable. Hence, there is the need for LPWA communication technology. We discuss the application of LPWA and UAV for sustainability of rivers in Malaysia as a case study. Preliminary results show that the use of UAV will increase the efficiency of measuring the water quality parameters compared to manual monitoring method. Also, real-time monitoring enables us to study the changes in water quality. Finally, we provide future direction in the application of UAV and LPWA for sustainability in river.","10.30880/ijie.2018.10.07.012","40553","0.1701299"
"468","hot_topic_cluster_no_4","rank_4","211","Development of a Real-time Remote Structural Monitoring Scheme for Civil Infrastructural Systems","Long-term structural monitoring has become an important requisite to ensure structural and operational safety for critical civil infrastructures. Long-term structural monitoring by acquiring data continuously or at small intervals may be difficult to achieve by employing conventional on-site monitoring methods. In order to overcome these difficulties remote structural monitoring (RSM), an advanced structural monitoring technique, can be used to acquire continuous data from the remotely instrumented structure. RSM methodology requires an interdisciplinary approach integrating areas such as structural engineering, sensor technology, communication technology, statistical mechanics, information technology for online data transmission (over larger distances) and damage detection/health assessment of the structure. RSM has many advantages like, continuous monitoring, early alarm of any incipient damage, and data acquisition even in adverse climatic/environmental conditions. This article describes the development of an RSM scheme, developed at SERC, Chennai. The article outlines the laboratory and field investigations carried out to validate the developed scheme. Brief information about the observations and modifications made during these trials are also presented. A part of the work carried out for synthesizing online data using Auto Regressive Moving Average model is also presented.","10.1177/1475921709340973","83494","0.1883495"
"469","hot_topic_cluster_no_4","rank_4","211","Design of Early Warning System Based on Wireless Sensor Network","In order to solve the shortcomings of the landslide monitoring technology method, a set of landslides monitoring and early warning system is designed. It can achieve real-time sensor data acquisition, remote transmission and query display. In addition, aiming at the harsh environment of landslide monitoring and the performance requirements of the monitoring system, an improved minimum hop routing protocol is proposed. It can reduce network energy consumption, enhance network robustness, and improve node layout and networking flexibility. In order to realize the remote transmission of data, GPRS wireless communication is used to transmit monitoring data. Combined with remote monitoring center, real-time data display, query, preservation and landslide warning and prediction are realized. The results show that the sensor data acquisition system is accurate, the system is stable, and the node network is flexible. Therefore, the monitoring system has a good use value.","10.3991/ijoe.v14i01.8060","41975","0.1942857"
"470","hot_topic_cluster_no_4","rank_4","211","Smart SCADA system for urban air pollution monitoring","There are a number of different types of monitoring stations for checking air quality levels in urban environments. These monitoring stations usually just perform data acquisition of the measured values from the sensors and store them in the database. The processing of the measured results as well as the statistical analysis is mainly done in other places where the data come from various communication systems. Acquisition of the measured data is commonly done on-line while the processing and statistical analysis is performed off-line. As opposed to these measurement systems, this implemented device enables the acquisition and statistical processing of the measured data in real time and the results are instantly available to all users. The system indicates the air pollutants using the ARMA model. The transmission of information in the realized smart SCADA system is done by the Modbus protocol using shared variables which gives the whole system a stronger hierarchical structure. ","10.1016/j.measurement.2014.08.036","68534","0.2293103"
"471","hot_topic_cluster_no_4","rank_5","28","Enhanced-Adaptive Pattern Attack Recognition Technique (E-APART) Against EDoS Attacks in Cloud Computing","Cloud Computing is most widely used in current technology. It provides a higher availability of resources to greater number of end users. In the cloud era, security has develop a reformed source of worries. Distributed Denial of Service (DDoS) and Economical Denial of Sustainability (EDoS) are attacks that can affect the 'pay-per-use' model. This model automatically scales the resources according to the demand of consumers. The functionality of this model is to mitigate the EDoS attack by some tactical attacker/s, group of attackers or zombie machine network (BOTNET) to minimize the availability of the target resources, which directly or indirectly reduces the profits and increase the cost for the cloud operators. This paper presents a model called Enhanced-APART which is step further of the authors' previous model (APART) that can be used to mitigate the EDoS attack from the cloud platform and shows the nature of the attack. Enhanced-APART model offers pre-shared security mechanism to ensure the access of legitimate users on the cloud services. It also performs pattern analysis in order to detect the EDoS caused by BOTNET mechanism and includes time-based and key-sharing post-setup authentication scheme to prevent the replication or replay attacks and thus results in mitigation of EDoS attack.","10.4018/JCIT.2015070105","65023","0.1967033"
"472","hot_topic_cluster_no_4","rank_5","28","Learning under p-tampering poisoning attacks","Recently, Mahloujifar and Mahmoody (Theory of Cryptography Conference'17) studied attacks against learning algorithms using a special case of Valiant's malicious noise, called p-tampering, in which the adversary gets to change any training example with independent probability p but is limited to only choose 'adversarial' examples with correct labels. They obtained p-tampering attacks that increase the error probability in the so called 'targeted' poisoning model in which the adversary's goal is to increase the loss of the trained hypothesis over a particular test example. At the heart of their attack was an efficient algorithm to bias the expected value of any bounded real-output function through p-tampering. In this work, we present new biasing attacks for increasing the expected value of bounded real-valued functions. Our improved biasing attacks, directly imply improved p-tampering attacks against learners in the targeted poisoning model. As a bonus, our attacks come with considerably simpler analysis. We also study the possibility of PAC learning under p-tampering attacks in the non-targeted (aka indiscriminate) setting where the adversary's goal is to increase the risk of the generated hypothesis (for a random test example). We show that PAC learning is possible under p-tampering poisoning attacks essentially whenever it is possible in the realizable setting without the attacks. We further show that PAC learning under 'no-mistake' adversarial noise is not possible, if the adversary could choose the (still limited to only p fraction of) tampered examples that she substitutes with adversarially chosen ones. Our formal model for such 'bounded-budget' tampering attackers is inspired by the notions of adaptive corruption in cryptography.","10.1007/s10472-019-09675-1","18979","0.2019231"
"473","hot_topic_cluster_no_4","rank_5","28","Mitigation of Distributed Denial of Service Attacks in the Cloud","Cybersecurity attacks resulting in loss of availability of cloud services can have significantly higher impact than those in the traditional stand-alone enterprise setups. Therefore, availability attacks, such as Denial of Service attacks (DoS); Distributed DoS attacks (DDoS) and Economical Denial of Sustainability (EDoS) attacks receive increasingly more attention. This paper surveys existing DDoS attacks analyzing the principles, ways of launching and their variants. Then, current mitigation systems are critically discussed. Based on the identification of the weak points, the paper proposes a new mitigation system named as DDoS-Mitigation System (DDoS-MS) that attempts to overcome the identified gap. The proposed framework is evaluated, and an enhanced version of the proposed system called Enhanced DDoS-MS is presented. In the end, the paper presents some future directions of the proposed framework.","10.1515/cait-2017-0040","49577","0.2068966"
"474","hot_topic_cluster_no_4","rank_5","28","Cluster based Detection and Reduction Techniques to Identify Wormhole Attacks in Underwater Wireless Sensor Networks","Underwater Wireless Sensor Networks (UWSN) is widely used in variety of applications but none of the applications have taken network security into considerations. Deployment of underwater network is a challenging task and because of the hash underwater environment, the network is vulnerable to large class of security attacks. Recent research on underwater communication focuses mainly on energy efficiency, network connectivity and maximum communication range. The nature of underwater sensor network makes it more attractive for the attackers. One of the most serious problems in underwater networks is wormhole attack. In this research work we concentrate on providing security to the underwater network against wormhole attacks. We introduce the wormhole attack in the network and propose a solution to detect this attack in underwater wireless networks. Energy Efficient Hybrid Optical Acoustic Cluster Based Routing Protocol (EEHRCP) is incorporated and using the round trip time and other characteristics of wormhole attack, the presence of the wormhole attack in the network is identified. The simulation results depicts that the proposed wormhole detection mechanism increases throughput by 26%, reduces energy consumption by 3%, reduces end to end delay by 13% and increases packet delivery ratio by 3%.",""," 8415","0.2209302"
"475","hot_topic_cluster_no_4","rank_5","28","Risk assessment in social lending via random forests","With the advance of electronic commerce and social platforms, social lending (also known as peer-to-peer lending) has emerged as a viable platform where lenders and borrowers can do business without the help of institutional intermediaries such as banks. Social lending has gained significant momentum recently, with some platforms reaching multi-billion dollar loan circulation in a short amount of time. On the other hand, sustainability and possible widespread adoption of such platforms depend heavily on reliable risk attribution to individual borrowers. For this purpose, we propose a random forest (RF) based classification method for predicting borrower status. Our results on data from the popular social lending platform Lending Club (LC) indicate the RF-based method outperforms the FICO credit scores as well as LC grades in identification of good borrowers. ","10.1016/j.eswa.2015.02.001","65120","0.2396825"
"476","hot_topic_cluster_no_5","rank_1","362","Adaptive building energy management with multiple commodities and flexible evolutionary optimization","To enable the efficient utilization of energy carriers and the successful integration of renewable energies into energy systems, building energy management systems (BEMS) are inevitable. In this article, we present a modular BEMS and its customizable architecture that enable a flexible approach towards the optimization of building operation. The system is capable of handling the energy flows in the building and across all energy carriers as well as the interdependencies between devices, while keeping a unitized approach towards devices and the optimization of their operation. Evaluations in realistic scenarios show the ability of the BEMS to increase energy efficiency, self-consumption, and self-sufficiency as well as to reduce energy consumption and costs by an improved scheduling of the devices that considers all energy carriers in buildings as well as their interdependencies. ","10.1016/j.renene.2015.09.003","60528","0.2285714"
"477","hot_topic_cluster_no_5","rank_1","362","Energy Management for Lifetime Extension of Energy Storage System in Micro-Grid Applications","Energy storage is needed in micro-grid to help solve the problem of intermittency introduced by renewable energy sources, enhance power quality and improve controllability of power flow. This paper presents an energy manager for energy storage system (ESS) in micro-grids. The objectives of the energy manager are focused on improving the energy efficiency and extending the life expectancy of ESS while ensuring constraints of energy storage modules are complied with. To this end a smart local prediction and local scheduling algorithm is proposed. A battery lifetime model that uses the proposed Peukert lifetime energy throughput based on the workload of the battery is developed. Verification shows that in the long run, the energy manger can improve overall energy efficiency of ESS from 74.1% to 85.5%, and improve estimated lifetime of 2 Battery Packs in ESS from 3.6 years and 2.4 years to 5 years and 5.7 years respectively.","10.1109/TSG.2013.2272835","73503","0.2290323"
"478","hot_topic_cluster_no_5","rank_1","362","Impact of inter-fuel substitution on energy intensity in Ghana","Energy intensity and elasticity, together with inter-fuel substitution are key issues in the current development stage of Ghana. Translog production and ridge regression are applied for studying these issues with a data range of 2000-2015. The current energy dynamics reveal the expected inverse relationship: higher energy intensity and lower elasticity with economic growth. There are evidences of energy-economic challenges: high energy cost, inefficiency and backfire rebound effect. The implications are higher energy losses in the system, more consumption of lower-quality energy together with low energy technology innovation. Energy is wasted and directly not productive with economic activities. It is observed further that the higher energy intensity invariably increases CO2 emission because approximately 95% of total energy is derived from hydrocarbons and biomass. An inter-fuel substitution future scenario design was further conducted and the results were positive with growth, lower energy intensity, and improved energy efficiency. Therefore, government and energy policymakers should improve energy efficiency, cost, and productiveness. That is, they should change energy compositions and augment energy technology innovation, thus, increasing renewable share to 15% by 2026, reducing wood and charcoal by about 69%, and increasing natural gas to about 776%. Energy policymakers should enhance the installation of smart energy, cloud energy solution, tokenization of energy system and storage.","10.1007/s11708-019-0656-5","18643","0.2542056"
"479","hot_topic_cluster_no_5","rank_1","362","Modern Aspects of Energy and Materials","Silicon (Si) plays a central role in the emerging energy technologies of the twenty first century. Silicon is directly involved in energy production, in energy storage, in energy distribution and in energy utilization. Silicon also plays a role in control systems, in energy network management, in energy related information technology and in all aspects of risk assessment & risk management as they apply to the supply of energy and to the use of energy. Various themes from the above topics are described herein.","10.1007/s12633-016-9449-1","55766","0.3243243"
"480","hot_topic_cluster_no_5","rank_1","362","An energy matching method for battery electric vehicle and hydrogen fuel cell vehicle based on source energy consumption rate","The smart cities development requires reducing energy consumption and using as much renewable energy as possible, so the widespread use of new energy vehicles is a very important measure. In this work, for the energy system configuration and energy efficiency balance of new energy vehicles, we propose an energy matching method to study its energy efficiency from the view point for energy life cycle. Nowadays, new energy vehicles mainly include battery electric vehicles (BEV) and hydrogen fuel cell vehicles (HFCEV). Firstly, we proposed the Source to Range (STR) model. Then, based on STR model, we used energy efficiency analysis chart to visually represent the conversion, delivery and consumption of the vehicle energy life cycle. Furthermore, we proposed a Source Energy Consumption Rate (SECR), which is used to evaluate the vehicles energy efficiency. Finally, based on STR model, we obtained the dividing line of the same SECR for new energy vehicles and equivalent fuel vehicles, which provides constraints on the vehicle energy system design. The results show that STR model can provide an effective tool for energy matching and energy efficiency analysis of new energy vehicles, and has a reference for product development of new energy vehicles. ","10.1016/j.ijhydene.2019.02.169","20149","0.3670732"
"481","hot_topic_cluster_no_5","rank_2","816","Development of an Energy Saving Strategy Model for Retrofitting Existing Buildings: A Korean Case Study","The building sector accounts for approximately 40% of national energy consumption, contributing to the environmental crisis of global warming. Using energy saving measures (e.g., improved thermal insulation, highly energy-efficient electrical and mechanical systems) provides opportunities to reduce energy consumption in existing buildings. Furthermore, if the life cycle cost (i.e., installation, operation and maintenance cost) of the measures is considered with their energy saving potential, it is possible to establish a cost-effective energy retrofit plan. Therefore, this research develops an energy saving strategy model considering its saving potential and life cycle cost of the measures for reducing energy consumption in existing buildings. To test the validity of the proposed model, a case study is carried out on an educational facility in South Korea, in response to its overconsumption of energy. The results demonstrate that in terms of energy saving and life cycle cost, the optimal energy retrofit plan is more cost-effective than the existing plan. Also, the break-even point for the optimal energy retrofit plan is within five years, and then revenue from energy saving continually occurs until 2052. For energy retrofit of existing buildings, using the proposed model would enable building owners to maximize energy savings while minimizing the life cycle cost.","10.3390/en12091626","26716","0.2426829"
"482","hot_topic_cluster_no_5","rank_2","816","Energy efficiency evaluation for machining systems through virtual part","Energy prices, environmental concerns, carbon dioxide emissions, and economic matters are driving factors for research on reducing machining system energy consumption. Energy efficiency evaluation for machining systems is an effective management strategy for reducing the energy consumption and improving the energy efficiency of machining systems. This paper proposes a methodology called virtual part method to evaluate the energy efficiency of machining systems, and the proposed method overcomes the deficiencies or limitations of major existing methods (such as the entitative part method) through equivalence and virtualization of all possible machining system parts that may be manufactured in future. Based on an analysis of the energy compositions and characteristics of the proposed virtual part, an energy efficiency evaluation for machining systems through virtual part is conducted in three steps: 1) selection of evaluation indexes; 2) corresponding data collection for calculating indexes; and 3) development of energy efficiency evaluation system. Its application in a machine tool suggests that the proposed method is more accurate than the existing ones and contributes to energy-saving activities including the development of energy efficiency standards, design of energy-efficient machining systems, and reform of old machining systems. ","10.1016/j.energy.2018.06.096","34758","0.2460526"
"483","hot_topic_cluster_no_5","rank_2","816","Research on Large-Scale Building Energy Efficiency Retrofit Based on Energy Consumption Investigation and Energy-Saving Potential Analysis","As building energy consumption continues to grow, large-scale building energy efficiency retrofit (LSBEER) has become a trend in China, especially in existing commercial districts (ECDs). In some mature ECDs, buildings often have complex functions, long running time, and a lack of a basic database, so the whole energy consumption and level of energy efficiency are unclear. It is a challenging task to put forward the effective energy efficiency retrofit measures (ERMs) and quantify energy-saving potential. To overcome these obstacles, this paper introduces a bottom-up approach to analyze the potential of LSBEER. Meanwhile, it examines a case study of energy-saving potential of an ECD in Shanghai, with very specific problems targeting and corresponding to measurements. One hundred key buildings are studied in this paper, and the authors give a comprehensive understanding and quantitative analysis of energy-saving potential. Moreover, this paper also suggests a list of ERMs, including envelope, cooling, lighting, heating, elevator, and so on. Through investigation, the annual energy saving (AES) is 533,268 GJ, which could reduce energy consumption by 24.3%. Furthermore, the authors supply a feasible potential map of LSBEER for decision-makers. ","10.1061/(ASCE)EY.1943-7897.0000618","19673","0.2487805"
"484","hot_topic_cluster_no_5","rank_2","816","Energy Storage System Control Algorithm by Operating Target Power to Improve Energy Sustainability of Smart Home","As energy issues are emerging around the world, a variety of smart home technologies aimed at realizing zero energy houses are being introduced. Energy storage system (ESS) for smart home energy independence is increasingly gaining interest. However, limitations exist in that most of them are controlled according to time schedules or used in conjunction with photovoltaic (PV) generation systems. In consideration of load usage patterns and PV generation of smart home, this study proposes an ESS control algorithm that uses constant energy of energy network while making maximum use of ESS. Constant energy means that the load consumes a certain amount of power under all conditions, which translates to low variability. The proposed algorithm makes a smart home a load of energy network with low uncertainty and complexity. The simulation results show that the optimal ESS operating target power not only makes the smart home use power constantly from the energy network, but also maximizes utilization of the ESS. In addition, since the smart home is a load that uses constant energy, it has the advantage of being able to operate an efficient energy network from the viewpoint of energy providers.","10.3390/su10010236","42223","0.2523256"
"485","hot_topic_cluster_no_5","rank_2","816","Analysis on the monitoring system of energy conservation and comfort in office buildings based on internet of things","In order to reduce the cost of central air conditioning, we need to reduce its energy consumption. This paper briefly introduced Internet of Things and the energy-saving and comfort monitoring system of central air conditioning based on the Internet of Things. The system took comfort degree as constraint and energy efficiency as objective to control energy saving of central air conditioning. Company X in Guanghan, Sichuan, China, was taken as an example for analysis. The system was compared with the energy-saving control system which took temperature and power as constraints. Compared with before the energy-saving control, the proportion of air conditioning downtime in the working hours of employees increased after the implementation of the two kinds of energy-saving control systems, and the proportion of downtime under the energy-saving control system designed in this study was larger; in addition, after the control of the two kinds of energy-saving systems, the energy efficiency of the air conditioning significantly improved, and the air conditioning under the control of the energy-saving system proposed in this study had more improvement in energy efficiency and higher energy-saving efficiency. The energy-saving control method proposed in this study can effectively reduce the power consumption of the central air conditioning in the office.","10.1093/ijlct/ctz083"," 7093","0.2786667"
"486","hot_topic_cluster_no_5","rank_3","328","Resolving conflict objectives between environment impact and energy efficiency - An optimization modeling on multiple-energy deployment","Energy consumption increases dramatically due to economic development, leading to the raising of toxic materials such as CO2 during electricity generation process. This study aims at solving the electricity generation portfolio issue to maximize electricity generation while minimizing the CO2 emission to protect the environment. In this research, multiple types of power plants to produce electricity, such as by gas, coal, solar, are alternatives for electricity generation portfolio. Especially, uncertainty in electricity demand is included and dealt by a stochastic chance constrained method. Further. The proposed model is solved by a non-dominated sorting genetic algorithm for multiple objectives. The result presents the optimal capacity portfolio plan for electricity generation and suggests a trade-off decision between conflicting goals to balance electricity generation efficiency and environmental protection.","10.1016/j.cie.2019.106111","19581","0.3031250"
"487","hot_topic_cluster_no_5","rank_3","328","A new perspective in optimum sizing of hybrid renewable energy systems: Consideration of component performance degradation issue","The ever increasing demand for energy and the concerns on the environmental sustainability issue all around the world lead to more interest in alternative sources for energy production. However, as the current costs of the alternative sources such as solar, wind energy conversion systems etc. are relatively higher as compared to the conventional means of energy production, an optimum sizing approach is quite necessary in order to avoid over-sizing of such systems without lowering the reliability of load demand supply in all possible conditions including the variability of meteorological conditions or the changing power demand of load. There are many research papers available in the literature dealing with this optimum sizing issue. Even the mentioned papers significantly contribute to the wider penetration of such sources, none of them consider the power output degradation of alternative energy sources due to aging during their pre-defined operating life time. Besides, there are a few studies utilizing detailed dynamic models of energy sources apart from first-degree linear equations based models that may fall short in presenting the exact dynamics of the related system. Thus, an ""observe and focus"" algorithm based optimization of a hybrid alternative energy system considering the power output degradation and detailed models of each hybrid system component is performed in this study. Related details presented within the paper can provide a new perspective in optimum sizing of such hybrid systems and may further be considered in future updates of famous sizing software programs commercially or freely available in websites of several laboratories or universities. ","10.1016/j.ijhydene.2012.04.071","77208","0.3254902"
"488","hot_topic_cluster_no_5","rank_3","328","Dynamic economic emission dispatch problem with renewable integration focusing on deficit scenario in India","Present scenario power grid has increased penetration of renewable energy sources (RESs). RESs are clean sources of energy and power production from them with minimal emission is a national target. Already, increased carbon footprint has put nations into jeopardy. Nowadays to study the benefits due to RESs in power system is of greater importance. Stochastic nature of RESs made it difficult to manage power dispatch scenario. Dynamic power demand added even more difficulty in obtaining real time economic schedule of generation dispatch. A test system of ten generator and emission dispatch with wind turbine (WT) and photovoltaic (PV) having dynamic load for 24 hours is economised. Stochastic method of particle swarm optimisation (PSO) is compared with anti-predatory particle swarm optimisation (APSO). It is identified that APSO method gives a better economy with reduced emission for the given problem.","","17997","0.3389831"
"489","hot_topic_cluster_no_5","rank_3","328","The Knowledge Discovery of the Nuclear Power Issue Using: the Artificial Intelligence Model: An Example of the CART and the SVM","After the nuclear accidents in the Fukushima Daiichi Nuclear Power Plant, the public has become increasingly concerned about nuclear power policies. We employed the CART and the SVM methods to establish and compare classification patterns in order to reveal obscure information in the survey data of Nuclear Power Policy'. Our results showed that the total classification accuracy of the CART is higher than that of SVM or logistic regression. 48.87% of the Taiwanese public opposed nuclear power, 34.16% supported nuclear power, and 16.97% did not respond to the question. However, the CART analysis showed that among the non-respondents, 56% supported nuclear power and 44% opposed its use. The two primary factors influencing the opinion for nuclear power are ""satisfaction with the safety of nuclear power"" and ""improvement in safety inspection and disaster prevention standards in nuclear power plants."" Therefore, by improving the safety of nuclear power plant, the government might regain public support for nuclear power.","10.6138/JIT.2016.17.6.20160524","56520","0.3466667"
"490","hot_topic_cluster_no_5","rank_3","328","A new energy exploration for Malaysia's future electric supply based on waste kinetic energy: ideas, concepts, and possibilities","This paper performs a concept on a new exploration of electricity generation by harvesting the kinetic energy from landing aircraft. Firstly, this paper analyzes and tabulates the current status of renewable energy sources and the conventional power generations in Malaysia's electric supply until 2017. It has been reviewed based on the demand, availability, and power production of the sources. At the same time, the locations and providers for several energy supplies have been identified with the evidence of its rated power being generated. Next, this paper also highlights the plan for achieving a low-carbon country based on the plans which have been in place between 1970 until 2017 in order to create a zero-carbon generation by 2030 for Malaysia's energy sector. Toward the end of this paper, the authors suggest a new method on decarbonization technologies for clean energy in order to reach zero-carbon generation earlier than 2030. It is by introducing a new topology of electrical power generation system using waste energy, which is kinetic energy based on landing aircraft, for the possibility of power generation for a new Malaysia.","10.1007/s00202-019-00862-1","19883","0.3600000"
"491","hot_topic_cluster_no_5","rank_4","642","Forecasting electricity consumption using a novel hybrid model","In recent years, the electricity industry has become increasingly important to social and economic development. For sustainability of the power industrial business, an accurate electricity consumption forecasting model can be used to adjust the production and consumption patterns of electricity, it can also support energy policy decision-making, such as load unit commitment, operational security of plants, and economic load dispatching. Using electricity consumption data to study electricity production and consumption patterns is useful in identifying the regulation of electricity economic development. This paper combines several machine learning approaches (the empirical mode decomposition (EMD) method, the support vector regression (SVR) model, and the particle swarm optimization (PSO) algorithm), thermal reaction dynamics theory, and the econometric model (ARGARCH model), to develop a novel hybrid forecasting model, namely EMD-SVR-PSO-AR-GARCH model, for forecasting electricity consumption. It adopts a new perspective on electricity usage and consuming economic behaviors. Using electricity consumption data from the New South Wales (NSW, Australia) market, the developed model is used to forecast electricity consumption. Then, the Nash equilibrium and Porter's five-force model are used to analyze the complex electricity usage and consuming economic behaviors, to identify the regulation of electricity and economic development, supporting the sustainable development of electricity.","10.1016/j.scs.2020.102320"," 4431","0.2471910"
"492","hot_topic_cluster_no_5","rank_4","642","Robust scheduling of hydrogen based smart micro energy hub with integrated demand response","Nowadays, to address the different demands of consumers, i.e., electricity, heat and gas, the concept of energy hub (EH) is emerged. Integrating multiple energy sources under the concept of energy hub can improve the efficiency and reliability of the system; moreover, efficiency increasing cause to decrease in environmental emission. Environmental emission is a very important issue from the global warming point of view that should be considered in the researches. In this regard, green hydrogen can be used as a clean energy carrier that can be stored for a long time without degradation. To this end, this paper focuses on the concept of hydrogen-based smart micro energy hub (SMEH) considering integrated demand response (IDR) and fuel cell-based hydrogen storage system (HSS). IDR is introduced to control consumers' electrical and heat demand patterns. Additionally, HSS not only can convert power from renewable energy sources (RESs) to hydrogen (P2H) in a low electricity price period and vice versa (H2P) in a high electricity price period but also can supply the hydrogen-based industry. The aim of the proposed model is to minimize the total energy cost of hydrogen-based energy hub integrated with IDR and HSS under a robust optimization (RO) approach considering the uncertainty of electricity price. The proposed optimization model provides day-ahead scheduling for SMEH, which would improve the operation of the energy system. Finally, simulation and numerical results are provided to confirm the effectiveness of the proposed model. According to the obtained results, considered technologies cause to decrease in operation cost up to 7.8% and improve the system robustness against electricity market uncertainty up to 30%.","10.1016/j.jclepro.2020.122041"," 5113","0.2482456"
"493","hot_topic_cluster_no_5","rank_4","642","Integration of hydrogen energy systems into renewable energy systems for better design of 100% renewable energy communities","This paper will introduce the concept of 100% renewable energy and the role hydrogen and fuel cells can play for storage and transportation sectors. The importance, necessity and possibility of an overall global transition from fossil and nuclear-based fuels to renewable energy fuels using electricity and hydrogen as clean energy carriers will be summarized. Polluted, inefficient and unhealthy societies of today are missing energy self-sufficient, better managed, in harmony with nature ecological communities. The possible role of hydrogen fuel cells in fulfilling this target will be demonstrated and the success stories of the leading countries will be given. Renewable energy investments which have started in 2013 increased steadily and reached to 290 billion dollars towards the end of 2015. Today's increasing investments and cheaper supply of renewable energy will in turn effect the hydrogen production costs and increase the diversion from fossil fuels to cheap renewable energy. This reality confirms that hydrogen energy technologies will play an important role in reaching the 100% renewable energy target. ","10.1016/j.ijhydene.2016.09.086","48779","0.2552632"
"494","hot_topic_cluster_no_5","rank_4","642","Energy system impacts from heat and transport electrification","Electrifying the energy system and powering it by low carbon electricity is one of the key decarbonisation pathways of the energy system. This study examines annual electricity and gas consumption in a high electrification scenario in Great Britain (GB) and the implications for electricity generation and transmission infrastructure using a suite of soft-linked models. High electrification of heating and transport services, which are two major fossil fuel consumers in GB, increases annual electricity consumption and peak electricity load by 35% and 93%, respectively, by 2050 while reducing overall annual energy consumption compared to a reference case. Meeting this high electricity consumption with a supply strategy that is dependent on offshore wind could more than double the supply-side investments required compared to a reference case, if demand-side measures are not available. High electrification would also impact existing gas and oil energy infrastructure by reducing consumption of these fuels. It was found that uncertainties in socio-economic growth can amplify these implications and therefore need serious consideration by analysts and policymakers involved in designing energy transition strategies. A case study and discussion demonstrate that smart-grid aided demand-side management has the potential to minimise electricity peak load and infrastructure requirements from high electrification.","10.1680/ener.14.00008","72481","0.2662791"
"495","hot_topic_cluster_no_5","rank_4","642","Energy and cost analyses of a hybrid renewable microgeneration system serving multiple residential and small office buildings","This study investigates the energy and cost performance of hybrid renewable ground source heat pump (GSHP) and natural gas fueled fuel cell (FC) microgeneration systems serving multiple residential and small office buildings in Ottawa (Canada) and Incheon (South Korea). The study is performed by simulations in TRNSYS environment. The performance of the microgeneration system is compared to a GSHP only system. In addition, the impact of the FC capacities, natural gas price and electricity price on the system's energy and cost performance is examined. The energy analysis results show that the GSHP FC systems have less primary energy consumption compared to the GSHP only system in both geographic locations. However, whether a GSHP FC system could achieve operational cost saving is strongly dependent on the local natural gas and electricity prices and also on the building heating, cooling and electrical loads and their patterns. The GSHP FC microgeneration systems could yield operational cost savings at locations where the natural gas (or other input fuel to the FC) price is much lower than the electricity price, such as in Ottawa. At locations where with exceptionally high natural gas to electricity price ratio, such as in Korea, no operational cost saving could be attained by the GSHP FC system. The cost analysis results indicate that, in Ottawa, the extra capital investment incurred to the GSHP FC system is possible to be returned within its lifespan, especially with the current trend of continuous price reductions of FC equipment and installation resulting from economy of scale and market expansion. Nevertheless, the GSHP FC microgeneration systems' capability to generate both electricity and thermal energy at the point of use is generally considered more attractive for inclusion in the ""smart"" energy networks, new and remote community applications. ","10.1016/j.applthermaleng.2014.01.049","70972","0.3299320"
"496","hot_topic_cluster_no_5","rank_5","334","Design of structure and optimization of organic Rankine cycle for heat recovery from gas turbine: The use of 4E, advanced exergy and advanced exergoeconomic analysis","This paper presents the evaluation and optimization of an organic Rankine cycle (ORC) approached from four different perspectives: (1) selecting the ORC Cycle; (2) selecting the working fluid in accordance with the thermodynamic properties and environmental impacts; (3) analyzing energy and exergoeconomic; and finally, (4) advanced exergy and advanced exergoeconomic. The working fluid is selected based on the thermal source temperature and the environmental impacts including the reduction of ozone depletion (ODP) and global warming potential (GWP). Then, the selected working fluids in different cycles are investigated in terms of the thermodynamic properties based on energy and exergy concepts. The parameters of the selected cycles (from the perspective of energy and exergy), including the temperature and pressure of the input working fluid of the turbine, the pinch and approach temperature, and so on, are optimized by a genetic algorithm. Two objective functions of price and exergy efficiency are selected as the objective functions for optimal cycles. The results of this study reveal that single-pressure and dual-pressure cycles with recuperator and the R123 working fluid have the highest power and the lowest cost. It is indicated that the net power generation of dual pressure cycle with recuperator, single pressure with recuperator, and dual pressure cycle with two working fluids are 2.2 kW, (R123: 2.31 kW, R600: 1.72 kW) and 2.26 kW, respectively. Also, the cost of generated electricity for dual pressure cycle with recuperator, single pressure cycle with recuperator, and dual pressure cycle with two working fluids are 14.59, (R123: 13.03, R600: 21.95) and 16.78 (Cent/kWh), respectively. In addition, the results show that the cycles of dual pressure with recuperator and single pressure with recuperator with R123 as a working fluid have the highest exergy efficiency. The advanced exergy analysis indicated that the HRSG and turbine components are important to be improved based on exergetic performance.","10.1016/j.applthermaleng.2018.09.128","29890","0.3356589"
"497","hot_topic_cluster_no_5","rank_5","334","Exergy, exergoeconomic and environmental analyses and evolutionary algorithm based multi-objective optimization of combined cycle power plants","A comprehensive exergy, exergoeconomic and environmental impact analysis and optimization is reported of several combined cycle power plants (CCPPs). In the first part, thermodynamic analyses based on energy and exergy of the CCPPs are performed, and the effect of supplementary firing on the natural gas-fired CCPP is investigated. The latter step includes the effect of supplementary firing on the performance of bottoming cycle and CO2 emissions, and utilizes the first and second laws of thermodynamics. In the second part, a multi-objective optimization is performed to determine the ""best"" design parameters, accounting for exergetic, economic and environmental factors. The optimization considers three objective functions: CCPP exergy efficiency, total cost rate of the system products and CO2 emissions of the overall plant. The environmental impact in terms of CO2 emissions is integrated with the exergoeconomic objective function as a new objective function. The results of both exergy and exergoeconomic analyses show that the largest exergy destructions occur in the CCPP combustion chamber, and that increasing the gas turbine inlet temperature decreases the CCPP cost of exergy destruction. The optimization results demonstrates that CO2 emissions are reduced by selecting the best components and using a low fuel injection rate into the combustion chamber. Crown ","10.1016/j.energy.2011.08.034","79386","0.3500000"
"498","hot_topic_cluster_no_5","rank_5","334","Thermodynamic, thermoeconomic and life cycle assessment of a novel integrated solar combined cycle (ISCC) power plant","In this study, thermodynamic, thermo-economic and life cycle assessments are carried out for a novel integrated solar combined cycle system (ISCCS). The novel cycle has been developed with the purpose of using parabolic trough solar cycle in an on-line conventional combined cycle power plant (CCPP) with the minimum modifications required for the primary cycle. An attempt is made to determine the components with the most exergy destruction and assessing the investment cost of each element and stream of the system. The thermodynamic analyses are applied to the solar field and the power plant to evaluate the plant performance and pinpointing the zones with the high exergy destruction. The results indicate that the combustion chamber, gas turbine, air compressor, solar collectors account for 63.71%, 13.76%, 7.26% and 3.1% of the plant total exergy destruction, respectively. The life cycle assessment (LCA) is performed for the system, and a comparison is made between different methods of energy generation in the proposed ISCC. The results have indicated that the energy generation in the solar plant of ISCC has a lower environmental impact in human health and resources damage categories; while, the power production in the natural CCPP has less impact in ecosystem damage category.","10.1016/j.seta.2018.04.011","37424","0.3653846"
"499","hot_topic_cluster_no_5","rank_5","334","Evaluation of a base-loaded combined heating and power system with thermal storage for different small building applications","The objective of this paper is to demonstrate the advantages of using a combined heating and power (CHP) system operating at full load to satisfy a fraction of the facility electric load, that is, a base load. In addition, the effect of using thermal storage during the CHP system operation (CHP-TS) is evaluated. A small office building and a restaurant with the same floor area, in Chicago, IL, and Hartford, CT, were used to evaluate the base-loaded CHP and CHP-TS operation based on operational cost, primary energy consumption (PEC), and carbon dioxide emissions (CDEs). Results indicate that, in general, the use of thermal storage is beneficial for the CHP system operation because it reduces cost, PEC, and CDEs compared with a CHP with no thermal storage. The CHP and CHP-TS operation is more beneficial for a restaurant than for a small office building for the evaluated cities, which clearly indicates the effect of the thermal load on the CHP system performance. ","10.1002/er.1892","75317","0.3731343"
"500","hot_topic_cluster_no_5","rank_5","334","Thermodynamic and Economic Analysis of an Integrated Solar Combined Cycle System","Integrating solar thermal energy into the conventional Combined Cycle Power Plant (CCPP) has been proved to be an efficient way to use solar energy and improve the generation efficiency of CCPP. In this paper, the energy, exergy, and economic (3E) methods were applied to the models of the Integrated Solar Combined Cycle System (ISCCS). The performances of the proposed system were not only assessed by energy and exergy efficiency, as well as exergy destruction, but also through varied thermodynamic parameters such as DNI and T-a. Besides, to better understand the real potentials for improving the components, exergy destruction was split into endogenous/exogenous and avoidable/unavoidable parts. Results indicate that the combustion chamber of the gas turbine has the largest endogenous and unavoidable exergy destruction values of 202.23 MW and 197.63 MW, and the values of the parabolic trough solar collector are 51.77 MW and 50.01 MW. For the overall power plant, the exogenous and avoidable exergy destruction rates resulted in 17.61% and 17.78%, respectively. In addition, the proposed system can save a fuel cost of 1.86 $/MW.h per year accompanied by reducing CO2 emissions of about 88.40 kg/MW.h, further highlighting the great potential of ISCCS.","10.3390/e20050313","37977","0.3922078"
"501","hot_topic_cluster_no_6","rank_1","459","A survey on IEEE 802.11ah: An enabling networking technology for smart cities","Smart technologies play a key role in sustainable economic growth. They transform houses, offices, factories, and even cities into autonomic, self-controlled systems acting often without human intervention and thus sparing people routine connected with information collecting and processing. The paper gives an overview of a novel Wi-Fi technology, currently under development, which aims to organize communication between various devices used in such applications as smart grids, smart meters, smart houses, smart healthcare systems, smart industry, etc. ","10.1016/j.comcom.2014.08.008","66433","0.2135135"
"502","hot_topic_cluster_no_6","rank_1","459","Toward the Internet of Things for Physical Internet: Perspectives and Challenges","The Physical Internet (PI, or pi) paradigm has been developed to be a global logistics system that aims to move, handle, store, and transport logistics products in a sustainable and efficient way. To achieve the goal, the PI requires a high-level interconnectivity in the physical, informational, and operational aspects enabled by an interconnected network of intermodal hubs, collaborative protocols, and standardized, modular, and smart containers. In this context, PI is a key player poised to benefit from the Internet-of-Things (IoT) revolution since it potentially provides an end-to-end visibility of the PI objects, operations, and systems through ubiquitous information exchange. This article is to investigate opportunities of application of the IoT technology in the PI vision. In addition, an IoT ecosystem (pi-IoT) encompassing key enabling IoT technologies, building blocks, and a service-oriented architecture (SoA) is proposed as a potential component for accelerating the implementation of PI. The major challenges regarding the deployment of IoT into the emerging logistics concept are also discussed intensively for further research.","10.1109/JIOT.2020.2971736","10259","0.2222222"
"503","hot_topic_cluster_no_6","rank_1","459","Enabling Technologies for Green Internet of Things","Recent technological advances have led to an increase in the carbon footprint. Energy efficiency in the Internet of Things (IoT) has been attracting a lot of attention from researchers and designers over the last couple of years, paving the way for an emerging area called green IoT. There are various aspects (such as key enablers, communications, services, and applications) of IoT, where efficient utilization of energy is needed to enable a green IoT environment. We explore and discuss how the various enabling technologies (such as the Internet, smart objects, sensors, etc.) can be efficiently deployed to achieve a green IoT. Furthermore, we also review various IoT applications, projects and standardization efforts that are currently under way. Finally, we identify some of the emerging challenges that need to be addressed in the future to enable a green IoT.","10.1109/JSYST.2015.2415194","46358","0.2250000"
"504","hot_topic_cluster_no_6","rank_1","459","Green IoT: An Investigation on Energy Saving Practices for 2020 and Beyond","Internet of Things (IoT) is an emerging concept, which aims to connect billions of devices with each other. The IoT devices sense, collect, and transmit important information from their surroundings. This exchange of very large amount of information amongst billions of devices creates a massive energy need. Green IoT envisions the concept of reducing the energy consumption of IoT devices and making the environment safe. Inspired by achieving a sustainable environment for IoT, we first give the overview of green IoT and the challenges that are faced due to excessive usage of energy hungry IoT devices. We then discuss and evaluate the strategies that can be used to minimize the energy consumption in IoT, such as designing energy efficient datacenters, energy efficient transmission of data from sensors, and design of energy efficient policies. Moreover, we critically analyze the green IoT strategies and propose five principles that can be adopted to achieve green IoT. Finally, we consider a case study of very important aspect of IoT, i.e., smart phones and we provide an easy and concise view for improving the current practices to make the IoT greener for the world in 2020 and beyond.","10.1109/ACCESS.2017.2686092","49857","0.2943662"
"505","hot_topic_cluster_no_6","rank_1","459","Design of Building Energy Autonomous Control System with the Intelligent Object Energy Chain Mechanism Based on Energy-IoT","The development of the Internet of Things (IoT) technologies has enabled smart objects to communicate with each other, and various IoT methods and techniques have appeared accordingly. At this point in time, there is a growing need for big data-based energy-IoT technology that can reduce energy consumption. However, despite the emergence of these technologies for IoT, there is still a lack of control systems to surmount the energy efficiency problem of hyperconnected IoT applications. In this paper, we attempt to reduce energy consumption by automatically controlling the communication between each device by establishing three different energy-IoT-based chains regarding the relationship of connections between devices.","10.1155/2015/931792","67261","0.3375000"
"506","hot_topic_cluster_no_6","rank_2","896","From Smart-Cities to Smart-Communities: How Can We Evaluate the Impacts of Innovation and Inclusive Processes in Urban Context?","Nowadays, through ICT supports and their applications, the concept of smart cities has evolved into smart communities, where the collaborative relationship between citizens and public administration generates multi-dimensional impacts: urban sites are living labs and agents of innovation and inclusion. As a first step, this article aims to critically review the state of the art of the assessment methods of these impacts through a set of synthetic indicators; the second step is to elaborate a specific framework to evaluate quality of life through a set of impact indicators for smart communities and inclusive urban processes. According to some referenced authors, cities and communities are smart if they perform well in six smart categories: smart economy; smart people; smart governance; smart mobility; smart environment; and smart living. Considering a recent experiment carried out in Turin (Italy), the authors propose a methodology, whose trial is ongoing, based on a hierarchical multiscale framework defining a set of smart community indicators.","10.4018/IJEPR.2019040102","27349","0.1773333"
"507","hot_topic_cluster_no_6","rank_2","896","Leveraging Circular Economy through a Methodology for Smart Service Systems Engineering","Product Service Systems (PSS) and Smart Services are powerful means for deploying Circular Economy (CE) goals in industrial practices, through dematerialization, extension of product lifetime and efficiency increase by digitization. Within this article, approaches from PSS design, Smart Service design and Model-based Systems Engineering (MBSE) are combined to form a Methodology for Smart Service Architecture Definition (MESSIAH). First, analyses of present system modelling procedures and systems modelling notations in terms of their suitability for Smart Service development are presented. The results indicate that current notations and tools do not entirely fit the requirements of Smart Service development, but that they can be adapted in order to do so. The developed methodology includes a modelling language system, the MESSIAH Blueprinting framework, a systematic procedure and MESSIAH CE, which is specifically designed for addressing CE strategies and practices. The methodology was validated on the example of a Smart Sustainable Street Light System for Cycling Security (SHEILA). MESSIAH proved useful to help Smart Service design teams develop service-driven and robust Smart Services. By applying MESSIAH CE, a sustainable Smart Service, which addresses CE goals, has been developed.","10.3390/su11133517","24778","0.1851351"
"508","hot_topic_cluster_no_6","rank_2","896","Could Smart Tourists Be Sustainable and Responsible as Well? The Contribution of Social Networking Sites to Improving Their Sustainable and Responsible Behavior","A key strategic aim of tourism destinations within the smart tourism paradigm is to achieve efficient, responsible and sustainable use of tourism resources. This aim can be achieved by promoting the appropriate practices and making tourists co-managers, co-designers and co-creators of tourism experiences. This paper argues that smart tourism destinations should manage their resources in a sustainable way and that smart technologies can make their contribution. Could a smart technology such as social media/social networking sites make a contribution to sustainable tourism within the smart tourism paradigm? To address this research question, a project was carried out to explore the perceptions and attitudes of Chinese tourist consumers about the contribution of social networking sites to adopting a sustainable and responsible behavior within the context of a smart tourism framework. First a research framework encompassing three hypotheses related to the influence of social networking sites at the three main stages of the travel cycle/tourist journey was designed. An exploratory quantitative research was then carried out using the online survey technique. The study's findings indicate that the use of social networking sites influences the smart tourists at all three stages on adoption of sustainable and responsible behavior, the most significant influence is at the first two stages. The article is completed by discussing the related conclusions and management implications in the smart tourism management framework.","10.3390/su12041470","14848","0.1934066"
"509","hot_topic_cluster_no_6","rank_2","896","Actualizing big data analytics for smart cities: A cascading affordance study","This study investigates how Big Data Analytics (BDA) can be leveraged to support a city's transformation into a smart destination. We conduct an in-depth case study of a city-in-transformation and adopt the perspective of technology affordances to uncover the varying opportunities enabled by BDA to facilitate the attainment of smart tourism goals. Our findings unveil three types of BDA affordances and demonstrate how these affordances are actualized in a cascading manner to enable informed decisions and a sustainable development of smart tourism. Implications are presented for future investigation of the affordances of BDA in smart tourism, as well as for policy makers and practitioners who engage in the development of innovative tourism services for the smart citizens.","10.1016/j.ijinfomgt.2020.102156"," 4597","0.1980769"
"510","hot_topic_cluster_no_6","rank_2","896","SMART CITY MANAGEMENT FOR A SMART, HEALTHY CITY: A CASE STUDY ON KHON KAEN CITY, THAILAND","The aim of this study was to analyze the strengths, weaknesses, opportunities, and threats (SWOT) of Khon Kaen smart city development, as well as to study Khon Kaen smart city management for a smart, healthy city. In this study, we used multiple empirical methods of data collection, including interviews and focus groups, and we used SWOT, content, and descriptive analyses for the data. The results show that to develop a smart city model for sustainable living, Khon Kaen should set up eight key factors as a smart city management model: smart mobility, smart people, smart living, smart economy, smart government, smart environment, smart energy, and smart health","","16039","0.2066667"
"511","hot_topic_cluster_no_6","rank_3","29","A simulation-optimization model for sustainable product design and efficient end-of-life management based on individual producer responsibility","This paper integrates two decision problems, namely, the design alternative selection and EOL option determination, for a family of products based on individual producer responsibility in the entire life cycle considering possible uncertainties. To address three pillars of sustainability (economic, environmental, and social), three objectives are considered: the maximization of the producer's profit, the minimization of the environmental impact, and the maximization of the social impact. Two constraints to control recovery and recycling rates are considered, which are usually imposed by legislative directives. A simulation-optimization model is developed to formulate and solve the model. An example based on a real-world case is provided to illustrate the application of the model. The proposed model is a useful tool for producers to evaluate the EOL performance of their products and to analyze the effect of EPR goals or regulations on their profitability, and for policy makers to predict the response of producers to a given package of circular-economy strategies.","10.1016/j.resconrec.2018.02.031","32203","0.2400000"
"512","hot_topic_cluster_no_6","rank_3","29","A Multi-objective Sustainable Medicine Supply Chain Network Design Using a Novel Hybrid Multi-objective Metaheuristic Algorithm","End-of-life products have a severe impact on the ecological system. Potential production policies and distribution strategies for the newly manufactured product have attracted significant attention to sustainable development. Sustainability in supply chain management has much importance to achieve eco-friendly goals. In this study, we have developed sustainable objectives in the supply chain optimization framework with different constraints. The trade-off between economic, environmental and social effects objectives have identified by ensuring the optimal allocation of different products among various levels. In this regard, a new sustainability multi-objective mixed-integer linear programming mathematical model in the medicine supply chain network is developed. Although the proposed model is an NP-hard problem, we develop a novel hybrid Particle Swarm Optimization and Genetic Algorithm to achieve Pareto solutions. Then, to adjust the important parameters of the algorithms and chose the optimum levels of the significant factors for more efficiency is employed the Taguchi method. The results show that the economic and environmental effects tend to be decreased and the social impacts tend to be increased in the medicine supply chain network which can exhibit the best sustainability performance. The various outcomes of numerical experiments indicate that the proposed solution algorithm is more reliable than other algorithms. The solution methods are complemented with several sensitivity analyses on the input parameters of the model.","10.5829/ije.2020.33.10a.17"," 4311","0.2426966"
"513","hot_topic_cluster_no_6","rank_3","29","Developing a sustainable supply chain optimization model for switchgrass-based bioenergy production: A case study","Renewable energies are attracting considerable interest as an appropriate alternative for the fossil fuel based sources of energy due to increasing concerns about global warming and environmental issues. In this regard, this paper takes a new look at designing a sustainable Switchgrass-based bioenergy supply chain network (BSCN) incorporating conflicting economic, environmental and social objectives. We propose a novel, multi-objective, mixed integer linear programming model as a decision-making (DM) tool. To deal with sustainability factors, the proposed model is solved using two-stage algorithms consisting of augmented e-constraint and TOPSIS. This approach paves the way to determine the suitable strategical and tactical decisions and manage bioenergy supply chain performance with respect to the preference of decision makers for appropriate trade-off among of the sustainability factors. Computational analysis is carried out to indicate the validity of the proposed model by using a real case study in Iran. The results demonstrate that achieving a desirable level of social and environmental preservation conducted a circa 15% increase in economic objective function at the end of planning horizon. The results also show that high investment cost is a precondition to improving BSCN. It behooves the government to prioritize their plans and compartmentalize their budgets and spending in more constructive and effective ways related to bioenergy supply chain planning. ","10.1016/j.jclepro.2018.07.226","33861","0.2541176"
"514","hot_topic_cluster_no_6","rank_3","29","Sustainability in fresh agricultural product supply chain based on radio frequency identification under an emergency","A two-level RFID-based fresh agricultural product (FAP) supply chain consisting of one manufacturer and one retailer under an emergency is taken into consideration. Firstly, the coordination of FAP supply chain after the application of RFID is studied. Secondly, emergency events will lead to a demand disruption, seriously affecting the profits of supply chain participants, which is not conducive to the sustainable development of the supply chain. As a response to this situation, this study improves the original revenue-sharing contract to coordinate the RFID-based FAP supply chain after the demand disruption. Finally, the impact of RFID application and supply chain coordination on the triple bottom line of sustainable development including corporate profits, social responsibility, and environmental responsibility is analyzed.","10.1007/s10100-019-00657-6","21085","0.2679245"
"515","hot_topic_cluster_no_6","rank_3","29","Decision support for collaboration planning in sustainable supply chains","Organizations have to deal with increasing challenges in their supply chains. Their decision makers brought to balance economic performance with environmental and social issues. Methods and concepts for jointly optimizing economic, environmental and social cost of operations in supply chains are challenging. Decision support systems could help organizations to support sustainability in their supply chains. A collaborative decision-making framework for sustainable supply chain planning is proposed in this paper. This framework facilitates the development of multi-party collaborative relationships across a network to improve the sustainability of delivered products. It supports a new Information and Communication Technology (ICT) system platform. The platform provides insight for infrastructure and visibility to support supply chain sustainability requirements. The proposed decision support system proposes simultaneously collaboration and sustainability functionalities missing in many existing supply chain planning systems. This system has been evaluated and validated through a pilot program across a range of food supply networks. The work is motivated and applied by a major European research program in the agri-food supply chain. ","10.1016/j.jclepro.2019.04.367","23330","0.2871795"
"516","hot_topic_cluster_no_6","rank_4","798","More than energy harvesting - Combining triboelectric nanogenerator and flexible electronics technology for enabling novel micro-/nano-systems","The cutting-edge flexible electronics have experienced an explosion growth in the past decades, driving the traditional rigid devices evolve into soft, light, thin, durable and comfortable devices with more and more addon functionalities. This rapid advancement of flexible electronics induces urgent demand of flexible and sustainable power source to overcome one of the bottlenecks of this technology. Triboelectric nanogenerators, since first invented in 2012, have become a promising energy harvesting technology during the past few years with significant development across the world, due to their diverse flexible/stretchable configurations, no material limitation and high output performance. Beyond energy harvesting, triboelectric nanogenerators can actively function as self-powered sensors and actuators to detect, monitor, interact and respond to the ambient changes induced by environment or human. These triboelectric devices can be key components to achieve sustainable functional systems. Therefore, the complementary marriage of triboelectric nanogenerators and flexible electronics yields a highly advanced technology to realize self-powered, flexible and smart functional systems. In this review, progress development of triboelectric nanogenerators and flexible electronics technology is firstly reviewed. Then various micro-/nano-systems enabled by the integration of triboelectric nanogenerators and flexible electronics technology are presented to show the feasibility to achieve sustainable functional systems. At the end, a future prospect of ""all-in-one"" multi-functional smart system is proposed under the same flexible platform towards convenient, miniaturized and sustainable micro-/nano-systems.","10.1016/j.nanoen.2019.01.002","28924","0.2169643"
"517","hot_topic_cluster_no_6","rank_4","798","3D printed supercapacitor using porous carbon derived from packaging waste","There is considerable interest in developing sustainable fabrication technologies such as additive manufacturing or 3D printing for solid-state supercapacitors. The 3D printing of supercapacitors offers advantages such as rapid prototyping of 3D electrode structures that can improve device performance. Most of the efforts on 3D printing of such devices till date have faced some challenges including low capacitance shown by printed devices, significant post-processing on printed electrodes, and the use of non-additive fabrication methods to make the solid electrolyte. To better derive the benefits of 3D printing, it is important to address these challenges. In this paper, we demonstrate a solid-state supercapacitor made by extrusion-based 3D printing without post-processing, using a commercial 3D printer. Both the electrodes and electrolyte were 3D printed. The electrode material is activated carbon synthesized from packaging waste. The device shows a capacitance of 328.95 m F/cm(2) at 2.5 mA, the highest among all solid-state supercapacitors made by extrusion-based 3D printing till date. This capacity is attributed to the porous carbon used as active material, and the high loading of this carbon in the electrodes. To the best of our knowledge, this is the first report of a 3D printed solid-state supercapacitor using activated carbon derived from waste material. In the context of sustainable development of supercapacitors, the work described here represents a significant technological advance.","10.1016/j.addma.2020.101525","  588","0.2186813"
"518","hot_topic_cluster_no_6","rank_4","798","Antibacterial triboelectric membrane-based highly-efficient self-charging supercapacitors","Supercapacitors as energy storage components can store but cannot generate electric energy, resulting in the periodical charging issues when sustainably powering electronic devices due to limited capacities. Here we report the feasibility of utilizing two supercapacitors to simultaneously generate and store electric energy by themselves for addressing the periodical charging issues. Flexible supercapacitors were fabricated by using activated carbons as electrodes and polybenzimidazole as both the separator and the solid polymer electrolyte when doped with H3PO4. Through the coupling of contact triboelectrification and electrostatic induction, the self-charging capability has been realized due to the ambient wind-induced vibrations of a triboelectric membrane between the two supercapacitors, where it can be easily charged up to 1 V in about 33 s. This research can provide important applications in self-powered electronics and smart windows.","10.1016/j.nanoen.2017.04.029","46648","0.2250000"
"519","hot_topic_cluster_no_6","rank_4","798","Electrochemical Oxidation of Para-Aminophenol With Rare Earth Doped Lead Dioxide Electrodes: Kinetics Modeling and Mechanism","In this study, La and Ce doped PbO2 electrodes were prepared and the characteristic of the electrodes were discussed with the help of structure analysis. The catalytic effects of the doped electrodes were explored through the degradation of para-aminophenol wastewater. The results showed that the para-aminophenol removal was 96.96%, 89.34%, and 77.55% after 180 min treatment with Ce-PbO2, La-PbO2, and PbO2, respectively. The para-aminophenol enhanced degradation mechanism was discussed with rare earth element doping electrodes and a kinetic model was established based on radical reactions mechanism with genetic algorithm (GA) calculation. The reaction constants of these electrodes were calculated and the results showed that the reaction constant of Ce-PbO2 electrode was the highest, which indicated that Ce-PbO2 electrode could have a better treatment effect. The EE/O was used as the index of energy consumption efficiency and the results were calculated and compared. This paper could provide basic data and technique reference of the prediction the oxidation reaction process of different electrodes for the electrochemical oxidation application in wastewater treatment.","10.3389/fchem.2019.00382","25333","0.2413793"
"520","hot_topic_cluster_no_6","rank_4","798","Functional Carbon-Based Nanomaterials for Energy Storage: Towards Smart Textile Supercapacitors","Hybrid supercapacitors emerged as an eco-friendly technology to address the grand challenge of sustainable and efficient energy storage. Functional carbon-based nanomaterials are promising building blocks for the design of advanced electrodes for this type of supercapacitors. The boost on wearable electronics opened new market opportunities for hybrid supercapacitors integrated in textiles using carbon-based electrodes. In this work, we will start by providing a brief introduction to the main principles of supercapacitors, followed by the importance of carbon ( nano) materials as electrodes for the design of high-performance supercapacitors for energy storage. Subsequently, the progress achieved by our team in the field of hybrid carbon-metal oxide nanomaterials and smart textile supercapacitors will be highlighted.","","59039","0.2518519"
"521","hot_topic_cluster_no_6","rank_5","466","Making Sense of Learning Analytics Dashboards: A Technology Acceptance Perspective of 95 Teachers","The importance of teachers in online learning is widely acknowledged to effectively support and stimulate learners. With the increasing availability of learning analytics data, online teachers might be able to use learning analytics dashboards to facilitate learners with different learning needs. However, deployment of learning analytics visualisations by teachers also requires buy-in from teachers. Using the principles of technology acceptance model, in this embedded case-study, we explored teachers' readiness for learning analytics visualisations amongst 95 experienced teaching staff at one of the largest distance learning universities by using an innovative training method called Analytics4Action Workshop. The findings indicated that participants appreciated the interactive and hands-on approach, but at the same time were skeptical about the perceived ease of use of learning analytics tools they were offered. Most teachers indicated a need for additional training and follow-up support for working with learning analytics tools. Our results highlight a need for institutions to provide effective professional development opportunities for learning analytics.","","33517","0.1513889"
"522","hot_topic_cluster_no_6","rank_5","466","A generalised exergy-based framework for sustainable design of IT systems","This paper proposes an exergy-based approach to evaluate the sustainability of different Information Technology (IT) systems. We suggest the need for metrics that include a second-law component as well as a life-cycle view in evaluating sustainability. Having identified such a metric, we demonstrate applicability of the framework for a sample IT system. We conclude by reflecting upon additional research needs and challenges associated with widespread implementation of such a framework.","10.1504/IJEX.2011.042065","80868","0.1833333"
"523","hot_topic_cluster_no_6","rank_5","466","The Smart City Business Model Canvas-A Smart City Business Modeling Framework and Practical Tool","Cities are challenged with increasing population growth and need to implement smart solutions to become more resilient to economic, environmental, and social challenges posed by ongoing urbanization. This study reviewed business model development frameworks and developed a practical tool to help cities assess business models by adapting components of the Business Model Canvas (BMC) and adding new ones that operationalize the smart city dimensions. The Smart City BMC (SC-BMC) proposed provides a practical framework that supports developing and communicating a more holistic and integrated view of a smart city business model. It also supports creatively innovating toward more sustainable value creation. As a framework, the SC-BMC bridges sustainable value creation for business model development and smart city innovation.","10.3390/en12244798","19053","0.1872727"
"524","hot_topic_cluster_no_6","rank_5","466","Enabling Big Geoscience Data Analytics with a Cloud-Based, MapReduce-Enabled and Service-Oriented Workflow Framework","Geoscience observations and model simulations are generating vast amounts of multi-dimensional data. Effectively analyzing these data are essential for geoscience studies. However, the tasks are challenging for geoscientists because processing the massive amount of data is both computing and data intensive in that data analytics requires complex procedures and multiple tools. To tackle these challenges, a scientific workflow framework is proposed for big geoscience data analytics. In this framework techniques are proposed by leveraging cloud computing, MapReduce, and Service Oriented Architecture (SOA). Specifically, HBase is adopted for storing and managing big geoscience data across distributed computers. MapReduce-based algorithm framework is developed to support parallel processing of geoscience data. And service-oriented workflow architecture is built for supporting on-demand complex data analytics in the cloud environment. A proof-of-concept prototype tests the performance of the framework. Results show that this innovative framework significantly improves the efficiency of big geoscience data analytics by reducing the data processing time as well as simplifying data analytical procedures for geoscientists.","10.1371/journal.pone.0116781","66330","0.2000000"
"525","hot_topic_cluster_no_6","rank_5","466","Defining analytics maturity indicators: A survey approach","The ability to derive new insights from data using advanced machine learning or analytics techniques can enhance the decision-making process in companies. Nevertheless, researchers have found that the actual application of analytics in companies is still in its initial stages. Therefore, this paper studies by means of a descriptive survey the application of analytics with regards to five different aspects as defined by the DELTA model: data, enterprise or organization, leadership, targets or techniques and applications, and the analysts who apply the techniques themselves. We found that the analytics organization in companies matures with regards to these aspects. As such, if companies started earlier with analytics, they apply nowadays more complex techniques such as neural networks, and more advanced applications such as HR analytics and predictive analytics. Moreover, analytics is differently propagated throughout companies as they mature with a larger focus on department-wide or organization-wide analytics and a more advanced data governance policy. Next, we research by means of clustering how these characteristics can indicate the analytics maturity stage of companies. As such, we discover four clusters with a clear growth path: no analytics, analytics bootstrappers, sustainable analytics adopters and disruptive analytics innovators. ","10.1016/j.ijinfomgt.2016.12.003","46606","0.2146341"
"526","hot_topic_cluster_no_7","rank_1","680","A Comprehensive Study on IoT Based Accident Detection Systems for Smart Vehicles","With population growth, the demand for vehicles has increased tremendously, which has created an alarming situation in terms of traffic hazards and road accidents. The road accidents percentage is growing exponentially and so are the fatalities caused due to accidents. However, the primary cause of the increased rate of fatalities is due to the delay in emergency services. Many lives could be saved with efficient rescue services. The delay happens due to traffic congestion or unstable communication to the medical units. The implementation of automatic road accident detection systems to provide timely aid is crucial. Many solutions have been proposed in the literature for automatic accident detection. The techniques include crash prediction using smartphones, vehicular ad-hoc networks, GPS/GSM based systems, and various machine learning techniques. With such high rates of deaths associated with road accidents, road safety is the most critical sector that demands significant exploration. In this paper, we present a critical analysis of various existing methodologies used for predicting and preventing road accidents, highlighting their strengths, limitations, and challenges that need to be addressed to ensure road safety and save valuable lives.","10.1109/ACCESS.2020.3006887","17188","0.2687500"
"527","hot_topic_cluster_no_7","rank_1","680","Green vehicle traffic routing system using ant-based algorithm","Vehicle traffic congestion leads to air pollution, driver frustration, and costs billions of dollars annually in fuel consumption. Finding a proper solution to vehicle congestion is a considerable challenge due to the dynamic and unpredictable nature of the network topology of vehicular environments, especially in urban areas. Vehicle Traffic Routing Systems (VTRSs) are one of the most significant solutions for this problem. Although most of the existing VTRSs obtained promising results for reducing travel time or improving traffic flow, they cannot guarantee reduction of the traffic-related nuisances such as air pollution, noise, and fuel consumption. Hence, in this paper, we present a green VTRS that reduces fuel consumption and consequently CO2 emissions via ant-based algorithm combined with fuel consumption model. This VTRS is an Ant-based Vehicle Congestion Avoidance System (AVCAS) that uses Signalized Intersection Design and Research Aid (SIDRA) fuel consumption and emission model in its vehicle routing procedure. This approach is called AVCAS +SIDRA which utilizes various criterion such as average travel time, speed, distance, vehicle density along with road map segmentation to reduce fuel consumption as much as possible by finding the least congested shortest paths in order to reduce the vehicle traffic congestion and their pollutant emissions. The proposed approach is evaluated and validated through simulation environment and tools (i.e. NS-2, SUMO, TraNS). Experimental results conducted on three different scenarios (i.e. various vehicle densities, system usage rates and accident condition) considering average travel time, travel speed, travel distance and fuel consumption as evaluation metrics. The obtained results show that the AVCAS + SIDRA outperforms the existing approaches in terms of average travel time, average travel speed and fuel consumption rate, by an average of 25.5%, 19.5% and 17%, respectively. Consequently, our proposed green VTRS alleviates energy consumption as it is not only becoming scarce and expensive but also causing a dramatic climate change and emission. ","10.1016/j.jnca.2015.08.003","62689","0.2880282"
"528","hot_topic_cluster_no_7","rank_1","680","The velocity regulation of power consumption with traffic lights for electric vehicles","Traffic conditions, especially at traffic crossings, have a great impact on the power consumption of vehicles. Regulating velocity using the information between vehicles and traffic systems can decrease the power consumption. This article mainly focuses on an electric vehicle equipped with radar sensors, which can get the traffic information from upto a 100-m-long distance between the controlled vehicle and the traffic lights. Using the information gathered from sensors, the top-level control unit regulates the velocity aiming at lower power consumption. When traveling through crossings, two different traffic conditions are discussed. For the first condition, no other vehicles run between the controlled vehicle and the traffic lights. Only the traffic lights information is considered. For the second condition, the controlled vehicle follows other vehicles to go through the crossing. The information of the nearest front vehicle and traffic lights is taken into consideration. In summary, the traffic lights information, including the controlled vehicle current state, the traffic lights remaining time, and the velocity and distance of the nearest former vehicle (for the second condition) are sent to the top-level control unit. Then, the control unit calculates a velocity list, which will be sent to the vehicle control unit. A simulation is conducted using a traffic simulation software named ""Simulation of Urban Mobility"" to verify the algorithm. The simulation results indicate that the energy efficiency is improved. For the first condition, the travel time is reduced by 8.27%, and the power consumption is reduced by 18.7%. For the second condition, the power consumption is reduced by 2.96%. Finally, for a 5.8-km driving cycle containing both conditions, the travel time is reduced by 6.9% and electricity consumption is reduced by 9.51%.","10.1177/0954407019856220","23790","0.2983471"
"529","hot_topic_cluster_no_7","rank_1","680","Optimal Torque Distribution Strategy for Minimizing Energy Consumption of Four-wheel Independent Driven Electric Ground Vehicle","In order to improve the energy efficiency of a four-wheel independent driven ground vehicle, a vehicle model consisting of the vehicle dynamics model, in-wheel motor model and driver model is built in Matlab/Simulink and Carsim. The feasibility of the control allocation to distribute the total required torque to four in-wheel motors is analyzed and proved. Generally, motor rotation speed and torque have a great influence on the motor efficiency. Therefore, it is an effective method to improve the energy efficiency of vehicle by reasonably allocating the torque of the four in-wheel motors to make the motors work in high efficient regions. The total cost function of vehicle economy and motor torque mutation is constructed, then, the dynamic programming (DP) algorithm is adopted to obtain the optimized control instruction sequence of the motor torque. Simulation of the new control allocation algorithm is carried out on the federal test program (FTP) drive cycle and the urban new European driving cycle (NEDC). The results show that the energy consumption is significantly reduced with the proposed control allocation.","10.6180/jase.201809_21(3).0008","41300","0.3116883"
"530","hot_topic_cluster_no_7","rank_1","680","Cooperative control of regenerative braking and friction braking in the transient process of anti-lock braking activation in electric vehicles","Regenerative braking significantly improves the energy efficiency in electric vehicles. Cooperative control between regenerative braking and friction braking during anti-lock braking control is a critical issue in brake system coupling. For safety concerns, regenerative braking is often terminated at the beginning of anti-lock braking control. Oscillations between activation of an anti-lock braking system and exit from anti-lock braking system control may occur under poorly matched control parameters. To solve these problems, we propose an index that indicates the possibility of activation of an anti-lock braking system. It is derived by a fuzzy logic algorithm which is based on the estimated regenerative braking torque, the estimated friction braking torque and other vehicle state variables. Regenerative braking can be adjusted on the basis of the index to ensure that such braking is of a low level when an anti-lock braking system is activated. Simulations and experiments are carried out to evaluate the effectiveness of the index. The results show that regenerative braking decreases as the index increases, thereby improving the braking safety and the driving comfort during the transient process of activation of an anti-lock braking system.","10.1177/0954407015613193","57828","0.3150685"
"531","hot_topic_cluster_no_7","rank_2","718","Promoting Active Transportation as a Partnership Between Urban Planning and Public Health: The Columbus Healthy Places Program","Active transportation has been considered as one method to address the American obesity epidemic. To address obesity prevention through built-environment change, the local public health department in Columbus, Ohio, established the Columbus Healthy Places (CHP) program to formally promote active transportation in numerous aspects of community design for the city. In this article, we present a case study of the CHP program and discuss the review of city development rezoning applications as a successful strategy to link public health to urban planning. Prior to the CHP review, 7% of development applications in Columbus included active transportation components; in 2009, 64% of development applications adopted active transportation components specifically recommended by the CHP review. Active transportation recommendations generally included adding bike racks, widening or adding sidewalks, and providing sidewalk connectivity. Recommendations and lessons learned from CHP are provided.","10.1177/00333549111260S107","80233","0.2513889"
"532","hot_topic_cluster_no_7","rank_2","718","Space-time mismatch between transit service and observed travel patterns in the Wasatch Front, Utah: A social equity perspective","In the absence of other alternatives, people who rely on public transportation to conduct their daily activities have travel patterns that differ from discretionary transit users, especially those who choose to use transit for work trips. At the same time, in many regions around the world, public transportation is primarily designed to accommodate peak-hour travel demands in order to reduce congestion and its impacts. It is theorized that this results in a mismatch between the demand and supply of public transportation among populations at risk of social exclusion. In this research, we characterize and compare the spatiotemporal patterns of travel demand and transit supply. Our analysis consists of a comparison between observed travel patterns and a new temporal measure of transit supply based on travel times. We measure travel demand with the observed trip-making characteristics (i.e. origin, destination, time-of-day) of the respondents to two transportation surveys conducted in Utah. Transit supply is characterized using a transit travel time cube, a three-dimensional array of origin-destination transit travel times computed for all origins, destinations and times of day. Mismatch is examined by descriptive and multivariate comparisons of observed trips and computed levels of transit provision. Our results confirm theory: more marginalized groups demand travel between locations at times of the day that are poorly served by transit. However, when controlling for all variables simultaneously in a multivariate regression, few socioeconomic factors remain significant, indicating the overall importance of employment status, making work trips, and traveling during peak times, in explaining mismatch. ","10.1016/j.tbs.2016.01.001","59566","0.2589147"
"533","hot_topic_cluster_no_7","rank_2","718","Household travel mode choice estimation with large-scale data-an empirical analysis based on mobility data in Milan","Data analysis plays a key role in supporting the development of sustainable transportation. Using the large-scale household mobility survey data collected in Milan, Italy during 2005-2006, we study whether the large-scale data contribute to improving accuracy in estimating household travel modes. This paper presents three machine learning methods including multinomial logit (MNL) model, random forest (RF) and support vector machine (SVM) to estimate the household travel mode. Their model accuracies are 70.41%, 71.89%, 72.74% respectively under the full sample size. It is found that the accuracies of these three methods fluctuate fiercely when the sample size is less than 20,000 and then stabilize gradually with continuous increasing it. After stabilization occurs, accuracies with these three methods do not significantly increase as the sample size continues to increase. We also study the travel characteristics derived from the large-scale survey data, which is fundamental for developing a sustainable transportation system. The collected data items include five explanatory variables, i.e., household size (HS), vehicle ownership, household income (HI), travel distance, travel time and one response variable (i.e., household travel mode), which includes public transport (PT), private car, usage of PT and private car simultaneously and the others travel modes (e.g., walk). We further investigate the importance of explanatory variables in terms of estimating household travel mode choice with the MNL model. It is found that vehicle ownership is the most critical factor influencing household travel mode choice, followed by travel distance, travel time, HS and HI. The ranking result is consistent with the RF approach.","10.1080/15568318.2019.1686782","20270","0.2675214"
"534","hot_topic_cluster_no_7","rank_2","718","Exploring Individual Travel Patterns Across Private Car Trajectory Data","Understanding the travel behavior of private cars will generate promising solutions on addressing urban problems such as alleviating traffic congestion and improving transport services. In this paper, we focus on investigating the individual travel patterns of private car users based on a large-scale private car trajectory dataset. To achieve this goal, we first analyze the stop-and-wait information from the private car trajectory data and utilize DBSCAN method to implement clustering with the aim at identifying the frequently-visit places (FVPs). After that, we leverage Markov chain to study the spatial-temporal transition characteristics when private cars travel among their FVPs. Finally yet importantly, we design the concept of spatial-temporal entropy rate and conduct a quantitative study for measuring the regularity of each individual private car's mobility. We validate the proposed methodology based on a real-world dataset including 25,564 private cars driving during one month in China. Extensive experiments demonstrate that the proposed method outperforms the existing methods in terms of the accuracy on measuring the mobility behavior. Moreover, we observe that, on one side, the travel pattern is easier to mine from the private car users with fewer FVPs, on the other side, there are also a small number of users whose FVPs are large, while their mobility are relatively regular. Our work is the first effort to explore individual travel patterns of private car users via studying private car trajectory big data, thereby being able to provide new insight into the research of human travel activities, traffic management and urban planning.","10.1109/TITS.2019.2948188"," 1203","0.2925926"
"535","hot_topic_cluster_no_7","rank_2","718","Sustainable transportation for Indian cities: role of intelligent transportation systems","This article highlights the relationship between sustainability (or maintainability) and efficiency of transport systems. It also outlines some of the problems and issues that exist in providing mobility to urban Indians. Finally, it enumerates some of the ways in which application of (modern) information and communication technologies can help improve the efficiency of the transportation system and ultimately help achieve a sustainable urban transportation system.","","80160","0.3105263"
"536","hot_topic_cluster_no_7","rank_3","742","A Novel Tensile Specimen Configuration for the Characterization of Bulk Mycelium Biopolymer","Mycelium is a biopolymer that has the potential to be a sustainable replacement for petroleum based foams in some engineering applications. During a recent study, with the objectives to first relate the effects of growing conditions on material properties and second to develop a model to predict the bulk behavior of mycelium, it was found that a variety of standard test methods are being used for the tensile characterization of mycelium in the literature. In the current study, these test methods were found to cause stress concentrations that resulted in maximum loads away from the gauge section, griping conditions that crushed the test samples through the thickness, and gauge sections that were smaller than the level of heterogeneity typically observed in samples of mycelium biopolymer. A new tensile specimen configuration has been developed that mitigates the issues associated with other specimen configurations. This new specimen configuration is used to determine the tensile strength, tensile modulus, and Poisson's ratio of mycelium biopolymer specimens.","10.1007/s40799-019-00348-6","21069","0.2564516"
"537","hot_topic_cluster_no_7","rank_3","742","Preparation of Silk Nanowhisker-Composited Amphoteric Cellulose/Chitin Nanofiber Membranes","Native biopolymer nanofibers (cellulose, chitin, and silk nanofiber) are one of the most important contributors to the outstanding functions and mechanical properties of natural materials. To enhance the mechanical performance, A great deal of top-down routes have been reported to prepare biopolymers nanofibers/nanowhiskers that retaining their nanostructures. Compared with advances in cellulose and chitin nanofibers/nanowhiskers, it remains difficult for direct downsizing the natural silk fibers into silk nanofibers/nanowhiskers (SNFs/SNWs) because of their high crystallinity and sophisticated structures. In this work, environmentally friendly and recyclable deep eutectic solvents (DESs) were used to direct pretreat and downsize natural silks into silk nanowhiskers with high yield. SNWs with similar diameter (3.1-22 nm for OA/ChCl DES treated SNWs, 2.7-20 nm for CA/ChCl DES treated SNWs) and contour length (329 +/- 140 nm for OA/ChCl DES treated SNWs, 365 +/- 200 nm for CA/ChCl DES treated SNWs) to individual nanofibers in natural silk fibers were obtained. In addition, the separated DES with a recovery yield of at least 92% could be reused four times to produce SNWs, indicating the possibility of DESs as green solvents for sustainable biopolymer nanomaterial extraction. Based on the inherent amphoteric properties of SNWs, multicompatibility was explored to facilely composite SNWs with various polymers for preparation of coextruded membranes with enhanced performance and endowed the composites with protein-endowed double adsorption properties. Overall, this work demonstrated that the DES pretreatment process is promising for green and low-cost biopolymer nanomaterial extraction and that the SNWs prepared via DES have good prospects as nanoscale materials in the environmental field and in development of smart biomaterials and drug delivery in biomedicine.","10.1021/acs.biomac.0c00223","12814","0.2792000"
"538","hot_topic_cluster_no_7","rank_3","742","Parametric study for tensile properties of molded high-density polyethylene for applications in additive manufacturing and sustainable designs","Test specimens following ASTM D638 standards are frequently used to measure the tensile properties of reinforced and unreinforced polymers machined with traditional machining and emerging manufacturing methods (additive manufacturing/3D printing). However, designs of large engineering structures may rely on mechanical properties based on ASTM D3039 for fiber-reinforced polymer composites. This parametric study examines the scaling effects present in uniaxial tensile test specimens of molded high-density polyethylene (HDPE), with geometries ranging from Types I to IV of ASTM D638 to ASTM D3039. HDPE is a thermoplastic polymer that is recyclable, can be 3D-printed, and has a wide range of engineering applications, from bottles to pipes to radiation protection shielding. The mechanical properties test results for the molded HDPE samples are validated using a Monte Carlo simulation to estimate uncertainties for the probability distribution of maximum stress at the yield point. A Finite Element study based on the empirical model shows how the proposed approach can be adopted for design purposes. The results of this work are a useful tool to enhance confidence in the tensile mechanical properties of ASTM D638 Types II and IV geometries as statistically similar to those of ASTM D3039 samples, impacting engineering designs with traditional and emerging manufacturing methods.","10.1002/app.49283","12235","0.2829268"
"539","hot_topic_cluster_no_7","rank_3","742","Red-mud geopolymer composite encapsulated phase change material for thermal comfort in built-sector","In housing sector, large quantity of energy is expended on thermal comfort and other domestic routines which increase the energy consumption and drives up demand, thus, contributes to greenhouse gas emissions. The development of smart, energy-saving enhanced materials can reduce these energy consumption and emission. In this study, expanded graphite (EG) is used to encapsulate phase change material (PCM) to produce EG/paraffin composites. And the EG/paraffin composite was mixed with a geopolymer aggregates using the vacuum impregnation technique to produce enhanced thermal wall material for building and construction industry. Effect of EG/paraffin composite on the thermophysical properties, compressive strength and thermal behaviour of the geopolymer composite wall was investigated by thermal constant analyser, uniaxial compressive strength, differential scanning calorimetry (DSC), and thermogravimetric analysis (TGA). The result of thermal conductivity, heat permeability, specific heat and density for the EG/PCM geopolymer composite are 46.4321 kj/kg, 2.4561 W/mk, 2.1834 Mj/m(3)k and 873.32 kg/m(3) respectively. While the compressive strength for the EG/PCM geopolymer composite was 10.3 MPa. Thermophysical properties of the EG/PCM geopolymer composite increased with increased concentration of EG/paraffin composite in the geopolymer aggregate, however, the compressive strength is reduced. This is because of the low rigidity and strength of the EG/PCM composite inside the geopolymer aggregates. A 25% EG/PCM concentration inside the geopolymer aggregates was the optimal mixing concentration that is high enough for application in building and construction industries. The PCM and EG/PCM composite melting and crystallization temperature curves exhibited identical peaks, which implies a stable composite was formed. The melting enthalpy and crystallization enthalpy of the EG/PCM are 131.13 J/g and 126.77 J/g, respectively. Whereas, melting and crystallization specific heat value for the EG/PCM are 26.11 J/g and 17.42 J/g, respectively. The outdoor test result of the EG/PCM composite geopolymer wall material thermal ability demonstrated higher thermal storage performance than the conventional cementite, clay, and gypsum. The EG/PCM composite geopolymer wall average temperature was 51 degrees C during the day time and 36 degrees C all through the night till 7 am. EG/PCM geopolymer composite was thermally and chemically stable under varying solar conditions. The EG/PCM composite geopolymer aggregate can be an excellent thermal comfort material for building and construction industries.","10.1016/j.solener.2019.02.029","28148","0.3170455"
"540","hot_topic_cluster_no_7","rank_3","742","Reinforcing silicone with hemp fiber for additive manufacturing","This study explores the 3D printability of a new material based on silicone and hemp fibers from renewable, sustainable and non-petroleum resources with the aim of enhancing mechanical properties of silicone. Incorporation of fibers improved the mechanical properties of the silicone matrix, but it also adversely affected the printability of silicone due to the high viscosity. Therefore, an additional solvent is added into the composition to alter the viscosity. To mature the composite printing technology, this research aims to find out the desired mixing composition of silicone, hemp fiber, and solvent. The behavior of the new engineered material was analyzed using rheological study to obtain a printable material. The composition containing 15 wt% hemp fibers and 20 wt % solvent with enhanced mechanical properties displayed the desirable printability. Moreover, the mechanical properties of the 3D printed and molded samples were studied. The results revealed that 3D printed samples outperformed the molded counterparts. Finally, a honeycomb structure and a simple gripper were fabricated to demonstrate the application of the developed material.","10.1016/j.compscitech.2020.108139"," 8197","0.4000000"
"541","hot_topic_cluster_no_7","rank_4","472","Energy absorption mechanics and design optimization of CFRP/aluminium hybrid structures for transverse loading","This study aimed to explore bending collapse behavior and energy absorption capacity of net aluminum (Al), net carbon fiber reinforced plastic (CFRP) and Al/CFRP hybrid tubes, respectively. Based upon the experimental tests, the transverse energy absorption of the Al/CFRP hybrid tubes was found to be even higher than the sum of corresponding net Al tube and net CFRP tube. Specifically, the CF-AL tube (the CFRP tube being placed outside the Al tube) increased the peak force by 6.7% and energy absorption by 20.6%. The AL-CF tube (the CFRP tube being placed inside the Al tube) improved the peak force by 14.1% and energy absorption by 19.1%. The experimental study indicated that overall, the CF-AL tube was of better crashworthiness characteristics. Subsequently, finite element (FE) analyses were carried out by correlating with the experimental results. Based upon this validated FE models, a parametric study and design optimization on the CF-AL tube (with respect to length, thickness and ply angle) were further performed. It was found that the optimization increased specific energy absorption (SEA) by 42.96% and mean crushing force by 37.75%; meanwhile the mass of optimum design decreased by 5.02%, exhibiting significant enhancement of crashworthiness characteristics.","10.1016/j.ijmecsci.2018.10.043","31729","0.2613636"
"542","hot_topic_cluster_no_7","rank_4","472","Regulating Charge-Transfer in Conjugated Microporous Polymers for Photocatalytic Hydrogen Evolution","Bandgap engineering in donor-acceptor conjugated microporous polymers (CMPs) is a potential way to increase the solar-energy harvesting towards photochemical water splitting. Here, the design and synthesis of a series of donor-acceptor CMPs [tetraphenylethylene (TPE) and 9-fluorenone (F) as the donor and the acceptor, respectively], F0.1CMP, F0.5CMP, and F2.0CMP, are reported. These CMPs exhibited tunable bandgaps and photocatalytic hydrogen evolution from water. The donor-acceptor CMPs exhibited also intramolecular charge-transfer (ICT) absorption in the visible region (lambda(max)=480nm) and their bandgap was finely tuned from 2.8 to 2.1 eV by increasing the 9-fluorenone content. Interestingly, they also showed emissions in the 540-580 nm range assisted by the energy transfer from the other TPE segments (not involved in charge-transfer interactions), as evidenced from fluorescence lifetime decay analysis. By increasing the 9-fluorenone content the emission color of the polymer was also tuned from green to red. Photocatalytic activities of the donor-acceptor CMPs (F0.1CMP, F0.5CMP, and F2.0CMP) are greatly enhanced compared to the 9-fluorenone free polymer (F0.0CMP), which is essentially due to improved visible-light absorption and low bandgap of donor-acceptor CMPs. Among all the polymers F0.5CMP with an optimum bandgap (2.3 eV) showed the highest H-2 evolution under visible-light irradiation. Moreover, all polymers showed excellent dispersibility in organic solvents and easy coated on the solid substrates.","10.1002/chem.201805478","28207","0.2686957"
"543","hot_topic_cluster_no_7","rank_4","472","The energy-absorbing characteristics of filament wound hybrid carbon fiber-reinforced plastic/polylactic acid tubes with different infill pattern structures","The study aims to investigate the effect of different infill pattern structures on the energy-absorbing characteristics of single filament wound carbon fiber-reinforced plastic tubes, single polylactic acid and hybrid carbon fiber-reinforced plastic/ polylactic acid tubes under quasi-static axial compression condition, which were fabricated using filament winding and additive manufacturing techniques. Five infill pattern structures of single polylactic acid tubes and hybrid tubes were studied and compared on their energy-absorbing characteristics, which referred to normal, triangle, square, hexagonal and tetrahedral patterns. It concluded that the effect of the infill pattern structure had a significant influence on energy-absorbing characteristics of single polylactic acid and hybrid carbon fiber-reinforced plastic/polylactic acid tubes. For pure polylactic acid tubes, the triangle infill pattern tube represented the highest values of energy absorption (EA) of 0.75 kJ, specific energy absorption (SEA) of 28.50 J/g, compressive strength and modulus of 69.72 MPa and 1.40 GPa, yield strength of 27.80 MPa, peak crushing force (F-peak) of 23.13 kN and mean crushing force (F-mean) of 18.82 kN. For the hybrid carbon fiber-reinforced plastic/polylactic acid tube, tetrahedral infill pattern tube showed the highest values of EA with 0.99 kJ, SEA with 29.66 J/g, F-peak with 22.68 MPa and yield strength with 29.91 MPa. Energy absorption interaction (EA(interaction)) and interaction ratio (phi(e)) of all specimens were evaluated, which showed that the tetrahedral infill pattern tube recorded the highest of all hybrid tubes with 259.92 J and 35.72 %. The result revealed that the tetrahedral pattern displayed better crashworthiness in terms of crushing force efficiency (CFE), EA and SEA in the hybrid structure, which had greater potential to apply as energy absorbers. Moreover, triangle and square infill patterns of hybrid tubes provided the negative interaction effect results, which conducted lower energy-absorbing characteristics compared to individual tubes, respectively.","10.1177/0731684419868018","23514","0.2703704"
"544","hot_topic_cluster_no_7","rank_4","472","Thermal, Structural, and Mechanical Effects of Nanofibrillated Cellulose in Polylactic Acid Filaments for Additive Manufacturing","As the additive manufacturing process gains worldwide importance, the need for bio-based materials, especially for in-home polymeric use, also increases. This work aims to develop a composite of polylactic acid (PLA) and nanofibrillated cellulose (NFC) as a sustainable approach to reinforce the currently commercially available PLA. The studied materials were composites with 5 and 10% NFC that were blended and extruded. Mechanical, structural, and thermal characterization was made before its use for 3D printing. It was found that the inclusion of 10% NFC increased the modulus of elasticity in the filaments from 2.92 to 3.36 GPa. However, a small decrease in tensile strength was observed from 55.7 to 50.8 MPa, which was possibly due to the formation of NFC aggregates in the matrix. This work shows the potential of using PLA mixed with NFC for additive manufacturing.","10.15376/biores.15.4.7954-7964"," 2609","0.2708333"
"545","hot_topic_cluster_no_7","rank_4","472","Acrylate-assisted fractal nanostructured polymer dispersed liquid crystal droplet based vibrant colored smart-windows","We have studied liquid crystals (LCs) and acrylate-assisted thiol-ene compositions to synthesize dye based colorful polymer dispersed liquid crystals (PDLCs) without using a photo-initiator for smart-windows applications. A typical PDLC mixture was prepared by mixing LCs with UV-curable monomers, which included triethylene glycol diacrylate (TEGDA), trimethylolpropane diallyl ether (TMPDE, di-functional ene monomer), trimethylolpropane tris(3-mercaptopropionate) (TMPTMP, a thiol as a cross-linker), and a dichroic dye. The ratios of the TMPDE/TMPTMP and the LCs/TEGDA showed significant effects in altering the properties of the UV-cured PDLCs. During the curing process, the monomers polymerize and led to the encapsulation of the LCs in the form of interesting fractal nanostructures by a polymerization induced phase separation process. The switching time, electro-optical properties, power consumption, and ageing of the fabricated PDLCs were investigated. It was possible to achieve a 70-80% contrast (T) at a voltage difference of approximate to 70 V with a fast switching time () as low as < 20 milliseconds (ms) and low power consumption. These PDLCs had a low threshold voltage that ranged between 10 and 20 V. The sustainability of the fabricated UV-cured PDLCs was analyzed for up to 90 days, and the PDLCs were observed to be stable.","10.1039/c9ra00729f","31364","0.2770642"
"546","hot_topic_cluster_no_7","rank_5","308","Sustainable consumption in the circular economy. An analysis of consumers' purchase intentions for waste-to-value food","This study is aimed at evaluating the relative influence of socio-demographic and psychological features that rule the extent to which consumers engage in the circular economy, purchasing waste-to-value (WTV) food enriched with ingredients otherwise wasted in the supply chain. 477 Italian consumers replied to a web-based questionnaire administered through different social media networks. Two different consumers' purchase intentions were analysed: consumers were asked both if they would be willing to buy WTV food and if they would buy WTV food if this would help to reduce the environmental impact of agricultural production. Binary logistic regressions are estimated to appraise the eventual drivers of consumers' statements. Among these drivers, attention was given to aspects related to the generalised aversion to new foods, i.e. food neophobia (FN) and the aversion to food processed in new ways, i.e. food technology neophobia (FTN). Other relevant economic and demographic factors were investigated, together with aspects related to generalised trust, purchase behaviours and preferences. The main results indicate that 56% of respondents declared to be willing to buy WTV food, however, FN and FTN negatively influence the probability of stating a positive purchase intention. Consumers who give importance to reading food labels and think that food could have environmental or health benefits, are more likely to be willing to buy WTV food. In addition, a core of sustainable consumers seems to emerge who express a positive purchase intention for WTV food to reduce the environmental impact of production and give importance to the origin and nutritional values of products. In conclusion, policy implications are drawn. ","10.1016/j.jclepro.2019.119870","12221","0.2103448"
"547","hot_topic_cluster_no_7","rank_5","308","Sustainability Performance of Certified and Non-certified Smallholder Coffee Farms in Uganda","The transition toward sustainable agricultural production can be supported with improved insight into the performance of existing farming systems. The Sustainability Assessments of Food and Agriculture Systems (SAFA) framework published by the Food and Agriculture Organization (FAO) provides a comprehensive and harmonised framework to assess and compare farming systems. We used the indicator-based SAFA consistent Sustainability Monitoring and Assessment RouTine (SMART) Farm Tool to analyze and compare the sustain ability performance of certified organic and fair trade as well as non-certified smallholder farms in Uganda. Using the respective sustainability scores, we analyzed the synergies and trade-offs between sustainability themes using the non-parametric Spearman correlation test. We find that certification is associated with improved sustainability performance of smallholder coffee farms. It enhances the achievement of governance goals through its influences on group organization and collective capacities - this results in positive effects on other sustainability dimensions. Major synergies were observed between social and governance themes, and between economic and environmental themes. Although, the extent and distribution of the synergies and trade-offs varied among farms, they were consistent between the production systems. These results show that the production systems can potentially have more influence on the sustainability performance than certification per se.","10.1016/j.ecolecon.2018.09.004","29800","0.2156250"
"548","hot_topic_cluster_no_7","rank_5","308","A global meat tax: from big data to a double dividend","The Food and Agriculture Organization of the United Nations (FAO) emphasizes the right of everyone to have access to safe, sufficient and nutritious food in its Rome Declaration. This article suggests how this noble FAO goal can be achieved. We suggest that a first step could be the introduction of a global meat tax, where the size of the negative externalities from meat production could be calculated based on foresight and big data. Applying the tool of a global meat tax will lead to a ""double dividend"" as negative externalities are reduced and at the same time huge tax revenues will be generated which could be used to make further steps in the direction of achieving the stated FAO goal in the Rome Declaration.","10.17221/270/2016-AGRICECON","41703","0.2244898"
"549","hot_topic_cluster_no_7","rank_5","308","A framework for food supply chain digitalization: lessons from Thailand","The promise of digitalization is enormous and nowhere is it more critical than in its potential to transform food supply chain. Consumers have become more educated and are demanding real-time updated information on foods they consumed through digital media. They are also increasingly demanding to know if the foods they consume are environmentally and socially sustainable or not. As a result, food product traceability, safety, and sustainability issues have become crucial concerns to food retailers, distributors, processors, and farmers. Digitalization allows food supply chains to be highly connected, efficient, and responsive to customer needs and regulation requirements. However, digitalizing a traditional food supply chain is challenging and resource demanding. This is more so for developing countries where moving food from farms to consumers can take months as it travels through an array of middlemen. Unfortunately, little is available in the existing literature on food supply chain digitalization. So far, current researchers mainly explore the benefits of digitalization. Using cases in three companies, this paper explores the practices, challenges, and opportunities faced by Thailand food manufacturers in digitalizing their food supply chains. A framework for food supply chain digitalization is proposed and its implications for research and practices are discussed.","10.1080/09537287.2019.1631462","18984","0.2462500"
"550","hot_topic_cluster_no_7","rank_5","308","A logistics model of sustainable food supply of the region","Introduction. The problem of sustainable food supply is connected with the optimisation of the distribution of food flows and determined by the quantity of food in terms of per capita consumption, energy value and availability of nutrients. The purpose of the article is to develop a logistics model of sustainable food supply in the region and test it on the example of Saratov region of Russia. Methods. In the process of economic-mathematical modelling, the authors used econometric and logistics methods. Results. The article describes the authors' approach to building local food systems. A two-level model of demand for food products was substantiated. The conducted calculations of the indicators of sustainability of the local food systems of Saratov region for the most important types of food products show that three local food systems centred in towns of Kalininsk, Novouzensk and Ershov are fully food self-sufficient. The local food systems of the towns of Volsk, Balashov and Balakovo have difficulties in the matters of self-sufficiency in meat products; and the local food systems of the city of Saratov and the town of Engels have to import meat and milk in order to have quality and sustainable provision of the population with such products. Conclusions. The use of the logistics approach allows us to develop a universal algorithm for the assessment of the overall sustainability of food supply of the region, consider the impact of transportation costs on food consumption by the population, substantiate the rational logistics and delivery routes of agricultural production for its further processing, which will increase the overall sustainability of food supply in a particular region.","10.21003/ea.V164-21","46169","0.2834951"
"551","hot_topic_cluster_no_8","rank_1","36","An efficient cluster-based communication protocol for wireless sensor networks","A wireless sensor network is a network of large numbers of sensor nodes, where each sensor node is a tiny device that is equipped with a processing, sensing subsystem and a communication subsystem. The critical issue in wireless sensor networks is how to gather sensed data in an energy-efficient way, so that the network lifetime can be extended. The design of protocols for such wireless sensor networks has to be energy-aware in order to extend the lifetime of the network because it is difficult to recharge sensor node batteries. We propose a protocol to form clusters, select cluster heads, select cluster senders and determine appropriate routings in order to reduce overall energy consumption and enhance the network lifetime. Our clustering protocol is called an Efficient Cluster-Based Communication Protocol (ECOMP) for Wireless Sensor Networks. In ECOMP, each sensor node consumes a small amount of transmitting energy in order to reach the neighbour sensor node in the bidirectional ring, and the cluster heads do not need to receive any sensed data from member nodes. The simulation results show that ECOMP significantly minimises energy consumption of sensor nodes and extends the network lifetime, compared with existing clustering protocol.","10.1007/s11235-013-9794-y","71103","0.5578947"
"552","hot_topic_cluster_no_8","rank_1","36","LEAUCH: low-energy adaptive uneven clustering hierarchy for cognitive radio sensor network","The integration of wireless sensor network (WSN) and cognitive radio (CR) technology enables a new paradigm of communication: cognitive radio sensor networks (CRSN). The existing WSN clustering algorithm cannot consider the advantage of channel resource brought by CR function in CRSN, and the CR network (CRN) clustering algorithm is designed based on the infinite energy nodes; thus both algorithms cannot operate with energy efficiency in CRSN. The paper proposes a low-energy adaptive uneven clustering hierarchy for CRSN, which can not only consider the advantage of the channel resource in reducing the energy consumption but also employ uneven clustering method for balancing the energy consumption among the cluster heads under multiple hops transmission means. Simulation results show that compared with the existing several typical clustering algorithms including WSN and CRSN clustering algorithms, low-energy adaptive clustering hierarchy (LEACH), HEED, energy-efficient unequal clustering (EEUC), cognitive LEACH (CogLEACH), and distributed spectrum-aware clustering (DSAC), the proposed algorithm can not only efficiently balance the energy consumption among cluster heads and network load in CRSN but also remarkably prolong the network lifetime.","10.1186/s13638-015-0354-x","65873","0.5701031"
"553","hot_topic_cluster_no_8","rank_1","36","Distributed fuzzy approach to unequal clustering and routing algorithm for wireless sensor networks","Due to inherent issue of energy limitation in sensor nodes, the energy conservation is the primary concern for large-scale wireless sensor networks. Cluster-based routing has been found to be an effective mechanism to reduce the energy consumption of sensor nodes. In clustered wireless sensor networks, the network is divided into a set of clusters; each cluster has a coordinator, called cluster head (CH). Each node of a cluster transmits its collected information to its CH that in turn aggregates the received information and sends it to the base station directly or via other CHs. In multihop communication, the CHs closer to the base station are burdened with high relay load; as a result, their energy depletes much faster as compared with other CHs. This problem is termed as the hot spot problem. In this paper, a distributed fuzzy logic-based unequal clustering approach and routing algorithm (DFCR) is proposed to solve this problem. Based on the cluster design, a multihop routing algorithm is also proposed, which is both energy efficient and energy balancing. The simulation results reinforce the efficiency of the proposed DFCR algorithm over the state-of-the-art algorithms, ie, energy-aware fuzzy approach to unequal clustering, energy-aware distributed clustering, and energy-aware routing algorithm, in terms of different performance parameters like energy efficiency and network lifetime.","10.1002/dac.3709","36142","0.5755319"
"554","hot_topic_cluster_no_8","rank_1","36","MACHFL-FT: a fuzzy logic based energy-efficient protocol to cluster heterogeneous nodes in wireless sensor networks","This article presents a fuzzy-based algorithm to cluster heterogeneous nodes using a fixed threshold in wireless sensor networks (MACHFL-FT). By using three clustering algorithm in different rounds, the proposed method chooses the best sensor nodes as cluster heads. On the other hand, in order to reduce sent messages, it is able to avoid selecting cluster heads by using a fixed threshold value in some rounds. Reducing the number of transmitted messages results in the reduction of nodes' energy consumption, leading to more network energy conservation. In this article, the proposed algorithm (MACHFL-FT) clusters heterogeneous nodes by using three different algorithms. To save more energy, it would avoid holding cluster head elections in some rounds by using a fixed threshold value. The proposed algorithm is compared to other methods in two scenarios. The assessment criteria used in the comparison include the network remaining energy, the number of dead nodes, the first dead node, half of dead nodes and the last dead node. The results show that MACHFL-FT could reduce network energy consumption and prolong network lifetime.","10.1007/s11276-018-1757-5","20769","0.6424242"
"555","hot_topic_cluster_no_8","rank_1","36","HCABS: The hierarchical clustering algorithm based on soft threshold and cluster member bounds for wireless sensor networks","Wireless Sensor Networks (WSNs) for reducing energy consumption and increasing sensors lifetime can use the clustering algorithms. We propose a new energy-efficient hierarchical clustering algorithm based on soft threshold cluster-head election and cluster member bounds for WSNs which called HCABS. Our simulation studies suggest that HCABS achieves longer lifespan and reduce energy consumption in WSNs as well as low latency and moderate overhead across the network.","10.1587/elex.9.685","78599","0.6758621"
"556","hot_topic_cluster_no_8","rank_2","368","Energy Efficiency of Large-Scale Multiple Antenna Systems with Transmit Antenna Selection","In this paper, we perform transmit antenna selection to improve the energy efficiency of large scale multiple antenna systems. We derive a good approximation of the distribution of the mutual information in this antenna selection system. It shows that channel hardening phenomenon is still retained as full complexity with antenna selection. Then, we use this closed-form expression to assess the energy efficiency performance. Specifically, we evaluate the performance of the energy efficiency in two different cases: 1) the circuit power consumption is comparable to or even dominates the transmit power, and 2) the circuit power can be ignored due to relatively much higher transmit power. The theoretical analysis indicates that there exists an optimal number of selected antennas to maximize the energy efficiency in the first case, whereas in the second case, the energy efficiency is maximized when all the available antennas are used. Based on these conclusions, two simple but efficient antenna selection algorithms are proposed to obtain the maximum energy efficiency. All the analytical results are verified through computer simulations.","10.1109/TCOMM.2014.011414.130498","71546","0.3929577"
"557","hot_topic_cluster_no_8","rank_2","368","Modeling and Performance Analysis of Energy Efficiency Binary Power Control in MIMO-OFDM Wireless Communication Systems","The energy efficiency optimization of the binary power control scheme for MIMO-OFDM wireless communication systems is formulated, and then a global optimization solution of power allocation is derived. Furthermore, a new energy efficiency binary power control (EEBPC) algorithm is designed to improve the energy efficiency of MIMO-OFDM wireless communication systems. Simulation results show that the EEBPC algorithm has better energy efficiency and spectrum efficiency than the average power control algorithm in MIMO-OFDM wireless communication systems.","10.1155/2011/946258","80828","0.3980000"
"558","hot_topic_cluster_no_8","rank_2","368","Energy-Efficient SWIPT in IoT Distributed Antenna Systems","The rapid growth of Internet of Things (IoT) dramatically increases power consumption of wireless devices. Simultaneous wireless information and power transfer (SWIPT) is a promising solution for sustainable operation of IoT devices. In this paper, we study energy efficiency (EE) in SWIPT-based distributed antenna system, where power splitting (PS) is applied at IoT devices to coordinate the energy harvesting and information decoding processes by varying transmit power of distributed antenna ports and PS ratios of IoT devices. In the case of single IoT device, we find the optimal closed-form solution by deriving some useful properties based on Karush-Kuhn-Tucker conditions and the solution is no need for numerical iterations. For the case of multiple IoT devices, we propose an efficient suboptimal algorithm to solve the EE maximization problem. Simulation results show that the proposed schemes achieve better EE performance compared with other benchmark schemes in both single and multiple IoT devices cases.","10.1109/JIOT.2018.2796124","35733","0.4000000"
"559","hot_topic_cluster_no_8","rank_2","368","Antenna Selection for Energy-Efficient MIMO Transmission","In previous works on multiple-inputmultiple-output (MIMO) antenna selection, it is usually assumed that the transmit power and the number of active antennas is fixed and the antenna subset is selected to maximize the capacity. In this paper, we jointly optimize the transmit power, the number of active antennas, and the antenna subsets at the transmitter and receiver to maximize the energy efficiency. The optimal solution can be obtained by exhaustive search; suboptimal algorithms are also developed to reduce the complexity. Simulation results show that antenna selection can improve the energy efficiency significantly.","10.1109/WCL.2012.082712.120417","76278","0.4219512"
"560","hot_topic_cluster_no_8","rank_2","368","Energy Efficiency Optimization for OFDM Based 5G Wireless Networks With Simultaneous Wireless Information and Power Transfer","In 5G wireless networks, the system capacity and the number of users will be significantly increased. However, the energy consumed for the communication will also be increased, which results in low system energy efficiency. Simultaneous wireless information and power transfer (SWIPT) can enable the user to perform energy harvesting from the radio-frequency signals parallel with the information decoding, which can effectively improve the system energy efficiency. In this paper, we study the energy efficiency optimization problem for the orthogonal frequency-division multiplexing-based 5G wireless networks with SWIPT, in which the subcarrier and power allocation are jointly optimized to maximize the system energy efficiency for single user and multiple user cases. For those two cases, through mathematical transforming, we use the Dinkelbach iterative method and the Lagrange dual method to solve the non-convex energy efficiency optimization problem. Simulation results show that compared with other algorithms, our proposed algorithm can achieve higher energy efficiency.","10.1109/ACCESS.2018.2883555","40557","0.4537313"
"561","hot_topic_cluster_no_8","rank_3","69","Economical Evaluation and Optimal Energy Management of a Stand-Alone Hybrid Energy System Handling in Genetic Algorithm Strategies","Hybrid renewable energy systems are a promising technology for clean and sustainable development. In this paper, an intelligent algorithm, based on a genetic algorithm (GA), was developed and used to optimize the energy management and design of wind/PV/tidal/ storage battery model for a stand-alone hybrid system located in Brittany, France. This proposed optimization focuses on the economic analysis to reduce the total cost of hybrid system model. It suggests supplying the load demand under different climate condition during a 25-years interval, for different possible cases and solutions respecting many constraints. The proposed GA-based optimization approach achieved results clear highlight its practicality and applicability to any hybrid power system model, including optimal energy management, cost constraint, and high reliability.","10.3390/electronics7100233","34432","0.2040816"
"562","hot_topic_cluster_no_8","rank_3","69","Impeller meridional plane optimization of pump as turbine","How to improve efficiency is still a very active research point for pump as turbine. This article comes up with a method for optimal design of pump as turbine impeller meridional plane. It included the parameterized control impeller meridional plane, the computational fluid dynamics technique, the optimized Latin hypercube sampling experimental design, the back propagation neural network optimized by genetic algorithm and genetic algorithm. Concretely, the impeller meridional plane was parameterized by the Pro/E software, the optimized Latin hypercube sampling was used to obtain the test sample points for back propagation neural network optimized by genetic algorithm, and the model corresponding to each sample point was calculated to obtain the performance values by the computational fluid dynamics techniques. Then, back propagation neural network learning and training are carried out by combining sample points and corresponding model performance values. Last but not least, back propagation neural network optimized by genetic algorithm and genetic algorithm were combined to deal with the optimization problem of impeller meridional plane. According to the aforementioned optimization design method, impeller meridional plane of the pump as turbine was optimized. The result manifests that the optimized pump as turbine energy-conversion efficiency was improved by 2.28% at the optimum operating condition, at the same time meet the pressure head constraint, namely the head difference between initial and optimized model is under the set numeric value. This demonstrates that the optimization method proposed in this article to optimize the impeller meridional plane is practicable.","10.1177/0036850419876542","22236","0.2080000"
"563","hot_topic_cluster_no_8","rank_3","69","Hybrid artificial bee colony-grey wolf algorithm for multi-objective engine optimization of converted plug-in hybrid electric vehicle","The paper proposes a hybrid approach of artificial bee colony (ABC) and grey wolf optimizer (GWO) algorithm for multi-objective and multidimensional engine optimization of a converted plug-in hybrid electric vehicle. The proposed strategy is used to optimize all emissions along with brake specific fuel consumption (FC) for converted parallel operated diesel plug-in hybrid electric vehicle (PHEV). All emissions particulate matter (PM), nitrogen oxide (NOx), carbon monoxide (CO) and hydrocarbon (HC) are considered as optimization parameters with weighted factors. 70 hp engine data of NOx, PM, HC, CO and FC obtained from Oak Ridge National Laboratory is used for the study. The algorithm is initialized with feasible solutions followed by the employee bee phase of artificial bee colony algorithm to provide exploitation. Onlooker and scout bee phase is replaced by GWO algorithm to provide exploration. MATLAB program is used for simulation. Hybrid ABC-GWO algorithm developed is tested extensively for various values of speeds and torque. The optimization performance and its environmental impact are discussed in detail. The optimization results obtained are verified by real data engine maps. It is also compared with modified ABC and GWO algorithm for checking the effectiveness of proposed algorithm. Hybrid ABC-GWO offers combine benefits of ABC and GWO by reducing computational load and complexity with less computation time providing a balance of exploitation and exploration and passes repeatability towards use for real-time optimization.","10.12989/eri.2020.7.1.035","13588","0.2108108"
"564","hot_topic_cluster_no_8","rank_3","69","A Novel Hybrid Model Based on TVIW-PSO-GSA Algorithm and Support Vector Machine for Classification Problems","The increasingly serious haze problem in China has brought about a growing public awareness of air quality. Precise air quality index (AQI) forecasts play an important role in both controlling air pollution and promoting the sustainable development of human society. However, the randomness, non-stationarity, and irregularity of the AQI series make its classifications very difficult. This paper introduces a time-varying inertia weighting (TVIW) strategy based on a combination of gravitation search algorithm (GSA) and particle swarm optimization (PSO) called the TVIW-PSO-GSA. The TVIW-PSO-GSA is utilized to optimize the penalty parameter C and kernel function parameter gamma of a support vector machine (SVM) to create a hybrid TVIW-PSO-GSA-SVM algorithm. Twenty-three benchmark functions, five UCI datasets, and an AQI hierarchical classification example are tested to find that the convergence speed and performance of the TVI-PSO-GSA exceed those of other algorithms, and the TVIW-PSO-GSA-SVM algorithm also achieves higher classification accuracy and efficiency than the PSO-GSA-SVM, GSA-SVM, GA-SVM, or PSO-SVM, which indicates that the TVIW-PSO-GSA-SVM reliably and accurately classifies AQI and UCI datasets.","10.1109/ACCESS.2019.2897644","31549","0.2264706"
"565","hot_topic_cluster_no_8","rank_3","69","Adaptive PSO for online identification of time-varying systems","Novel adaptive PSO (particle swarm optimization) algorithms called OPSO (online PSO) and ?PSO are proposed. These algorithms are designed to prevent increasing of calculation cost in order to embed into online systems, and they have high adaptive performances to environmental changes. The efficiency of the proposed PSO algorithms are demonstrated by numerical simulation and the online identification of an actual dynamic system. ","10.1002/ecj.11391","77253","0.2440000"
"566","hot_topic_cluster_no_8","rank_4","895","A Bit String Content Aware Chunking Strategy for Reduced CPU Energy on Cloud Storage","In order to achieve energy saving and reduce the total cost of ownership, green storage has become the first priority for data center. Detecting and deleting the redundant data are the key factors to the reduction of the energy consumption of CPU, while high performance stable chunking strategy provides the groundwork for detecting redundant data. The existing chunking algorithm greatly reduces the system performance when confronted with big data and it wastes a lot of energy. Factors affecting the chunking performance are analyzed and discussed in the paper and a new fingerprint signature calculation is implemented. Furthermore, a Bit String Content Aware Chunking Strategy (BCCS) is put forward. This strategy reduces the cost of signature computation in chunking process to improve the system performance and cuts down the energy consumption of the cloud storage data center. On the basis of relevant test scenarios and test data of this paper, the advantages of the chunking strategy are verified.","10.1155/2015/242086","68064","0.2774648"
"567","hot_topic_cluster_no_8","rank_4","895","Energy-aware task scheduling in heterogeneous computing environments","Efficient application scheduling is critical for achieving high performance in heterogeneous computing (HC) environments. Because of such importance, there are many researches on this problem and various algorithms have been proposed. Duplication-based algorithms are one kind of well known algorithms to solve scheduling problems, which achieve high performance on minimizing the overall completion time (makespan) of applications. However, they pursuit of the shortest makespan overly by duplicating some tasks redundantly, which leads to a large amount of energy consumption and resource waste. With the growing advocacy for green computing systems, energy conservation has been an important issue and gained a particular interest. An existing technique to reduce energy consumption of an application is dynamic voltage/frequency scaling (DVFS), whose efficiency is affected by the overhead of time and energy caused by voltage scaling. In this paper, we propose a new energy-aware scheduling algorithm with reduced task duplication called Energy-Aware Scheduling by Minimizing Duplication (EAMD), which takes the energy consumption as well as the makespan of an application into consideration. It adopts a subtle energy-aware method to search and delete redundant task copies in the schedules generated by duplication-based algorithms, and it is easier to operate than DVFS, and produces no extra time and energy consumption. This algorithm not only consumes less energy but also maintains good performance in terms of makespan compared with duplication-based algorithms. Two kinds of DAGs, i.e., randomly generated graphs and two real-world application graphs, are tested in our experiments. Experimental results show that EAMD can save up to 15.59 % energy consumption for HLD and HCPFD, two classic duplication-based algorithms. Several factors affecting the performance are also analyzed in the paper.","10.1007/s10586-013-0297-0","70334","0.2905983"
"568","hot_topic_cluster_no_8","rank_4","895","An intelligent efficient scheduling algorithm for big data in communication systems","Recently, a large number of mobile computing devices with embedded systems have been widely employed for big data analysis in communication systems. However, mobile computing devices usually have a limited energy supply. Consequently, green and low-energy computing has become an important research topic for big data analysis in communication systems when using energy-limited mobile computing devices. In this paper, we propose a reinforcement learning-based intelligent scheduling algorithm for big data analysis by increasing the utilization and reducing the energy consumption of the processors. Specially, we design a reinforcement learning model, as an important big data intelligent technique, to select an appropriate dynamic voltage and frequency scaling technique for configuring the voltage and frequency according to the current system state, which can improve the utilization and optimize the energy consumption effectively. Furthermore, we implement a learning algorithm to train the parameters of the reinforcement learning model. Our proposed scheduling approach is able to improve the resource utilization and save the energy for big data analysis in communication systems when performing tasks on mobile computing devices with embedded systems. Simulation results demonstrate that the proposed method can save 2% to 4% energy than the compared algorithm.","10.1002/dac.3465","33240","0.3026316"
"569","hot_topic_cluster_no_8","rank_4","895","Energy-aware scheduling with reconstruction and frequency equalization on heterogeneous systems","With the increasing energy consumption of computing systems and the growing advocacy for green computing, energy efficiency has become one of the critical challenges in high-performance heterogeneous computing systems. Energy consumption can be reduced by not only hardware design but also software design. In this paper, we propose an energy-aware scheduling algorithm with equalized frequency, called EASEF, for parallel applications on heterogeneous computing systems. The EASEF approach aims to minimize the finish time and overall energy consumption. First, EASEF extracts the set of paths from an application. Then, it reconstructs the application based on the extracted set of paths to achieve a reasonable schedule. Finally, it adopts a progressive way to equalize the frequency of tasks to reduce the total energy consumption of systems. Randomly generated applications and two real-world applications are examined in our experiments. Experimental results show that the EASEF algorithm outperforms two existing algorithms in terms of makespan and energy consumption.","10.1631/FITEE.1400399","64839","0.3148148"
"570","hot_topic_cluster_no_8","rank_4","895","A multi-factor monitoring fault tolerance model based on a GPU cluster for big data processing","High-performance computing clusters are widely used in large-scale data mining applications, and have higher requirements for persistence, stability and real-time use and sre therefore computationally intensive. To support large-scale data processing, we design a multi-factor real-time monitoring fault tolerance (MRMFT) model based on a GPU cluster. However, the higher clock frequency of GPU chips results in excessively high energy consumption in computing systems. Moreover, the ability to support a long-lasting high temperature operation varies greatly between different GPUs owing to the individual differences between the chips. In this paper, we design a GPU cluster energy consumption monitoring system based on wireless sensor networks (WSNs) and propose an energy consumption aware checkpointing (ECAC) for high energy consumption problems with the following two advantages: the system sets checkpoints according to actual energy consumption and the device temperature to improve the utilization of checkpoints and reduce time cost; and it exploits the parallel computing features of CPU and GPU to hide the CPU detection overhead in GPU parallel computation, and further reduce the time and energy consumption overhead in the fault tolerance phase. Using ECAC as the constraint and aiming for a persistent and reliable operation, the dynamic task migration mechanism is designed, and the reliability of the cluster is greatly improved. The theoretical analysis and experiment results show that the model improves the persistence and stability of the computing system while reducing checkpoint overhead. ","10.1016/j.ins.2018.04.053","23186","0.3235849"
"571","hot_topic_cluster_no_8","rank_5","105","A novel optimal capacitor placement algorithm using Nelder-Mead PSO","In this paper, a new efficient algorithm is developed for optimal placement of capacitor using Nelder-Mead particle swarm optimisation (NM-PSO). The developed capacitor placement problem (CPP) has been formulated to make the objective function (i.e., planning method) more realistic by incorporating essential economic factors, viz., maintenance cost, interest on investment cost and inflation. This developed CPP is found capable of providing more cost-effective solution than existing approaches. Simulation results are obtained for IEEE 9-bus and 69-bus radial distribution system. Results obtained with NM-PSO, are compared with the PSO approach. For both the IEEE test systems, simulation results reveal more effective and profitable planning with NM-PSO. In addition, sensitivity analysis is carried out to highlight the impact of newly incorporated economic factors, justifying their inclusion in CPP.","10.1504/IJBIC.2014.065012","71777","0.2175439"
"572","hot_topic_cluster_no_8","rank_5","105","Total Optimization of Energy Networks in a Smart City by Multi-Swarm Differential Evolutionary Particle Swarm Optimization","This paper proposes total optimization of energy networks in a smart city (SC) by multi-swarm differential evolutionary particle swarm optimization (MS-DEEPSO). Efficient utilization of energy is necessary for reduction of CO2 emission, and SC demonstration projects have been conducted around the world for reducing total energies and the amount of CO2 emission. The problem can be formulated as a mixed integer nonlinear programming problem and various evolutionary computation techniques such as particle swarm optimization, differential evolution, and DEEPSO have been applied to the problem. However, there is still room for improving solution quality. Multi-swarm based evolutionary computation methods have been verified to improve solution quality and the approach has a possibility for improving solution quality. Considering these backgrounds, the paper proposes total optimization of energy networks in an SC by MS-DEEPSO with only migration model and with only abest model. The results of MS-DEEPSO with both migration and abest, only migration, and only abest model based methods are compared with those by the original DEEPSO based method with a single swarm. Various migration topologies, migration policies and intervals, and the number of sub-swarms are also investigated. It is verified that MS-DEEPSO with both migration and abest model based method with hyper-cube topology and the W-B policy is the most effective among all of multi-swarm parameters.","10.1109/TSTE.2018.2882203","21918","0.2191781"
"573","hot_topic_cluster_no_8","rank_5","105","A Green Competitive Vehicle Routing Problem under Uncertainty Solved by an Improved Differential Evolution Algorithm","Regarding the development of distribution systems in the recent decades, fuel consumption of trucks has increased noticeably, which has a huge impact on greenhouse gas emissions. For this reason, the reduction of fuel consumption has been one of the most important research areas in the last decades. The aim of this paper is to propose a robust mathematical model for a variant of a vehicle routing problem (VRP) to optimize sales of distributers, in which the time of distributor service to customers is uncertain. To solve the model precisely, the improved differential evolution (IDE) algorithm is used and obtained results were compared with the result of a particle swarm optimization (PSO) algorithm. The results indicate that the IDE algorithm is able to obtain better solutions in solving large-sized problems; however, the computational time is worse than PSO.","10.5829/ije.2019.32.07a.10","24565","0.2354167"
"574","hot_topic_cluster_no_8","rank_5","105","The influence of carbon dioxide trading scheme on economic dispatch of generators","This paper aims at probing into the topic of power units' operation and dispatch based on Carbon Dioxide (CO2) trading scheme. The trading cost of CO2 emission is embedded into the traditional economic dispatch model, which will be solved by the New Particle Swarm Optimization (NPSO). By considering the CO2 trading scheme, the influences of the various strategies for unit's dispatch are simulated and analyzed in this paper. The proposed method, NPSO is developed in such a way that PSO with Constriction Factor (PSO-CF) algorithm is applied as a based level search. NPSO introduces two operators, ""Random Particles"" and ""Fine-Tuning"" into the PSO-CF algorithm to improve the drawback of searching global optimum and make the search method more efficient at the end of search. The efficiency and ability of NPSO is demonstrated by the six generating units. Simulation results indicated that reasonable solutions provide a practical and flexible framework for power sectors. They can be also used for generating alternatives and thus help decision makers to obtain the goals of minimal operation cost under their desired emission's policies. Crown ","10.1016/j.apenergy.2011.06.025","79166","0.2434211"
"575","hot_topic_cluster_no_8","rank_5","105","Nested optimization design for combined cooling, heating, and power system coupled with solar and biomass energy","Renewable energy source integration in combined cooling, heating, and power (RES-CCHP) systems can improve energy efficiency and foster renewable energy consumption, which is a promising solution to the current energy and environmental crisis. This paper studies the off-design characteristics of these systems, establishes a two-way interaction mechanism between the system capacity and operation strategy, and proposes a two-stage nested optimization design method for the RES-CCHP. In the first stage, the capacity of each system component is obtained by the genetic algorithm and then used as the constraint for the operation optimization. In the second stage, the nonlinear programming algorithm is employed to optimize the operational energy consumption, costs, and CO2 emissions taking the off-design characteristics of core devices into consideration. Several case studies are conducted to verify the feasibility of the two-stage optimal design method. It was found that the two-stage nested optimization design increases the primary energy saving ratio, cost saving ratio, and carbon dioxide emission reduction ratio by 4.5%, 4.32%, and 3.27% compared with the conventional optimal design method, respectively. Overall, the proposed two-stage nested optimization design was tested to improve the performance and the renewable energy consumption.","10.1016/j.ijepes.2020.106236"," 1476","0.2662162"
"576","hot_topic_cluster_no_9","rank_1","618","From universal towards child-specific protection of the right to privacy online: Dilemmas in the EU General Data Protection Regulation","The new European Union (EU) General Data Protection Regulation aims to adapt children's right to privacy to the 'digital age'. It explicitly recognizes that children deserve specific protection of their personal data, and introduces additional rights and safeguards for children. This article explores the dilemmas that the introduction of the child-tailored online privacy protection regime creates - the 'empowerment versus protection' and the 'individualized versus average child' dilemmas. It concludes that by favouring protection over the empowerment of children, the Regulation risks limiting children in their online opportunities, and by relying on the average child criteria, it fails to consider the evolving capacities and best interests of the child.","10.1177/1461444816686327","47169","0.1785714"
"577","hot_topic_cluster_no_9","rank_1","618","Data capitalism and the user: An exploration of privacy cynicism in Germany","Ever since empirical studies found only a weak, if any, relationship between privacy concerns and privacy behavior, scholars have struggled to explain the so-called privacy paradox. Today, a number of theoretical arguments illuminate users' privacy rationales, including the privacy calculus, privacy literacy, and contextual differentiations. A recent approach focuses on user resignation, apathy, or fatigue. In this piece, we concentrate on privacy cynicism, an attitude of uncertainty, powerlessness, mistrust, and resignation toward data handling by online services that renders privacy protection subjectively futile. We discuss privacy cynicism in the context of data capitalism, as a coping mechanism to address the tension between digital inclusion and a desire for privacy. Moreover, we introduce a measure for privacy cynicism and investigate the phenomenon based on a large-scale survey in Germany. The analysis highlights the multidimensionality of the construct, differentiating its relationships with privacy concerns, threat experience, Internet skills, and protection behavior.","10.1177/1461444820912544"," 8714","0.1851852"
"578","hot_topic_cluster_no_9","rank_1","618","How Online Privacy Literacy Supports Self-Data Protection and Self-Determination in the Age of Information","Current debates on online privacy are rooted in liberal theory. Accordingly, privacy is often regarded as a form of freedom from social, economic, and institutional influences. Such a negative perspective on privacy, however, focuses too much on how individuals can be protected or can protect themselves, instead of challenging the necessity of protection itself. In this article, I argue that increasing online privacy literacy not only empowers individuals to achieve (a necessarily limited) form of negative privacy, but has the potential to facilitate a privacy deliberation process in which individuals become agents of social change that could lead to conditions of positive privacy and informational self-determination. To this end, I propose a four-dimensional model of online privacy literacy that encompasses factual privacy knowledge, privacy-related reflection abilities, privacy and data protection skills, and critical privacy literacy. I then outline how this combination of knowledge, abilities, and skills 1) enables to individuals to protect themselves against some horizontal and vertical privacy intrusions and 2) motivates individuals to critically challenge the social structures and power relations that necessitate the need for protection in the first place. Understanding these processes, as well as critically engaging with the normative premises and implications of the predominant negative concepts of privacy, offers a more nuanced direction for future research on online privacy literacy and privacy in general.","10.17645/mac.v8i2.2855","17261","0.1905660"
"579","hot_topic_cluster_no_9","rank_1","618","Remediation of Marine Pollution by Microorganisms in the Comprehensive Management of Coastal Zones","Degradation characteristics of marine pollutants were studied in this paper. A microbial degradation agent for degradation of pollutants was developed and applied to a coastal bioremediation experiment. At the same time, the degradation of marine pollutants in intertidal and subtidal zones was investigated. The degradation rate of total alkane and total aromatic hydrocarbon was analyzed to evaluate the degradation effect. The results showed that the degrading bacteria group DC10 could degrade most of the alkanes and aromatics, and the degradation was significant in bioremediation experiments on a polluted coastal beach.","10.2112/SI82-035.1","35084","0.1971429"
"580","hot_topic_cluster_no_9","rank_1","618","Data privacy against innovation or against discrimination?: The case of the California Consumer Privacy Act (CCPA)","Conducing a case study on the California Consumer Privacy Act (CCPA), this paper analyzes divergent frames of privacy argued by different stakeholders. While the United States has allowed corporate self-regulation of consumer privacy, California became the first state to introduce its own privacy law in June 2018. In early 2019, California held public forums on CCPA, which then became a battleground for various stakeholders to discuss data privacy regulations. Examining 105 public comments made by 99 speakers in 7 CCPA public forums, this study identified that corporate representatives and consumer advocates differed in seven major areas: (1) the purpose of CCPA, (2) definitions of personal information and consumer, (3) operationalization of opt-out, (4) non-discrimination rules, (5) economic ramifications, (6) consumer literacy, and (7) comparison with other privacy frameworks. The findings suggest that corporate speakers follow the frame of privacy as a commodity, while consumer speakers seek the frame of privacy as a right.","10.1016/j.tele.2020.101431"," 6401","0.2025316"
"581","hot_topic_cluster_no_9","rank_2","176","Analysis of land use and cover change in Sichuan province, China","A series of environmental policies in Sichuan province was executed to restore the grassland and forestland on some degraded lands after 2000. But the effectiveness on land use and cover change (LUCC) has not yet been systematically investigated. We undertook a detailed analysis about land use and cover change between 2000 and 2005 in Sichuan province. Our study mainly utilized remotely sensed data of 2005 China-Brazil Earth Resources Satellite II (CBERS II) and 2000 Landsat 5 thematic mapper (TM) data. Land use and cover change between 2000 and 2005 was visually interpreted by CBERS II with ArcInfo Workstation based on land use and cover database interpreted from TM. Then LUCC was validated by ground truth with global positioning system receivers. Our analysis illustrates that the conservation policies to restore the grassland and forestland were successful to a lesser extent. But more measures to restore the grassland and forestland of Sichuan province have to be taken further in the future. ","10.1117/1.JRS.6.063587","76344","0.1687500"
"582","hot_topic_cluster_no_9","rank_2","176","Urban Network and Regions in China: An Analysis of Daily Migration with Complex Networks Model","This paper analyzed urban network and regions in China using a complex network model. Data of daily migration among 348 prefectural-level cities from the Baidu Map location-based service (LBS) Open Platform were used to calculate urban network metrics and to delineate boundaries of urban regions. Results show that urban network in China displays an obvious hierarchy in terms of attracting and distributing population and controlling regional interaction. Regional integration has become increasingly prominent, as administrative boundaries and natural barriers no longer have strong impacts on urban connections. Overall, 18 urban regions were identified according to urban connectivity, and the degree of urban connection is higher among cities in the same urban region. Due to geographical proximity and close interaction, several provincial capital cities form an urban region with cities from neighboring provinces instead of those from the same province. Identification of urban region boundaries is of significant importance for sustainable development and policymaking on the demarcation of urban economic zones, urban agglomerations, and future adjustment of provincial administrative boundaries in China.","10.3390/su12083208","12618","0.1698925"
"583","hot_topic_cluster_no_9","rank_2","176","Regional sustainability system as ecosystem: case study of China's two leading economic circles from a keystone perspective","Exploring the structure, function, and dynamics of regional sustainability system (as ecosystem) can help providing insights into the importance of how environmental management answers to socioeconomic and environmental changes for transitioning to sustainable development. The connectivity (structure) and stability (function) of regional sustainability system are investigated through making regional radiation capability model and spatial coordination index model, which is relied on how keystone province influences the other provinces from society, economy and environment dimensions. The brand new models are then applied to the two largest urban agglomerations in China during period 2001-2014. The results show that the connectivity and stability in BHR are distinctly weaker than in YRD, and reveal the essential characteristics and underlying drivers of regional sustainability system. To be specific, initially, it reflects that on the whole BHR harbors larger difference and lower radiation capability than YRD. Additionally, the regional sustainability system of YRD has much higher stability and better interactive function than of BHR. Thirdly, although the sustainable development level of both BHR and YRD increases at a certain speed year by year, the disparity among various provinces shows anapparent decline and the contribution mainly comes from intra-BHR. Finally, it also provides that how the key province and the surrounding province interact and what the main contributors to the dynamics of regional sustainability system are. Overall, taking regional sustainability system as ecosystem can well reveal this complex system and can provide references to any other urban agglomerations in worldwide.","10.1007/s10668-017-0068-9","27813","0.1717172"
"584","hot_topic_cluster_no_9","rank_2","176","Measurement of climate complexity using sample entropy","A climate system is a complex nonlinear system. Estimation of the complexity is of great interest in climatic forecast and prediction. In this paper, we propose the use of sample entropy (SampEn), an entropy-based algorithm, to measure the complexity of daily temperature series. Estimations of SampEn were calculated for 50 meteorological stations in the mountains of Southwest China, particularly in Yunnan Province. On the basis of these data, stations were grouped in climatically homogenous regions (climate provinces), and the spatial pattern of SampEn for each climate province was investigated. The SampEn value of spatial distribution of climate provinces reflects the varying degree of influence of the monsoonal air masses. High SampEn values occur in interactive regions of different air masses, owing to large regional differences in weather processes, while the southwest region is under the influence of the Southwest Monsoon leading to a homogenous climatic environment, low SampEn values and small spatial variations of SampEn. The results suggest that SampEn is an alternative nonlinear approach for analyzing and predicting complexity of climatic time series. ","10.1002/joc.1357","87334","0.1855263"
"585","hot_topic_cluster_no_9","rank_2","176","Spatiotemporal evolution of urban agglomerations in China during 2000-2012: a nighttime light approach","Context Urban agglomeration is an advanced spatial organization of cities, usually caused by urbanization processes when cities develop to a certain level - typically associated with higher population density and a certain density of built environment. However, compared with various studies focusing on specific cities, urban agglomerations are still understudied, especially for the quantitative identification of spatiotemporal evolution of urban agglomerations. Objectives This study aims to identify the boundary of urban agglomerations in China from 2000 to 2012, and to explore the temporal evolution and spatial difference of urban agglomerations. Methods Firstly, the core zone of urban agglomerations was identified using an appropriate threshold of the digital number (DN) of nighttime light. Secondly, the mean patch area and gravity model were used to determine the affected zone of urban agglomerations. Thirdly, spatiotemporal contrast was conducted focusing on the 23 main urban agglomerations in China. Results By 2012, the most highly developed Yangtze River Delta and Pearl River Delta urban agglomerations met the standard of world level, with the Beijing-Tianjin-Hebei urban agglomeration for regional level, as well as 11 urban agglomerations for sub-regional level. Regional differences in urban agglomerations between southern and northern China, or between coastal and inland China remained stable over the study period of 2000-2012. Compared with the western urban agglomerations, the outward expansion of eastern urban agglomerations decelerated. From 2000 to 2012, the overall development mode of urban agglomerations shifted from the core-expansion to the peripheral-development, together with slower expansion of urban agglomerations after 2006. Conclusions Nighttime light data are effective in exploring the spatiotemporal evolution of urban agglomerations.","10.1007/s10980-019-00956-y","15147","0.2346535"
"586","hot_topic_cluster_no_9","rank_3","583","Relationship Between Parental and Adolescent eHealth Literacy and Online Health Information Seeking in Taiwan","This study examined the relationship between parental and adolescent eHealth literacy and its impact on online health information seeking. Data were obtained from 1,869 junior high school students and 1,365 parents in Taiwan in 2013. Multivariate analysis results showed that higher levels of parental Internet skill and eHealth literacy were associated with an increase in parental online health information seeking. Parental eHealth literacy, parental active use Internet mediation, adolescent Internet literacy, and health information literacy were all related to adolescent eHealth literacy. Similarly, adolescent Internet/health information literacy, eHealth literacy, and parental active use Internet mediation, and parental online health information seeking were associated with an increase in adolescent online health information seeking. The incorporation of eHealth literacy courses into parenting programs and school education curricula is crucial to promote the eHealth literacy of parents and adolescents.","10.1089/cyber.2015.0110","63662","0.1671053"
"587","hot_topic_cluster_no_9","rank_3","583","Parental e-nvolvement: a phenomenological research on electronic parental involvement","This phenomenological study explored parental e-nvolvement (or electronic parental involvement), defined as parental efforts to plan, engage in, support, monitor and/or assess the learning experiences of their children either at home or at school predominantly using technological devices and media. Data were gathered from 23 volunteering parents through semi-structured interviews. The findings suggested that most parents use or have their children use a variety of technologies with tablets, computers, (smart)phones, and internet taking the lead. Participating parents used technology to have their children study, make practice on or repeat what they have learned at school, to help them prepare research projects, homework or presentations, and to communicate with teachers, schools and other parents. The pros of parental e-nvolvement mainly included enabling parents to supervise their children in terms of academic, personal or social well-being; increasing technology literacy; enabling easy and quick access to information sources; enhancing learner autonomy and academic achievement. Cons included mainly the risk of exposure to inconvenient websites/content; technology addiction; and making the child antisocial. Finally, different strategies to parents use to prevent their children from the harms and risks during parental e-nvolvement were presented and discussed.","10.1080/22040552.2016.1227255","58366","0.1695652"
"588","hot_topic_cluster_no_9","rank_3","583","Digitization and the Future of Natural History Collections","Natural history collections (NHCs) are the foundation of historical baselines for assessing anthropogenic impacts on biodiversity. Along these lines, the online mobilization of specimens via digitization-the conversion of specimen data into accessible digital content-has greatly expanded the use of NHC collections across a diversity of disciplines. We broaden the current vision of digitization (Digitization 1.0)-whereby specimens are digitized within NHCs-to include new approaches that rely on digitized products rather than the physical specimen (Digitization 2.0). Digitization 2.0 builds on the data, workflows, and infrastructure produced by Digitization 1.0 to create digital-only workflows that facilitate digitization, curation, and data links, thus returning value to physical specimens by creating new layers of annotation, empowering a global community, and developing automated approaches to advance biodiversity discovery and conservation. These efforts will transform large-scale biodiversity assessments to address fundamental questions including those pertaining to critical issues of global change.","10.1093/biosci/biz163","13667","0.1703704"
"589","hot_topic_cluster_no_9","rank_3","583","Mediating social media use: Connecting parents' mediation strategies and social media literacy","Increasingly complex and multipurpose social media platforms require digital competences from parents and adolescents alike. While adolescents grow up with social media, parents have more difficulties with them, leading to uncertainties regarding their adolescents' social media mediation. This study contributes to parental mediation research by (1) investigating whether mediation strategies defined by previous research are also relevant for social media use, and (2) exploring whether parents' social media literacy is connected to the choice for a certain mediation strategy, as previous research already identified other impact factors such as children's age or family composition. Using a qualitative research design, we interviewed 14 parents and 13 adolescents from 10 families in Belgium. Results indicate that, consistent with previous research, parents in this study mostly use active mediation focusing on risks and safety on social media. However, some parents monitor their children through social media accounts specifically set up for monitoring, or specialized mobile apps. Furthermore, parents with high (mostly critical) social media literacy choose active mediation over restrictive or technical strategies, recognizing opportunities of social media and letting adolescents explore on their own.","10.5817/CP2017-3-5","48942","0.1707317"
"590","hot_topic_cluster_no_9","rank_3","583","Distress and Apprehension Among New Parents During the COVID-19 Pandemic: The Contribution of Personal Resources","The study examined parental distress and apprehension about raising an infant during the COVID-19 pandemic among new Israeli parents, investigating the role of personal resources (low attachment avoidance and anxiety, high self-mastery) and various COVID-19-related anxieties and comparing mothers and fathers. A convenience sample of 606 Israeli parents (469 mothers and 137 fathers) whose first child was 3-12 months old was recruited through social media. No significant differences emerged between mothers and fathers in level of parental distress or apprehension. Poorer health, higher attachment avoidance and anxiety, lower self-mastery. and a higher level of COVID-19-related anxiety over going for infant health checkups contributed significantly to greater parental distress. Lower level of education, being a woman. higher attachment avoidance and anxiety, and higher levels of all COVID-19-related anxieties contributed significantly to greater pandemic-related apprehension. The findings show that new parents may experience parental distress and concerns about raising a child during the crisis, and that whereas specific COVID-19-anxieties are unrelated to global parental distress, they are linked to the apprehension aroused by the pandemic. Moreover, they highlight the contribution of parents' personal resources, showing that lower attachment avoidance and anxiety are associated with lower distress and apprehension. whereas self-mastery is especially significant for lessening the apprehension about raising an infant in this period. These insights may be used in targeted interventions to reduce distress in vulnerable populations, such as individuals who recently became parents for the first time.","10.1037/ort0000497","16364","0.1974790"
"591","hot_topic_cluster_no_9","rank_4","47","Estimation and easy calculation of the Palmer Drought Severity Index from the meteorological data by using the advanced machine learning algorithms","Drought, which has become one of the most severe environmental problems worldwide, has serious impacts on ecological, economic, and socially sustainable development. The drought monitoring process is essential in the management of drought risks, and drought index calculation is critical in the tracking of drought. The Palmer Drought Severity Index is one of the most widely used methods in drought calculation. The drought calculation according to Palmer is a time-consuming process. Such a troublesome can be made easier using advanced machine learning algorithms. Therefore, in this study, the advanced machine learning algorithms (LR, ANN, SVM, and DT) were employed to calculate and estimate the Palmer drought Z-index values from the meteorological data. Palmer Z-index values, which will be used as training data in the classification process, were obtained through a special-purpose software adopting the classical procedure. This special-purpose software was developed within the scope of the study. According to the classification results, the best R-value (0.98) was obtained in the ANN method. The correlation coefficient was 0.98, Mean Squared Error was 0.40, and Root Mean Squared Error was 0.56 in this success. Consequently, the findings showed that drought calculation and prediction according to the Palmer Index could be successfully carried out with advanced machine learning algorithms.","10.1007/s10661-020-08539-0"," 6807","0.2500000"
"592","hot_topic_cluster_no_9","rank_4","47","Analysis of hydrologic and agricultural droughts in central part of Iran","In this paper, a comprehensive statistical. analysis of climatic and hydrologic data such as precipitation, temperature, and streamflow data is performed. Then an algorithm is developed to study the drought characteristics, such as duration and severity for a region. A large sequence of synthetic data is generated in order to develop the probability density function of drought characteristics for planning and water allocation purposes. A water balance model is also developed to estimate the hydrologic and agricultural parameters in order to calculate the expected precipitation for normal conditions. This information is needed to determine the soil moisture anomaly index Z and the Palmer Drought Severity Index. The proposed algorithm is applied to the Isfahan region in the central part of Iran. The drought cycles are determined and compared using different techniques. A planning horizon of 30 years is selected and the probable water shortages are determined. The results of the study show the significant value of the proposed methodology for drought studies in and and semiarid regions with limited data availability.","10.1061/(ASCE)1084-0699(2004)9:5(402)","89034","0.2584615"
"593","hot_topic_cluster_no_9","rank_4","47","Elucidating Diverse Drought Characteristics from Two Meteorological Drought Indices (SPI and SPEI) in China","This study elucidates drought characteristics in China during 1980-2015 using two commonly used meteorological drought indices: standardized precipitation index (SPI) and standardized precipitation-evapotranspiration index (SPEI). The results show that SPEI characterizes an overall increase in drought severity, area, and frequency during 1998-2015 compared with those during 1980-97, mainly due to the increasing potential evapotranspiration. By contrast, SPI does not reveal this phenomenon since precipitation does not exhibit a significant change overall. We further identify individual drought events using the three-dimensional (i.e., longitude, latitude, and time) clustering algorithm and apply the severity-area-duration (SAD) method to examine the drought spatiotemporal dynamics. Compared to SPI, SPEI identifies a lower drought frequency but with larger total drought areas overall. Additionally, SPEI identifies a greater number of severe drought events but a smaller number of slight drought events than the SPI. Approximately 30% of SPI-detected drought grids are not identified as drought by SPEI, and 40% of SPEI-detected drought grids are not recognized as drought by SPI. Both indices can roughly capture the major drought events, but SPEI-detected drought events are overall more severe than SPI. From the SAD analysis, SPI tends to identify drought as more severe over small areas within 1 million km(2) and short durations less than 2 months, whereas SPEI tends to delineate drought as more severe across expansive areas larger than 3 million km(2) and periods longer than 3 months. Given the fact that potential evapotranspiration increases in a warming climate, this study suggests SPEI may be more suitable than SPI in monitoring droughts under climate change.","10.1175/JHM-D-19-0290.1"," 8329","0.3234848"
"594","hot_topic_cluster_no_9","rank_4","47","Impacts of reservoir operations on multi-scale correlations between hydrological drought and meteorological drought","Although numerous studies have investigated the relationships between hydrological drought (HD) and meteorological drought (MD) in different regions, few studies have examined the influence of reservoir operations during drought periods on such relationships, particularly at multiple time scales. This study presents a useful framework to examine the influences of reservoir operation rules during drought periods on the multi-scale correlations between HD and MD. Two standardized drought indices (Standardized Streamfiow Index (SSI) and Standardized Precipitation Index (SPI)) with different timescales (1, 3, 6, 12, and 24 months) were used to compare the pre-reservoir and post-reservoir periods. Also, a simple algorithm was proposed to examine the difference value (D-value) between the inflow and outflow under different reservoir operation rules and it was used to evaluate the impacts of the reservoir operation rules during the drought on the correlation of the HD and MD. Monthly streamflow and precipitation records from 1960 to 2015 and monthly inflow and outflow records of a large reservoir (Xinfengjiang reservoir) during 1974-2009 in the basin of the Dongjiang River in south China were used to demonstrate the applicability of the analysis methodology. The results indicate that (1) the reservoir operations during the post-reservoir period (1974-2015) exerted a significant influence (positive impact) on the evolution of the short-term (1 and 3 months) HD by reducing the drought duration and alleviating its magnitude and altered the linear correlation between the HD and MD compared to the pre-reservoir period (1960-1972). However, the effects on the long-term (e.g., 12 and 24 months) HD and its correlation with the MD were not apparent. (2) The impacts of the reservoir operation under the drought condition on the relationship between the HD and MD were clearly different from those under normal rules. This study provides additional information for policy-makers to regarding reservoir management during a drought.","10.1016/j.jhydrol.2018.06.053","35758","0.3385246"
"595","hot_topic_cluster_no_9","rank_4","47","Meteorological drought over the Chinese Loess Plateau: 1971-2010","In this study, the Variable Infiltration Capacity model and Palmer Drought Severity Index (PDSI) were combined for drought identification on the Loess Plateau. The calibration method of climatic characteristic (K (j) ) in PDSI was improved. Land cover datasets in 1980 and 2005 were used to drive the model. The driest periods over the past four decades of the study region emerged in 1976-1982, 1997-2001 and 2003-2008. Regardless of ranking by duration, spatial extent or severity, most of the prominent droughts occurred in the detected driest periods. The drought severity and area over the upper reaches of the Yellow River were higher than other domains. A total of 53 droughts with area greater than the 25,000 km(2) threshold were identified with durations longer than 3 months using clustering algorithm. Most regions of the study area exhibited spatially increasing trends in drought severity and frequency, indicating that the Loess Plateau has experienced apparent drying and warming processes between 1971 and 2010.","10.1007/s11069-013-0553-x","74387","0.3686567"
"596","hot_topic_cluster_no_9","rank_5","352","Infrastructure deficiencies and adoption of mobile money in Sub-Saharan Africa","We use survey data conducted in 11 countries in Sub-Saharan Africa in 2011 to analyze how the availability of physical infrastructure influences adoption of mobile phones and usage of mobile services. The availability of physical service infrastructure is approximated by data on nighttime light intensity in the areas in which survey respondents reside. After controlling for a number of individual and household characteristics including disposable income, we find that adoption of mobile phones is higher in areas with better physical infrastructure. However, mobile phone users who live in areas with poor infrastructure are more likely to rely on mobile phones to make financial transactions than individuals living in areas with better infrastructure. On the other hand, the use of mobile phones to access services such as email, skype, social media networks and Internet browsing is not dependent on the availability of physical infrastructure. Our results support the notion that mobile phones improve the livelihood of individuals residing in remote areas by providing them with access to financial services which are otherwise not available physically. ","10.1016/j.infoecopol.2017.05.003","44769","0.1636364"
"597","hot_topic_cluster_no_9","rank_5","352","Activity-Based Human Mobility Patterns Inferred from Mobile Phone Data: A Case Study of Singapore","In this study, with Singapore as an example, we demonstrate how we can use mobile phone call detail record (CDR) data, which contains millions of anonymous users, to extract individual mobility networks comparable to the activity-based approach. Such an approach is widely used in the transportation planning practice to develop urban micro simulations of individual daily activities and travel; yet it depends highly on detailed travel survey data to capture individual activity-based behavior. We provide an innovative data mining framework that synthesizes the state-of-the-art techniques in extracting mobility patterns from raw mobile phone CDR data, and design a pipeline that can translate the massive and passive mobile phone records to meaningful spatial human mobility patterns readily interpretable for urban and transportation planning purposes. With growing ubiquitous mobile sensing, and shrinking labor and fiscal resources in the public sector globally, the method presented in this research can be used as a low-cost alternative for transportation and planning agencies to understand the human activity patterns in cities, and provide targeted plans for future sustainable development.","10.1109/TBDATA.2016.2631141","46590","0.1688172"
"598","hot_topic_cluster_no_9","rank_5","352","WiFi indoor positioning based on regularized online sequence extreme learning machine","WiFi positioning based on fingerprint has received widespread attention and practical applications. However, the fingerprints are susceptible to environmental changes, such as shadowing, multipath, temperature, humidity and obstacles. Due to the instability of received signal strength (RSS), it brings plenty of difficult for WiFi positioning with high accuracy. In this paper, a regularised online sequence extreme learning machine with forgetting parameters (FP-ELM) is adopted to solve the issue accordingly. Forgetting factor and regular factor are adopted in FP-ELM to cope with the time-varying nature of RSS and overcome the issue of irreversible matrix in OS-ELM. The fast running speed of the online sequence extreme learning machine (OS-ELM) is also maintained in FP-ELM. Extensive experiments are carried out in simulation and real experimental areas to explore the characteristics of FP-ELM. Moreover, the positioning results of FP-ELM are compared with the conventional algorithms (OS-ELM and KNN). The simulation and experimental results show when the regular factor is set properly, the positioning result based on FP-ELM algorithm is better than conventional algorithms.","10.1080/19479832.2020.1821100"," 4721","0.1714286"
"599","hot_topic_cluster_no_9","rank_5","352","Towards an empowerment framework for evaluating mobile phone use and impact in developing countries","This paper challenges the dominant optimism around mobile phone contribution to socioeconomic development in the developing world. It argues in favor of how mobile phones could empower socio-economically marginalized young people to overcome their marginality by examining its use within the broader socio-cultural, economic and political contexts. Drawing on the information and communication capabilities of mobile phones, this paper uses an ethnographic data and a synthesized empowerment framework to analyze the relationship between mobile phone usage and empowerment of marginalized young people in Sierra Leone. In doing so, the paper focuses on the socio-culture, political and economic uses of mobile phones. The focus on these uses is based on the argument that for mobile phones to empower marginalized young people their uses should facilitate economic, political and socio-cultural issues underpinning their marginality. The study results suggest that mobile phones empower marginalized young people to communicate and access vital livelihood information to articulate their everyday activities. However, it is not strongly evident that the use of mobile phones completely emancipates them from socio-economic and political exclusion. ","10.1016/j.tele.2016.06.003","48763","0.1901408"
"600","hot_topic_cluster_no_9","rank_5","352","Data analysis and call prediction on dyadic data from an understudied population","In this paper we predict outgoing mobile phone calls using machine learning and time clusters based approaches. We analyze to which extent the calling activity of mobile phone users is predictable. The premise is that mobile phone users exhibit temporal regularity in their interactions with majority of their contacts. In the sociological context, most social interactions have fairly reliable temporal regularity. If we quantify the extension of this behavior to interactions on mobile phones we expect that pairwise interaction is not merely a result of randomness, rather it exhibits a temporal pattern. To this end, we not only tested our approach on an original mobile phone usage dataset from a developing country, Pakistan, but we also analyzed the famous Reality Mining Dataset and the Nokia Dataset (from a European country), where we found an equitable basis for comparison with our data. Our original data consists of 783 users and more than 12,000 active dyads. Our results show that temporal information about pairwise user interactions can predict future calls with reasonable accuracy. ","10.1016/j.pmcj.2017.08.002","44267","0.2054054"
